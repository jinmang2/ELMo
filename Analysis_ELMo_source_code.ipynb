{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo Char-CNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# *~ coding convention ~*\n",
    "from overrides import overrides\n",
    "from typing import Callable\n",
    "\n",
    "# Python Standard Library\n",
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Python Installed Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction: dict to namedtuple\n",
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)\n",
    "\n",
    "# input your directories path\n",
    "model_dir = 'C:/workspace/ELMo/161/'\n",
    "args2 = dict2namedtuple(\n",
    "    json.load(\n",
    "        codecs.open(\n",
    "            os.path.join(model_dir, 'config.json'), \n",
    "            'r', encoding='utf-8')\n",
    "    )\n",
    ")\n",
    "\n",
    "# args2.config_path == 'cnn_50_100_512_4096_sample.json'\n",
    "\n",
    "# load config\n",
    "with open(os.path.join(model_dir, args2.config_path), 'r') as fin:\n",
    "    config = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder': {'name': 'elmo',\n",
       "  'projection_dim': 512,\n",
       "  'cell_clip': 3,\n",
       "  'proj_clip': 3,\n",
       "  'dim': 4096,\n",
       "  'n_layers': 2},\n",
       " 'token_embedder': {'name': 'cnn',\n",
       "  'activation': 'relu',\n",
       "  'filters': [[1, 32],\n",
       "   [2, 32],\n",
       "   [3, 64],\n",
       "   [4, 128],\n",
       "   [5, 256],\n",
       "   [6, 512],\n",
       "   [7, 1024]],\n",
       "  'n_highway': 2,\n",
       "  'word_dim': 100,\n",
       "  'char_dim': 50,\n",
       "  'max_characters_per_token': 50},\n",
       " 'classifier': {'name': 'sampled_softmax', 'n_samples': 8192},\n",
       " 'dropout': 0.1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\t발\t없는\t말이\t천리\t간다\t<EOS>\t<PAD>\t<PAD>\t<PAD>\t\n",
      "<BOS>\t다시\t사랑한다\t말\t할까\t<EOS>\t<PAD>\t<PAD>\t<PAD>\t<PAD>\t\n",
      "<BOS>\t유독\t너와\t헤어지다\t싫다\t밤\t집\t으로\t돌아가다\t<EOS>\t\n"
     ]
    }
   ],
   "source": [
    "sents = [['<BOS>', '발', '없는', '말이', '천리', '간다', '<EOS>', '<PAD>', '<PAD>', '<PAD>'],\n",
    "         ['<BOS>', '다시', '사랑한다', '말', '할까', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'],\n",
    "         ['<BOS>', '유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다', '<EOS>']]\n",
    "for sent in sents:\n",
    "    for i in sent:\n",
    "        print(i, end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Inputs\n",
    "sents = [['발', '없는', '말이', '천리', '간다'],\n",
    "         ['다시', '사랑한다', '말', '할까'],\n",
    "         ['유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다']]\n",
    "\n",
    "# Set maximum number of characters\n",
    "max_chars = 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, textset = [], []\n",
    "for sent in sents:\n",
    "    # Add begin of sentence(bos)\n",
    "    data = ['<bos>']\n",
    "    text = []\n",
    "    for token in sent:\n",
    "        text.append(token)\n",
    "        # ELMo's input is character\n",
    "        # Since ElMo uses char-CNN, input_dim must be SAME\n",
    "        # if numChars+2 < max_chars: why +2? bos & eos\n",
    "        #     pad values to pad_id\n",
    "        # else:\n",
    "        #     cut token:= token[:max_chars - 2]\n",
    "        if max_chars is not None and len(token) + 2 > max_chars:\n",
    "            token = token[:max_chars - 2]\n",
    "        data.append(token)\n",
    "    # Add end of sentence(eos)\n",
    "    data.append('<eos>')\n",
    "    dataset.append(data)\n",
    "    textset.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', '발', '없는', '말이', '천리', '간다', '<eos>'],\n",
       " ['<bos>', '다시', '사랑한다', '말', '할까', '<eos>'],\n",
       " ['<bos>', '유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다', '<eos>']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['발', '없는', '말이', '천리', '간다'],\n",
       " ['다시', '사랑한다', '말', '할까'],\n",
       " ['유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If GPU is available, use_cuda:= True\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    EmbeddingLayer\n",
    "    \n",
    "    두 가지 역할을 수행\n",
    "    1. word/character를 사전 규칙에 따라 index로 변환\n",
    "    2. config['token_embedder']['char_dim']으로 차원을 축소\n",
    "    \"\"\"\n",
    "    def __init__(self, n_d, word2id, embs=None, fix_emb=True, oov='<oov>', pad='<pad>', normalize=True):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        if embs is not None:\n",
    "            embwords, embvecs = embs\n",
    "            logging.info(f\"{len(word2id)} pre-trained word embeddings loaded.\")\n",
    "            if n_d != len(embvecs[0]):\n",
    "                logging.warning(f\"[WARNINGS] n_d ({n_d}) != word vector size ({len(embvecs[0])}). \"\n",
    "                                f\"Use {len(embvecs[0])} for embeddings.\")\n",
    "                n_d = len(embvecs[0])\n",
    "        self.word2id = word2id\n",
    "        self.id2word = {i: word for word, i in word2id.items()}\n",
    "        self.n_V, self.n_d = len(word2id), n_d\n",
    "        self.oovid = word2id[oov]\n",
    "        self.padid = word2id[pad]\n",
    "        # n_V -> n_d, 차원 축소\n",
    "        self.embedding = nn.Embedding(self.n_V, n_d, padding_idx=self.padid)\n",
    "        self.embedding.weight.data.uniform_(-0.25, 0.25)\n",
    "        \n",
    "        if embs is not None:\n",
    "            weight = self.embedding.weight\n",
    "            weight.data[:len(embwords)].copy_(torch.from_numpy(embvecs))\n",
    "            logging.info(\"embedding shape: {}\".format(weight.size()))\n",
    "            \n",
    "        if normalize:\n",
    "            weight = self.embedding.weight\n",
    "            norms = weight.data.norm(2, 1)\n",
    "            if norms.dim() == 1:\n",
    "                norms = norms.unsqueeze(1)\n",
    "            weight.data.div_(norms.expand_as(weight.data))\n",
    "            \n",
    "        if fix_emb:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            \n",
    "    def forward(self, input_):\n",
    "        return self.embedding(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the model trained with character-based word encoder.\n",
    "if config['token_embedder']['char_dim'] > 0:\n",
    "    char_lexicon = {}\n",
    "    with codecs.open(os.path.join(model_dir, 'char.dic'), 'r', encoding='utf-8') as fpi:\n",
    "        \"\"\"\n",
    "        # char.dic\n",
    "        <\t0\n",
    "        b\t1\n",
    "        ...\n",
    "        특\t18\n",
    "        별\t19\n",
    "        기\t20\n",
    "        고\t21\n",
    "        ...\n",
    "        ữ\t17675\n",
    "        븟\t17676\n",
    "        铸\t17677\n",
    "        鋳\t17678\n",
    "        <bos>\t17679\n",
    "        <eos>\t17680\n",
    "        <oov>\t17681\n",
    "        <pad>\t17682\n",
    "        <bow>\t17683\n",
    "        <eow>\t17684\n",
    "        \"\"\"\n",
    "        for line in fpi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            if len(tokens) == 1:\n",
    "                tokens.insert(0, '\\u3000')\n",
    "            token, i = tokens\n",
    "            char_lexicon[token] = int(i)\n",
    "    char_emb_layer = EmbeddingLayer(\n",
    "        config['token_embedder']['char_dim'], char_lexicon, fix_emb=False, embs=None)\n",
    "    if use_cuda:\n",
    "        char_emb_layer = char_emb_layer.cuda()\n",
    "    logging.info('char embedding size: ' +\n",
    "                str(len(char_emb_layer.word2id)))\n",
    "else:\n",
    "    char_lexicon = None\n",
    "    char_emb_layer = None\n",
    "\n",
    "# For the model trained with word form word encoder.\n",
    "if config['token_embedder']['word_dim'] > 0:\n",
    "    word_lexicon = {}\n",
    "    with codecs.open(os.path.join(model_dir, 'word.dic'), 'r', encoding='utf-8') as fpi:\n",
    "        \"\"\"\n",
    "        <oov>\t0\n",
    "        <bos>\t1\n",
    "        <eos>\t2\n",
    "        <pad>\t3\n",
    "        ,\t4\n",
    "        .\t5\n",
    "        호텔\t6\n",
    "        ...\n",
    "        (Penobscot\t427840\n",
    "        Tornesch\t427841\n",
    "        Wodociągi\t427842\n",
    "        피트리\t427843\n",
    "        ArmeniaThe\t427844\n",
    "        Cascade에서\t427845\n",
    "        Retrophilia\t427846\n",
    "        kmCala\t427847\n",
    "        노스다코타Dickinson\t427848\n",
    "        \"\"\"\n",
    "        for line in fpi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            if len(tokens) == 1:\n",
    "                tokens.insert(0, '\\u3000')\n",
    "            token, i = tokens\n",
    "            word_lexicon[token] = int(i)\n",
    "    word_emb_layer = EmbeddingLayer(\n",
    "        config['token_embedder']['word_dim'], word_lexicon, fix_emb=False, embs=None)\n",
    "    if use_cuda:\n",
    "        word_emb_layer = word_emb_layer.cuda()\n",
    "    logging.info('word embedding size: ' +\n",
    "                str(len(word_emb_layer.word2id)))\n",
    "else:\n",
    "    word_lexicon = None\n",
    "    word_emb_layer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer(\n",
       "  (embedding): Embedding(17685, 50, padding_idx=17682)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer(\n",
       "  (embedding): Embedding(427849, 100, padding_idx=3)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = word_lexicon\n",
    "char2id = char_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset\n",
    "text = textset\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "x = test\n",
    "perm = None\n",
    "shuffle = False\n",
    "# sort = True\n",
    "sort = False\n",
    "\n",
    "ind = list(range(len(x)))\n",
    "lst = perm or ind\n",
    "print(lst)\n",
    "if shuffle:\n",
    "    random.shuffle(lst)\n",
    "    \n",
    "if sort:\n",
    "    lst.sort(key=lambda l: -len(x[l]))\n",
    "    print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', '발', '없는', '말이', '천리', '간다', '<eos>'], ['<bos>', '다시', '사랑한다', '말', '할까', '<eos>'], ['<bos>', '유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다', '<eos>']]\n",
      "[0, 1, 2]\n",
      "[['발', '없는', '말이', '천리', '간다'], ['다시', '사랑한다', '말', '할까'], ['유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다']]\n"
     ]
    }
   ],
   "source": [
    "x = [x[i] for i in lst]\n",
    "ind = [ind[i] for i in lst]\n",
    "if text is not None:\n",
    "    text = [text[i] for i in lst]\n",
    "\n",
    "print(x)\n",
    "print(ind)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_len = 0.0\n",
    "batches_w, batches_c, batches_lens, batches_masks, batches_text, batches_ind = [], [], [], [], [], []\n",
    "size = batch_size\n",
    "nbatch = (len(x) - 1) // size + 1\n",
    "\n",
    "nbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xec\\x82\\xac\\xeb\\x9e\\x91\\xed\\x95\\x9c\\xeb\\x8b\\xa4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'사랑한다'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> : 17679\n",
      "<eos> : 17680\n",
      "<oov> : 17681\n",
      "<pad> : 17682\n",
      "<bow> : 17683\n",
      "<eow> : 17684\n"
     ]
    }
   ],
   "source": [
    "for i in ['<bos>', '<eos>', '<oov>', '<pad>', '<bow>', '<eow>']:\n",
    "    print(f\"{i} : {char2id[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 문장:\n",
      "tensor([[17684, 17679, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   217, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   186,    31, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   134,    53, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   419,    40, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,    65,    92, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684, 17680, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682]])\n",
      "2번째 문장:\n",
      "tensor([[17684, 17679, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,    92,    42, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,    43,   580,    59,    92, 17683, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   134, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   185,    78, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684, 17680, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682]])\n",
      "3번째 문장:\n",
      "tensor([[17684, 17679, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,    87,   416, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   183,   223, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   478,    27,    76,    92, 17683, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,  1209,    92, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,  1217, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   439, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   198,    57, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   774,   136,    32,    92, 17683, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684, 17680, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682]])\n"
     ]
    }
   ],
   "source": [
    "oov='<oov>'\n",
    "pad='<pad>'\n",
    "\n",
    "# Create batch\n",
    "for i in range(nbatch):\n",
    "    start_id, end_id = i * size, (i + 1) * size\n",
    "    # Create one_batch---------------------------------------\n",
    "    x_b = x[start_id: end_id]\n",
    "    batch_size = len(x_b)\n",
    "    lst = list(range(batch_size))\n",
    "    if sort:\n",
    "        lst.sort(key=lambda l: -len(x[l]))\n",
    "    # shuffle the sentences by\n",
    "    x_b = [x_b[i] for i in lst]\n",
    "    lens = [len(x_b[i]) for i in lst]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # get a batch of word id whose size is (batch x max_len)\n",
    "    if word2id is not None:\n",
    "        oov_id = word2id.get(oov, None)\n",
    "        pad_id = word2id.get(pad, None)\n",
    "        assert oov_id is not None and pad_id is not None\n",
    "        batch_w = torch.LongTensor(batch_size, max_len).fill_(pad_id)\n",
    "        for i, x_i in enumerate(x_b):\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_w[i][j] = word2id.get(x_ij, oov_id)\n",
    "    else:\n",
    "        batch_w = None\n",
    "    \n",
    "    # get a batch of character id whose size is (batch x max_chars)\n",
    "    if char2id is not None:\n",
    "        bow_id, eow_id, oov_id, pad_id = [\n",
    "            char2id.get(key, None) \n",
    "            for key in ('<eow>', '<bow>', oov, pad)\n",
    "        ] # 왜 거꾸로 받지???ㄷㄷ;;\n",
    "        assert ((bow_id is not None) and \n",
    "                (eow_id is not None) and\n",
    "                (oov_id is not None) and\n",
    "                (pad_id is not None))\n",
    "        if config['token_embedder']['name'].lower() == 'cnn':\n",
    "            max_chars = config['token_embedder']['max_characters_per_token']\n",
    "            assert max([len(w) for i in lst for w in x_b[i]]) + 2 <= max_chars\n",
    "        elif config['token_embedder']['name'].lower() == 'lstm':\n",
    "            max_chars = max([len(w) for i in lst for w in x_b[i]]) + 2\n",
    "        else:\n",
    "            raise ValueError('Unknown token_embedder: {0}'.format(config['token_embedder']['name']))\n",
    "        batch_c = torch.LongTensor(batch_size, max_len, max_chars).fill_(pad_id)\n",
    "        for i, x_i in enumerate(x_b):\n",
    "            print(f\"{i+1}번째 문장:\")\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_c[i][j][0] = bow_id\n",
    "                if x_ij in ['<bos>', '<eos>']:\n",
    "                    batch_c[i][j][1] = char2id.get(x_ij)\n",
    "                    batch_c[i][j][2] = eow_id\n",
    "                else:\n",
    "                    for k, c in enumerate(x_ij):\n",
    "                        batch_c[i][j][k+1] = char2id.get(c, oov_id)\n",
    "                    batch_c[i][j][len(x_ij)+1] = eow_id\n",
    "            print(batch_c[i])\n",
    "    else:\n",
    "        batch_c = None\n",
    "        \n",
    "    masks = [torch.LongTensor(batch_size, max_len).fill_(0), [], []]\n",
    "    \n",
    "    for i, x_i in enumerate(x_b):\n",
    "        for j in range(len(x_i)):\n",
    "            masks[0][i][j] = 1\n",
    "            if j + 1 < len(x_i):\n",
    "                masks[1].append(i * max_len + j)\n",
    "            if j > 0:\n",
    "                masks[2].append(i * max_len + j)\n",
    "\n",
    "    assert len(masks[1]) <= batch_size * max_len\n",
    "    assert len(masks[2]) <= batch_size * max_len\n",
    "\n",
    "    masks[1] = torch.LongTensor(masks[1])\n",
    "    masks[2] = torch.LongTensor(masks[2])                            \n",
    "    # -------------------------------------------------------\n",
    "    bw, bc, blens, bmasks = batch_w, batch_c, lens, masks\n",
    "    sum_len += sum(blens)\n",
    "    batches_w.append(bw)\n",
    "    batches_c.append(bc)\n",
    "    batches_lens.append(blens)\n",
    "    batches_masks.append(bmasks)\n",
    "    batches_ind.append(ind[start_id: end_id])\n",
    "    if text is not None:\n",
    "        batches_text.append(text[start_id: end_id])\n",
    "        \n",
    "if sort:\n",
    "    perm = list(range(nbatch))\n",
    "    random.shuffle(perm)\n",
    "    batches_w = [batches_w[i] for i in perm]\n",
    "    batches_c = [batches_c[i] for i in perm]\n",
    "    batches_lens = [batches_lens[i] for i in perm]\n",
    "    batches_masks = [batches_masks[i] for i in perm]\n",
    "    batches_ind = [batches_ind[i] for i in perm]\n",
    "    if text is not None:\n",
    "        batches_text = [batches_text[i] for i in perm]\n",
    "\n",
    "logging.info(\"{} batches, avg len: {:.1f}\".format(\n",
    "    nbatch, sum_len / len(x)))\n",
    "recover_ind = [item for sublist in batches_ind for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[     1,   1820,    325,   3232, 345792,   7127,      2,      3,      3,\n",
       "               3],\n",
       "         [     1,    237,  50660,   1489,  13000,      2,      3,      3,      3,\n",
       "               3],\n",
       "         [     1,  36081,  26437,      0,  65226,   1973,   2607,   3650,      0,\n",
       "               2]])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,   217, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,   186,    31,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,    92,    42,  ..., 17682, 17682, 17682],\n",
       "          [17684,    43,   580,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,    87,   416,  ..., 17682, 17682, 17682],\n",
       "          [17684,   183,   223,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17684,   198,    57,  ..., 17682, 17682, 17682],\n",
       "          [17684,   774,   136,  ..., 17682, 17682, 17682],\n",
       "          [17684, 17680, 17683,  ..., 17682, 17682, 17682]]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 6, 10]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       "  tensor([ 0,  1,  2,  3,  4,  5, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25, 26,\n",
       "          27, 28]),\n",
       "  tensor([ 1,  2,  3,  4,  5,  6, 11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 26, 27,\n",
       "          28, 29])]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['발', '없는', '말이', '천리', '간다'],\n",
       "  ['다시', '사랑한다', '말', '할까'],\n",
       "  ['유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다']]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recover_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    batches_w[0] = batches_w[0].cuda()\n",
    "    batches_c[0] = batches_c[0].cuda()\n",
    "    batches_masks[0] = [mask.cuda() for mask in batches_masks[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[     1,   1820,    325,   3232, 345792,   7127,      2,      3,      3,\n",
       "               3],\n",
       "         [     1,    237,  50660,   1489,  13000,      2,      3,      3,      3,\n",
       "               3],\n",
       "         [     1,  36081,  26437,      0,  65226,   1973,   2607,   3650,      0,\n",
       "               2]], device='cuda:0')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,   217, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,   186,    31,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,    92,    42,  ..., 17682, 17682, 17682],\n",
       "          [17684,    43,   580,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,    87,   416,  ..., 17682, 17682, 17682],\n",
       "          [17684,   183,   223,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17684,   198,    57,  ..., 17682, 17682, 17682],\n",
       "          [17684,   774,   136,  ..., 17682, 17682, 17682],\n",
       "          [17684, 17680, 17683,  ..., 17682, 17682, 17682]]], device='cuda:0')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       "  tensor([ 0,  1,  2,  3,  4,  5, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25, 26,\n",
       "          27, 28], device='cuda:0'),\n",
       "  tensor([ 1,  2,  3,  4,  5,  6, 11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 26, 27,\n",
       "          28, 29], device='cuda:0')]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이어서 계속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w, test_c, test_lens, test_masks, test_text, recover_ind = (\n",
    "    batches_w, batches_c, batches_lens, batches_masks, batches_text, recover_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, c, lens, masks, texts = next(zip(test_w, test_c, test_lens, test_masks, test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output = self.model.forward(w, c, masks)\n",
    "# token_embedder = ConvTokenEmbedder(\n",
    "#     config, word_emb_layer, char_emb_layer, use_cuda)\n",
    "\n",
    "emb_dim = 0\n",
    "output_dim = config['encoder']['projection_dim']\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if word_emb_layer is not None:\n",
    "    emb_dim += word_emb_layer.n_d\n",
    "emb_dim    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cnn',\n",
       " 'activation': 'relu',\n",
       " 'filters': [[1, 32],\n",
       "  [2, 32],\n",
       "  [3, 64],\n",
       "  [4, 128],\n",
       "  [5, 256],\n",
       "  [6, 512],\n",
       "  [7, 1024]],\n",
       " 'n_highway': 2,\n",
       " 'word_dim': 100,\n",
       " 'char_dim': 50,\n",
       " 'max_characters_per_token': 50}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['token_embedder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = config['token_embedder']['filters']\n",
    "char_embed_dim = config['token_embedder']['char_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutions = []\n",
    "\n",
    "for i, (width, num) in enumerate(filters):\n",
    "    conv = nn.Conv1d(\n",
    "        in_channels=char_embed_dim,\n",
    "        out_channels=num, # 문자를 몇 개나 볼 것인지\n",
    "        kernel_size=width,\n",
    "        bias=True\n",
    "    )\n",
    "    if use_cuda:\n",
    "        conv = conv.cuda()\n",
    "    convolutions.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv1d(50, 32, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(50, 32, kernel_size=(2,), stride=(1,)),\n",
       " Conv1d(50, 64, kernel_size=(3,), stride=(1,)),\n",
       " Conv1d(50, 128, kernel_size=(4,), stride=(1,)),\n",
       " Conv1d(50, 256, kernel_size=(5,), stride=(1,)),\n",
       " Conv1d(50, 512, kernel_size=(6,), stride=(1,)),\n",
       " Conv1d(50, 1024, kernel_size=(7,), stride=(1,))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
       "  (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
       "  (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
       "  (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
       "  (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
       "  (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
       "  (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolutions = nn.ModuleList(convolutions)\n",
    "convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_filters = sum(f[1] for f in filters)\n",
    "n_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_highway = config['token_embedder']['n_highway']\n",
    "n_highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 num_layers: int = 1,\n",
    "                 activation: Callable[[torch.Tensor], torch.Tensor] = torch.nn.functional.relu) -> None:\n",
    "        super(Highway, self).__init__()\n",
    "        self._input_dim = input_dim\n",
    "        self._layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, input_dim * 2).cuda() if use_cuda \n",
    "             else nn.Linear(input_dim, input_dim * 2).cuda()\n",
    "             for _ in range(num_layers)])\n",
    "        self._activation = activation\n",
    "        for layer in self._layers:\n",
    "            # We should bias the highway layer to just carry its input forward.  We do that by\n",
    "            # setting the bias on `B(x)` to be positive, because that means `g` will be biased to\n",
    "            # be high, to we will carry the input forward.  The bias on `B(x)` is the second half\n",
    "            # of the bias vector in each Linear layer.\n",
    "            layer.bias[input_dim:].data.fill_(1)\n",
    "        \n",
    "    @overrides\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ\n",
    "        current_input = inputs\n",
    "        for layer in self._layers:\n",
    "            projected_input = layer(current_input)\n",
    "            linear_part = current_input\n",
    "            # NOTE: if you modify this, think about whether you should modify the initialization\n",
    "            # above, too.\n",
    "            nonlinear_part = projected_input[:, (0 * self._input_dim):(1 * self._input_dim)]\n",
    "            gate = projected_input[:, (1 * self._input_dim):(2 * self._input_dim)]\n",
    "            nonlinear_part = self._activation(nonlinear_part)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            current_input = gate * linear_part + (1 - gate) * nonlinear_part\n",
    "        return current_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "highways = Highway(n_filters, n_highway, torch.nn.functional.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2148"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim += n_filters\n",
    "emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = nn.Linear(emb_dim, output_dim, bias=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.forward(w, c, masks)\n",
    "# token_embedder.forward()\n",
    "\n",
    "word_inp = w\n",
    "\n",
    "chars_package = c\n",
    "chars_inp = chars_package\n",
    "\n",
    "mask_package = masks\n",
    "shape = mask_package[0].size(0), mask_package[0].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     1,   1820,    325,   3232, 345792,   7127,      2,      3,      3,\n",
       "              3],\n",
       "        [     1,    237,  50660,   1489,  13000,      2,      3,      3,      3,\n",
       "              3],\n",
       "        [     1,  36081,  26437,      0,  65226,   1973,   2607,   3650,      0,\n",
       "              2]], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "         [17684,   217, 17683,  ..., 17682, 17682, 17682],\n",
       "         [17684,   186,    31,  ..., 17682, 17682, 17682],\n",
       "         ...,\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       "\n",
       "        [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "         [17684,    92,    42,  ..., 17682, 17682, 17682],\n",
       "         [17684,    43,   580,  ..., 17682, 17682, 17682],\n",
       "         ...,\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       "\n",
       "        [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "         [17684,    87,   416,  ..., 17682, 17682, 17682],\n",
       "         [17684,   183,   223,  ..., 17682, 17682, 17682],\n",
       "         ...,\n",
       "         [17684,   198,    57,  ..., 17682, 17682, 17682],\n",
       "         [17684,   774,   136,  ..., 17682, 17682, 17682],\n",
       "         [17684, 17680, 17683,  ..., 17682, 17682, 17682]]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = []\n",
    "batch_size, seq_len = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-1.0409e-01,  1.2906e-02, -4.8705e-02,  ..., -7.6832e-02,\n",
       "            1.6660e-01,  2.5650e-02],\n",
       "          [ 1.0560e-01, -1.2401e-01, -8.1487e-02,  ...,  8.3041e-02,\n",
       "            3.5578e-02, -1.3760e-01],\n",
       "          [-1.0116e-01, -1.1700e-01,  1.5287e-01,  ..., -9.3966e-02,\n",
       "           -1.6059e-01,  4.2700e-02],\n",
       "          ...,\n",
       "          [-1.4495e-01,  8.1293e-05, -6.6826e-02,  ...,  7.0309e-02,\n",
       "            1.7170e-01,  3.1018e-02],\n",
       "          [-1.4495e-01,  8.1293e-05, -6.6826e-02,  ...,  7.0309e-02,\n",
       "            1.7170e-01,  3.1018e-02],\n",
       "          [-1.4495e-01,  8.1293e-05, -6.6826e-02,  ...,  7.0309e-02,\n",
       "            1.7170e-01,  3.1018e-02]],\n",
       " \n",
       "         [[-1.0409e-01,  1.2906e-02, -4.8705e-02,  ..., -7.6832e-02,\n",
       "            1.6660e-01,  2.5650e-02],\n",
       "          [-1.0567e-01, -4.6330e-02,  2.5632e-02,  ..., -1.4543e-01,\n",
       "           -1.2751e-03,  8.7498e-02],\n",
       "          [-5.9886e-02, -1.4820e-01,  1.4243e-01,  ...,  3.4602e-02,\n",
       "           -1.6809e-01, -6.1678e-02],\n",
       "          ...,\n",
       "          [-1.4495e-01,  8.1293e-05, -6.6826e-02,  ...,  7.0309e-02,\n",
       "            1.7170e-01,  3.1018e-02],\n",
       "          [-1.4495e-01,  8.1293e-05, -6.6826e-02,  ...,  7.0309e-02,\n",
       "            1.7170e-01,  3.1018e-02],\n",
       "          [-1.4495e-01,  8.1293e-05, -6.6826e-02,  ...,  7.0309e-02,\n",
       "            1.7170e-01,  3.1018e-02]],\n",
       " \n",
       "         [[-1.0409e-01,  1.2906e-02, -4.8705e-02,  ..., -7.6832e-02,\n",
       "            1.6660e-01,  2.5650e-02],\n",
       "          [ 6.4631e-02, -1.6669e-01, -7.0840e-02,  ...,  1.3873e-01,\n",
       "            1.6704e-01,  8.9316e-02],\n",
       "          [ 1.1592e-01,  1.0139e-01,  1.1302e-01,  ...,  1.6105e-01,\n",
       "            6.2629e-02,  1.1515e-01],\n",
       "          ...,\n",
       "          [-1.3331e-01, -2.1768e-02, -1.5130e-01,  ...,  9.5508e-02,\n",
       "            3.5607e-02, -1.6277e-01],\n",
       "          [ 1.5486e-01, -8.8706e-02,  8.3125e-02,  ...,  1.0443e-01,\n",
       "           -1.4918e-01,  1.2041e-01],\n",
       "          [ 1.4664e-01, -6.3655e-02,  2.3400e-03,  ...,  5.1055e-02,\n",
       "           -2.2278e-02, -1.0562e-01]]], device='cuda:0',\n",
       "        grad_fn=<EmbeddingBackward>)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if word_emb_layer is not None:\n",
    "    batch_size, seq_len = word_inp.size()\n",
    "    variable = Variable(word_inp)\n",
    "    if use_cuda:\n",
    "        variable = variable.cuda()\n",
    "    word_emb = word_emb_layer(variable)\n",
    "    embs.append(word_emb)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 100])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb_layer is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 50])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 50])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_inp = chars_inp.view(batch_size * seq_len, -1)\n",
    "chars_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(17685, 50, padding_idx=17682)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb_layer.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_embedding = char_emb_layer(\n",
    "    Variable(chars_inp).cuda() if use_cuda\n",
    "    else Variable(chars_inp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 50, 50])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 50, 50])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_embedding = torch.transpose(character_embedding, 1, 2)\n",
    "character_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = getattr(torch.nn.functional, \n",
    "                     config['token_embedder']['activation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = convolutions[i](character_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 32, 50])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = torch.max(convolved, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 32])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = activation(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs.append(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([30, 32, 49])\n",
      "torch.Size([30, 32])\n",
      "torch.Size([30, 32])\n",
      "2\n",
      "torch.Size([30, 64, 48])\n",
      "torch.Size([30, 64])\n",
      "torch.Size([30, 64])\n",
      "3\n",
      "torch.Size([30, 128, 47])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128])\n",
      "4\n",
      "torch.Size([30, 256, 46])\n",
      "torch.Size([30, 256])\n",
      "torch.Size([30, 256])\n",
      "5\n",
      "torch.Size([30, 512, 45])\n",
      "torch.Size([30, 512])\n",
      "torch.Size([30, 512])\n",
      "6\n",
      "torch.Size([30, 1024, 44])\n",
      "torch.Size([30, 1024])\n",
      "torch.Size([30, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(convolutions)):\n",
    "    print(i)\n",
    "    convolved = convolutions[i](character_embedding)\n",
    "    print(convolved.shape)\n",
    "    convolved = torch.max(convolved, dim=-1)[0]\n",
    "    print(convolved.shape)\n",
    "    convolved = activation(convolved)\n",
    "    print(convolved.shape)\n",
    "    convs.append(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([30, 32]),\n",
       " torch.Size([30, 32]),\n",
       " torch.Size([30, 64]),\n",
       " torch.Size([30, 128]),\n",
       " torch.Size([30, 256]),\n",
       " torch.Size([30, 512]),\n",
       " torch.Size([30, 1024])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[conv.shape for conv in convs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2048])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb = torch.cat(convs, dim=-1)\n",
    "char_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2048])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb = highways(char_emb)\n",
    "char_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 2048])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_emb_ = char_emb.view(batch_size, -1, n_filters)\n",
    "chat_emb_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs.append(chat_emb_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 100]), torch.Size([3, 10, 2048]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[0].shape, embs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 2148])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding = torch.cat(embs, dim=2)\n",
    "token_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 512])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding = projection(token_embedding)\n",
    "token_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
       "         [ 0.0684, -0.0786,  0.0208,  ...,  0.0929,  0.0057,  0.0524],\n",
       "         [ 0.0423, -0.0481, -0.0020,  ...,  0.1149,  0.0152,  0.0592],\n",
       "         ...,\n",
       "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
       "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
       "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459]],\n",
       "\n",
       "        [[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
       "         [ 0.0618, -0.0532, -0.0128,  ...,  0.0917, -0.0056,  0.0506],\n",
       "         [ 0.0441, -0.0607,  0.0049,  ...,  0.1335, -0.0122,  0.0597],\n",
       "         ...,\n",
       "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
       "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
       "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459]],\n",
       "\n",
       "        [[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
       "         [ 0.0346, -0.0736,  0.0202,  ...,  0.0988,  0.0028,  0.0412],\n",
       "         [ 0.0437, -0.0743,  0.0047,  ...,  0.1194, -0.0323,  0.0387],\n",
       "         ...,\n",
       "         [ 0.0363, -0.0648,  0.0206,  ...,  0.1120, -0.0184,  0.0414],\n",
       "         [ 0.0597, -0.0749,  0.0128,  ...,  0.1011, -0.0258,  0.0474],\n",
       "         [ 0.0351, -0.0268,  0.0019,  ...,  0.0816,  0.0207,  0.0365]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이어서 계속\n",
    "- `Model.forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " tensor([ 0,  1,  2,  3,  4,  5, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28], device='cuda:0'),\n",
       " tensor([ 1,  2,  3,  4,  5,  6, 11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28, 29], device='cuda:0')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = Variable(mask_package[0]).cuda()\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ElmobiLM`을 이해하기 위해선\n",
    "- `PackedSequence, pad_packed_sequence, pack_padded_sequence`\n",
    "- `_EncoderBase`\n",
    "- `LSTMCellWithPorjection`\n",
    "\n",
    "을 이해해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules/utils.py\n",
    "\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([ 7,  6, 10], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_lengths_from_binary_sequence_mask(mask: torch.Tensor):\n",
    "    return mask.long().sum(-1)\n",
    "\n",
    "print(mask)\n",
    "print(get_lengths_from_binary_sequence_mask(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activate Function...\n",
      "\n",
      "sequence_lengths, and it's size = tensor([ 7,  6, 10], device='cuda:0') torch.Size([3])\n",
      "input tensor's size = torch.Size([3, 10, 512])\n",
      "Is datatype is Variable? True\n",
      "* sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "* permutation_index = tensor([2, 0, 1], device='cuda:0')\n",
      "Index Sorting...\n",
      "* sorted_tensor = \n",
      " tensor([[[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
      "         [ 0.0346, -0.0736,  0.0202,  ...,  0.0988,  0.0028,  0.0412],\n",
      "         [ 0.0437, -0.0743,  0.0047,  ...,  0.1194, -0.0323,  0.0387],\n",
      "         ...,\n",
      "         [ 0.0363, -0.0648,  0.0206,  ...,  0.1120, -0.0184,  0.0414],\n",
      "         [ 0.0597, -0.0749,  0.0128,  ...,  0.1011, -0.0258,  0.0474],\n",
      "         [ 0.0351, -0.0268,  0.0019,  ...,  0.0816,  0.0207,  0.0365]],\n",
      "\n",
      "        [[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
      "         [ 0.0684, -0.0786,  0.0208,  ...,  0.0929,  0.0057,  0.0524],\n",
      "         [ 0.0423, -0.0481, -0.0020,  ...,  0.1149,  0.0152,  0.0592],\n",
      "         ...,\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459]],\n",
      "\n",
      "        [[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
      "         [ 0.0618, -0.0532, -0.0128,  ...,  0.0917, -0.0056,  0.0506],\n",
      "         [ 0.0441, -0.0607,  0.0049,  ...,  0.1335, -0.0122,  0.0597],\n",
      "         ...,\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459]]],\n",
      "       device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "index_range = tensor([0, 1, 2], device='cuda:0'), torch.int64, <class 'torch.Tensor'>\n",
      "<warning!> 왜 바로 아래 작업을 해주는지 이해 불가... 바뀌는게 없는거 같은데?\n",
      "index_range to Variable:long...\n",
      "index_range = tensor([0, 1, 2], device='cuda:0'), torch.int64, <class 'torch.Tensor'>\n",
      "reverse_mapping = tensor([1, 2, 0], device='cuda:0')\n",
      "* restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "\n",
      "Return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index\n"
     ]
    }
   ],
   "source": [
    "def sort_batch_by_length(tensor: torch.autograd.Variable,\n",
    "                         sequence_lengths: torch.autograd.Variable):\n",
    "    if not isinstance(tensor, Variable) or not isinstance(sequence_lengths, Variable):\n",
    "        raise Exception(\"Both the tensor and sequence lengths must be torch.autograd.Variables.\")\n",
    "        \n",
    "    sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n",
    "    sorted_tensor = tensor.index_select(0, permutation_index)\n",
    "    \n",
    "    # This is ugly, but required - we are creating a new variable at runtime, so we\n",
    "    # must ensure it has the correct CUDA vs non-CUDA type. We do this by cloning and\n",
    "    # refilling one of the inputs to the function.\n",
    "    index_range = sequence_lengths.data.clone().copy_(torch.arange(0, len(sequence_lengths)))\n",
    "    # This is the equivalent of zipping with index, sorting by the original\n",
    "    # sequence lengths and returning the now sorted indices.\n",
    "    index_range = Variable(index_range.long())\n",
    "    _, reverse_mapping = permutation_index.sort(0, descending=False)\n",
    "    restoration_indices = index_range.index_select(0, reverse_mapping)\n",
    "    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index\n",
    "\n",
    "# sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "# sort_batch_by_length(token_embedding, sequence_lengths)\n",
    "print('Activate Function...', end='\\n\\n')\n",
    "sequence_lengths = mask.long().sum(-1)\n",
    "print(\"sequence_lengths, and it's size = \", end='')\n",
    "print(sequence_lengths, sequence_lengths.size())\n",
    "print(\"input tensor's size = \", end='')\n",
    "print(token_embedding.size())\n",
    "print(\"Is datatype is Variable? \", end='')\n",
    "print(not (not isinstance(token_embedding, Variable) or \n",
    "       not isinstance(sequence_lengths, Variable)))\n",
    "\n",
    "sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n",
    "print('* sorted_sequence_lengths =', sorted_sequence_lengths)\n",
    "print('* permutation_index =', permutation_index)\n",
    "print(\"Index Sorting...\")\n",
    "sorted_tensor = token_embedding.index_select(0, permutation_index)\n",
    "print('* sorted_tensor = \\n', sorted_tensor)\n",
    "index_range = sequence_lengths.data.clone().copy_(torch.arange(0, len(sequence_lengths)))\n",
    "print(f'index_range = {index_range}, {index_range.dtype}, {type(index_range)}')\n",
    "print(\"<warning!> 왜 바로 아래 작업을 해주는지 이해 불가... 바뀌는게 없는거 같은데?\")\n",
    "print(\"index_range to Variable:long...\")\n",
    "index_range = Variable(index_range.long())\n",
    "print(f'index_range = {index_range}, {index_range.dtype}, {type(index_range)}')\n",
    "_, reverse_mapping = permutation_index.sort(0, descending=False)\n",
    "print(f\"reverse_mapping = {reverse_mapping}\")\n",
    "restoration_indices = index_range.index_select(0, reverse_mapping)\n",
    "print(f\"* restoration_indices = {restoration_indices}\", end=\"\\n\\n\")\n",
    "print('Return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules.encoder_base.py\n",
    "from typing import Tuple, Union, Optional, Callable\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "RnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]  # pylint: disable=invalid-name\n",
    "RnnStateStorage = Tuple[torch.Tensor, ...]  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _EncoderBase(nn.Module):\n",
    "    # pyling: disable=abstract-method\n",
    "    \"\"\"\n",
    "    This abstract class serves as a base for the 3 ``Encoder`` abstractions in AllenNLP.\n",
    "    - :class:`~allennlp.modules.seq2seq_encoders.Seq2SeqEncoders`\n",
    "    - :class:`~allennlp.modules.seq2vec_encoders.Seq2VecEncoders`\n",
    "    Additionally, this class provides functionality for sorting sequences by length\n",
    "    so they can be consumed by Pytorch RNN classes, which require their inputs to be\n",
    "    sorted by length. Finally, it also provides optional statefulness to all of it's\n",
    "    subclasses by allowing the caching and retrieving of the hidden states of RNNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, stateful: bool = False) -> None:\n",
    "        super(_EncoderBase, self).__init__()\n",
    "        self.stateful = stateful\n",
    "        self._states: Optional[RnnStateStorage] = None\n",
    "    \n",
    "    def sort_and_run_forward(self,\n",
    "                             module: Callable[[PackedSequence, Optional[RnnState]],\n",
    "                                              Tuple[Union[PackedSequence, torch.Tensor], RnnState]],\n",
    "                             inputs: torch.Tensor,\n",
    "                             mask: torch.Tensor,\n",
    "                             hidden_state: Optional[RnnState] = None):\n",
    "        \"\"\"\n",
    "        Pytorch RNNs는 input이 passing되기 전에 정렬되있어야 함\n",
    "        Seq2xxxEncoders가 이러한 기능을 모두 사용하기에 base class로 제공\n",
    "        \"\"\"\n",
    "        # In some circumstances you may have sequences of zero length. ``pack_padded_sequence``\n",
    "        # requires all sequence lengths to be > 0, so remove sequences of zero length before\n",
    "        # calling self._module, then fill with zeros.\n",
    "        \n",
    "        # First count how many sequences are empty.\n",
    "        batch_size = mask.size(0)\n",
    "        num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "        \n",
    "        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n",
    "            sort_batch_by_length(inputs, sequence_lengths)\n",
    "        \n",
    "        # Now create a PackedSequence with only the non-empty, sorted sequences.\n",
    "        # pad token 제외, 유의미한 값들만 packing\n",
    "        packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                                     sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                                     batch_first=True)\n",
    "        \n",
    "        # Prepare teh initial states.\n",
    "        if not self.stateful:\n",
    "            if hidden_state == None:\n",
    "                initial_states = hidden_state\n",
    "            elif isinstance(hidden_state, tuple):\n",
    "                initial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :]\n",
    "                                  for state in hidden_state]\n",
    "            else:\n",
    "                initial_stats = self._get_initial_states(batch_size, num_valid, sorting_indices)    \n",
    "        else:\n",
    "            initial_states = selt._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "            \n",
    "        # Actually call the module on the sorted PackedSequence\n",
    "        module_output, final_states = module(packed_sequence_input, initial_states)\n",
    "        \n",
    "        return module_output, final_states, restoration_indices\n",
    "    \n",
    "    def _get_initial_states(self,\n",
    "                            batch_size: int,\n",
    "                            num_valid: int,\n",
    "                            sorting_indices: torch.LongTensor) -> Optional[RnnState]:\n",
    "        \"\"\"\n",
    "        RNN의 초기 상태를 반환\n",
    "        추가적으로, 이 메서드는 batch의 새로운 요소의 초기 상태를 추가하기 위해 상태를 변경하여(mutate)\n",
    "            호출시 batch size를 처리\n",
    "        또한 이 메서드는 \n",
    "            1. 배치의 요소 seq. length로 상태를 정렬하는 것과\n",
    "            2. pad가 끝난 row 제거도 처리\n",
    "        중요한 것은 현재의 배치 크기가 이전에 호출되었을 때보다 더 크면 이 상태를 \"혼합\"하는 것이다.\n",
    "        \n",
    "        이 메서드는 (1) 처음 호출되어 아무 상태가 없는 경우 (2) RNN이 heterogeneous state를 가질 때\n",
    "        의 경우를 처리해야 하기 때문에 return값이 복잡함\n",
    "        \n",
    "        (1) module이 처음 호출됬을 때 ``module``의 타입이 무엇이든 ``None`` 반환\n",
    "        (2) Otherwise, \n",
    "            - LSTM의 경우 tuple of ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "                 and ``(num_layers, num_valid, memory_size)``\n",
    "            - GRU의 경우  single ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "        \"\"\"\n",
    "        # We don't know the state sizes the first time calling forward,\n",
    "        # so we let the module define what it's initial hidden state looks like.\n",
    "        if self._states is None:\n",
    "            return None\n",
    "        \n",
    "        # Otherwise, we have some previous states.\n",
    "        if batch_size > self._states[0].size(1):\n",
    "            # This batch is larger than the all previous states.\n",
    "            # If so, resize the states.\n",
    "            num_states_to_concat = batch_size - self._states[0].size(1)\n",
    "            resized_states = []\n",
    "            # state has shape (num_layers, batch_size, hidden_size)\n",
    "            for state in self._states:\n",
    "                # This _must_ be inside the loop because some\n",
    "                # RNNs have states with different last dimension sizes.\n",
    "                zeros = state.data.new(state.size(0),\n",
    "                                       num_states_to_concat,\n",
    "                                       state.size(2)).fill_(0)\n",
    "                zeros = Variable(zeros)\n",
    "                resized_states.append(torch.cat([state, zeros], 1))\n",
    "            self._states = tuple(resized_states)\n",
    "            correctly_shaped_states = self._states\n",
    "        elif batch_size < self._states[0].size(1):\n",
    "            # This batch is smaller than the previous one.\n",
    "            correctly_shaped_states = tuple(staet[:, :batch_size, :] for state in self._states)\n",
    "        else:\n",
    "            correctly_shaped_states = self._states\n",
    "            \n",
    "        # At this point, out states are of shape (num_layers, batch_size, hidden_size).\n",
    "        # However, the encoder uses sorted sequences and additionally removes elements\n",
    "        # of the batch which are fully padded. We need the states to match up to these\n",
    "        # sorted and filtered sequences, so we do that in the next two blocks before\n",
    "        # returning the states.\n",
    "        if len(self._states) == 1:\n",
    "            # GRU\n",
    "            correctly_shaped_state = correctly_shaped_states[0]\n",
    "            sorted_state = correctly_shaped_state.index_select(1, sorting_indices)\n",
    "            return sorted_state[:, :num_valid, :]\n",
    "        else:\n",
    "            # LSTM\n",
    "            sorted_states = [state.index_select(1, sorting_indices)\n",
    "                             for state in correctly_shaped_states]\n",
    "            return tuple(state[:, :num_valid, :] for state in sorted_stest)\n",
    "        \n",
    "    def _update_states(self,\n",
    "                       final_states: RnnStateStorage,\n",
    "                       restoration_indices: torch.LongTensor) -> None:\n",
    "        \"\"\"\n",
    "        RNN forward 동작 후에 state를 update\n",
    "        새로운 state로 update하며 몇 가지 book-keeping을 실시\n",
    "        즉, 상태를 해제하고 완전히 padding된 state가 업데이트되지 않도록 함\n",
    "        마지막으로 graph가 매 batch iteration후에 gc되도록 계산 그래프에서 \n",
    "        state variable을 떼어냄.\n",
    "        \"\"\"\n",
    "        # TODO(Mark)L seems weird to sort here, but append zeros in the subclasses.\n",
    "        # which way around is best?\n",
    "        new_unsorted_states = [state.index_select(1, restoration_indices)\n",
    "                               for state in final_states]\n",
    "        \n",
    "        if self._states is None:\n",
    "            # We don't already have states, so just set the\n",
    "            # ones we receive to be the current state.\n",
    "            self._states = tuple([Variable(state.data) \n",
    "                                  for state in new_unsorted_states])\n",
    "        else:\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            # Now we've sorted the states back so that they correspond to the original\n",
    "            # indices, we need to figure out what states we need to update, because if we\n",
    "            # didn't use a state for a particular row, we want to preserve its state.\n",
    "            # Thankfully, the rows which are all zero in the state correspond exactly\n",
    "            # to those which aren't used, so we create masks of shape (new_batch_size,),\n",
    "            # denoting which states were used in the RNN computation.\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            # Masks for the unused states of shape (1, new_batch_size, 1)\n",
    "            used_new_rows_mask = [(state[0, :, :].sum(-1)\n",
    "                                   != 0.0).float().view(1, new_state_batch_size, 1)\n",
    "                                  for state in new_unsorted_states]\n",
    "            new_states = []\n",
    "            if current_state_batch_size > new_state_batch_size:\n",
    "                # The new state is smaller than the old one,\n",
    "                # so just update the indices which we used.\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows in the previous state\n",
    "                    # which _were_ used in the current state.\n",
    "                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(old_state.data))\n",
    "            else:\n",
    "                # The states are the same size, so we just have to\n",
    "                # deal with the possibility that some rows weren't used.\n",
    "                new_states = []\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows which _were_ used in the current state.\n",
    "                    masked_old_state = old_state * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    new_state += masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(new_state.data))\n",
    "\n",
    "            # It looks like there should be another case handled here - when\n",
    "            # the current_state_batch_size < new_state_batch_size. However,\n",
    "            # this never happens, because the states themeselves are mutated\n",
    "            # by appending zeros when calling _get_inital_states, meaning that\n",
    "            # the new states are either of equal size, or smaller, in the case\n",
    "            # that there are some unused elements (zero-length) for the RNN computation.\n",
    "            self._states = tuple(new_states)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'elmo',\n",
       " 'projection_dim': 512,\n",
       " 'cell_clip': 3,\n",
       " 'proj_clip': 3,\n",
       " 'dim': 4096,\n",
       " 'n_layers': 2}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout_mask(dropout_probability: float,\n",
    "                     tensor_for_masking: Variable):\n",
    "    binary_mask = tensor_for_masking.clone()\n",
    "    binary_mask.data.copy_(torch.rand(tensor_for_masking.size()) > dropout_probability)\n",
    "    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)\n",
    "    return dropout_mask\n",
    "\n",
    "def block_orthogonal(tensor: torch.Tensor,\n",
    "                     split_sizes: List[int],\n",
    "                     gain: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    An initializer which allows initaliizing model parametes in \"block\".\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, Variable):\n",
    "    # in pytorch 4.0, Variable equals Tensor\n",
    "    #     block_orthogonal(tensor.data, split_sizes, gain)\n",
    "    # else:\n",
    "        sizes = list(tensor.size())\n",
    "        if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n",
    "            raise ConfigurationError(\n",
    "                \"tensor dimentions must be divisible by their respective \"\n",
    "                f\"split_sizes. Found size: {size} and split_sizes: {split_sizes}\")\n",
    "        indexes = [list(range(0, max_size, split))\n",
    "                   for max_size, split in zip(sizes, split_sizes)]\n",
    "        # Iterate over all possible blocks within the tensor.\n",
    "        for block_start_indices in itertools.product(*indexes):\n",
    "            index_and_step_tuples = zip(block_start_indices, split_sizes)\n",
    "            block_slice = tuple([slice(start_index, start_index + step)\n",
    "                                 for start_index, step in index_and_step_tuples])\n",
    "            tensor[block_slice] = nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCellWithProjection(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELMobiLM __init__\n",
    "\n",
    "# super.__init__()\n",
    "stateful = True\n",
    "_states = None\n",
    "config = config\n",
    "use_cuda = use_cuda\n",
    "input_size = config['encoder']['projection_dim']\n",
    "hidden_size = config['encoder']['projection_dim']\n",
    "cell_size = config['encoder']['dim']\n",
    "num_layers = config['encoder']['n_layers']\n",
    "memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "state_projection_clilp_value = config['encoder']['proj_clip']\n",
    "recurrent_dropout_probability = config['dropout']\n",
    "\n",
    "forward_layers = []\n",
    "backwards_layers = []\n",
    "\n",
    "lstm_input_size = input_size\n",
    "go_forward = True\n",
    "for layer_index in range(num_layers):\n",
    "    forward_layer = LSTMCellWithProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 3, num_valid = 3\n",
      "sequence_lengths = tensor([ 7,  6, 10], device='cuda:0')\n",
      "1. sorted_inputs = \n",
      "tensor([[[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
      "         [ 0.0346, -0.0736,  0.0202,  ...,  0.0988,  0.0028,  0.0412],\n",
      "         [ 0.0437, -0.0743,  0.0047,  ...,  0.1194, -0.0323,  0.0387],\n",
      "         ...,\n",
      "         [ 0.0363, -0.0648,  0.0206,  ...,  0.1120, -0.0184,  0.0414],\n",
      "         [ 0.0597, -0.0749,  0.0128,  ...,  0.1011, -0.0258,  0.0474],\n",
      "         [ 0.0351, -0.0268,  0.0019,  ...,  0.0816,  0.0207,  0.0365]],\n",
      "\n",
      "        [[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
      "         [ 0.0684, -0.0786,  0.0208,  ...,  0.0929,  0.0057,  0.0524],\n",
      "         [ 0.0423, -0.0481, -0.0020,  ...,  0.1149,  0.0152,  0.0592],\n",
      "         ...,\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459]],\n",
      "\n",
      "        [[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
      "         [ 0.0618, -0.0532, -0.0128,  ...,  0.0917, -0.0056,  0.0506],\n",
      "         [ 0.0441, -0.0607,  0.0049,  ...,  0.1335, -0.0122,  0.0597],\n",
      "         ...,\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459],\n",
      "         [ 0.0064, -0.0216,  0.0173,  ...,  0.0550,  0.0039,  0.0459]]],\n",
      "       device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "2. sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "3. restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "4. sorting_indices = tensor([2, 0, 1], device='cuda:0')\n",
      "             sorted_inputs.shape = torch.Size([3, 10, 512])\n",
      "packed_sequence_input.data.shape = torch.Size([23, 512])\n",
      "packed_sequence_input.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# .sort_and_run_forward()\n",
    "batch_size = mask.size(0)\n",
    "num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "print(f\"batch_size = {batch_size}, num_valid = {num_valid}\")\n",
    "\n",
    "sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "print(f\"sequence_lengths = {sequence_lengths}\")\n",
    "\n",
    "sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n",
    "    sort_batch_by_length(token_embedding, sequence_lengths)\n",
    "print(f\"1. sorted_inputs = \\n{sorted_inputs}\")\n",
    "print(f\"2. sorted_sequence_lengths = {sorted_sequence_lengths}\")\n",
    "print(f\"3. restoration_indices = {restoration_indices}\")\n",
    "print(f\"4. sorting_indices = {sorting_indices}\")\n",
    "packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                             sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                             batch_first=True)\n",
    "print(f\"             sorted_inputs.shape = {sorted_inputs.shape}\")\n",
    "print(f\"packed_sequence_input.data.shape = {packed_sequence_input.data.shape}\")\n",
    "print(f\"packed_sequence_input.batch_sizes = {packed_sequence_input.batch_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0435, 0.0346, 0.0437, 0.0677, 0.0203, 0.0358, 0.0473, 0.0363, 0.0597,\n",
       "         0.0351],\n",
       "        [0.0435, 0.0684, 0.0423, 0.0844, 0.0396, 0.0619, 0.0351, 0.0064, 0.0064,\n",
       "         0.0064],\n",
       "        [0.0435, 0.0618, 0.0441, 0.0633, 0.0835, 0.0351, 0.0064, 0.0064, 0.0064,\n",
       "         0.0064]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_inputs[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
       "        [ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
       "        [ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
       "        ...,\n",
       "        [ 0.0363, -0.0648,  0.0206,  ...,  0.1120, -0.0184,  0.0414],\n",
       "        [ 0.0597, -0.0749,  0.0128,  ...,  0.1011, -0.0258,  0.0474],\n",
       "        [ 0.0351, -0.0268,  0.0019,  ...,  0.0816,  0.0207,  0.0365]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_sequence_input.data[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 by 10 matrix with pad token which has value 0.0064\n",
    "```\n",
    "[\n",
    "    [0.0435, 0.0346, 0.0437, 0.0677, 0.0203, 0.0358, 0.0473, 0.0363, 0.0597, 0.0351],\n",
    "    [0.0435, 0.0684, 0.0423, 0.0844, 0.0396, 0.0619, 0.0351, 0.0064, 0.0064, 0.0064],\n",
    "    [0.0435, 0.0618, 0.0441, 0.0633, 0.0835, 0.0351, 0.0064, 0.0064, 0.0064, 0.0064]\n",
    "]\n",
    "```\n",
    "- pad token visualization\n",
    "```\n",
    "[\n",
    "    [0.0435, 0.0346, 0.0437, 0.0677, 0.0203, 0.0358, 0.0473, 0.0363, 0.0597, 0.0351],\n",
    "    [0.0435, 0.0684, 0.0423, 0.0844, 0.0396, 0.0619, 0.0351, -.----, -.----, -.----],\n",
    "    [0.0435, 0.0618, 0.0441, 0.0633, 0.0835, 0.0351, -.----, -.----, -.----, -.----]\n",
    "]\n",
    "```\n",
    "- count non-padding value (batch_sizes)\n",
    "```\n",
    "[\n",
    "    [-----3, -----3, -----3, -----3, -----3, -----3, -----2, -----1, -----1, -----1]\n",
    "]\n",
    "```\n",
    "- Extract pad token, and then merge data\n",
    "```\n",
    "[\n",
    "    0.0435, 0.0435, 0.0435,   # 3\n",
    "    0.0346, 0.0684, 0.0618,   # 3\n",
    "    0.0437, 0.0423, 0.0441,   # 3\n",
    "    0.0677, 0.0844, 0.0633,   # 3\n",
    "    0.0203, 0.0396, 0.0835,   # 3\n",
    "    0.0358, 0.0619, 0.0351,   # 3\n",
    "    0.0473, 0.0351,           # 2\n",
    "    0.0363,                   # 1\n",
    "    0.0597,                   # 1\n",
    "    0.0351                    # 1\n",
    "]\n",
    "```\n",
    "- \\* 512 Dimensions\n",
    "```\n",
    "tensor([[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
    "        [ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
    "        [ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
    "        ...,\n",
    "        [ 0.0363, -0.0648,  0.0206,  ...,  0.1120, -0.0184,  0.0414],\n",
    "        [ 0.0597, -0.0749,  0.0128,  ...,  0.1011, -0.0258,  0.0474],\n",
    "        [ 0.0351, -0.0268,  0.0019,  ...,  0.0816,  0.0207,  0.0365]],\n",
    "       device='cuda:0', grad_fn=<SliceBackward>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules.elmo.py\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.autograd import Variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
