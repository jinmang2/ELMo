{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# *~ coding convention ~*\n",
    "from overrides import overrides\n",
    "from typing import Callable\n",
    "\n",
    "# Python Standard Library\n",
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Python Installed Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction: dict to namedtuple\n",
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)\n",
    "\n",
    "# input your directories path\n",
    "model_dir = 'C:\\workspace\\implement_elmo\\elmo\\configs'\n",
    "args2 = dict2namedtuple(\n",
    "    json.load(\n",
    "        codecs.open(\n",
    "            os.path.join(model_dir, 'config.json'), \n",
    "            'r', encoding='utf-8')\n",
    "    )\n",
    ")\n",
    "\n",
    "# args2.config_path == 'cnn_50_100_512_4096_sample.json'\n",
    "\n",
    "# load config\n",
    "with open(os.path.join(model_dir, args2.config_path), 'r') as fin:\n",
    "    config = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = torch.load('token_embedding.pt') \n",
    "masks = [torch.load(f'mask[{ix}]') for ix in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
       "         [ 0.0433, -0.1056,  0.0312,  ..., -0.0326,  0.0076, -0.0091],\n",
       "         [ 0.0530, -0.0870,  0.0403,  ..., -0.0145,  0.0064,  0.0179],\n",
       "         ...,\n",
       "         [-0.0089, -0.0576,  0.0266,  ...,  0.0237,  0.0107, -0.0232],\n",
       "         [-0.0089, -0.0576,  0.0266,  ...,  0.0237,  0.0107, -0.0232],\n",
       "         [-0.0089, -0.0576,  0.0266,  ...,  0.0237,  0.0107, -0.0232]],\n",
       "\n",
       "        [[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
       "         [ 0.0273, -0.0663,  0.0425,  ..., -0.0234,  0.0174, -0.0203],\n",
       "         [ 0.0412, -0.0818,  0.0333,  ..., -0.0363,  0.0524,  0.0140],\n",
       "         ...,\n",
       "         [-0.0089, -0.0576,  0.0266,  ...,  0.0237,  0.0107, -0.0232],\n",
       "         [-0.0089, -0.0576,  0.0266,  ...,  0.0237,  0.0107, -0.0232],\n",
       "         [-0.0089, -0.0576,  0.0266,  ...,  0.0237,  0.0107, -0.0232]],\n",
       "\n",
       "        [[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
       "         [ 0.0230, -0.0824,  0.0369,  ..., -0.0135,  0.0166, -0.0028],\n",
       "         [ 0.0258, -0.0843,  0.0384,  ..., -0.0171,  0.0255,  0.0179],\n",
       "         ...,\n",
       "         [ 0.0468, -0.0994,  0.0035,  ..., -0.0169,  0.0224,  0.0065],\n",
       "         [ 0.0604, -0.0773,  0.0357,  ..., -0.0188,  0.0194,  0.0118],\n",
       "         [ 0.0247, -0.1005,  0.0432,  ..., -0.0369,  0.0135,  0.0009]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " tensor([ 0,  1,  2,  3,  4,  5, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28], device='cuda:0'),\n",
       " tensor([ 1,  2,  3,  4,  5,  6, 11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28, 29], device='cuda:0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _EncoderBase\n",
    "stateful = False\n",
    "_states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder': {'name': 'elmo',\n",
       "  'projection_dim': 512,\n",
       "  'cell_clip': 3,\n",
       "  'proj_clip': 3,\n",
       "  'dim': 4096,\n",
       "  'n_layers': 2},\n",
       " 'token_embedder': {'name': 'cnn',\n",
       "  'activation': 'relu',\n",
       "  'filters': [[1, 32],\n",
       "   [2, 32],\n",
       "   [3, 64],\n",
       "   [4, 128],\n",
       "   [5, 256],\n",
       "   [6, 512],\n",
       "   [7, 1024]],\n",
       "  'n_highway': 2,\n",
       "  'word_dim': 100,\n",
       "  'char_dim': 50,\n",
       "  'max_characters_per_token': 50},\n",
       " 'classifier': {'name': 'sampled_softmax', 'n_samples': 8192},\n",
       " 'dropout': 0.1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size = 512\n",
      "hidden_size = 512\n",
      "cell_size = 4096\n",
      "num_layers = 2\n",
      "memory_cell_clip_value = 3\n",
      "state_projection_clip_value = 3\n",
      "recurrent_dropout_probability = 0.1\n"
     ]
    }
   ],
   "source": [
    "input_size = config['encoder']['projection_dim']\n",
    "hidden_size = config['encoder']['projection_dim']\n",
    "cell_size = config['encoder']['dim']\n",
    "num_layers = config['encoder']['n_layers']\n",
    "memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "state_projection_clip_value = config['encoder']['proj_clip']\n",
    "recurrent_dropout_probability = config['dropout']\n",
    "\n",
    "print(f\"input_size = {input_size}\")\n",
    "print(f\"hidden_size = {hidden_size}\")\n",
    "print(f\"cell_size = {cell_size}\")\n",
    "print(f\"num_layers = {num_layers}\")\n",
    "print(f\"memory_cell_clip_value = {memory_cell_clip_value}\")\n",
    "print(f\"state_projection_clip_value = {state_projection_clip_value}\")\n",
    "print(f\"recurrent_dropout_probability = {config['dropout']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_layers = []\n",
    "backward_layers = []\n",
    "\n",
    "lstm_input_size = input_size\n",
    "go_forward = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable\n",
    "import logging\n",
    "import itertools\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def get_lengths_from_binary_sequence_mask(mask: torch.Tensor):\n",
    "    return mask.long().sum(-1)\n",
    "\n",
    "def get_dropout_mask(dropout_probability: float,\n",
    "                     tensor_for_masking: Variable):\n",
    "    binary_mask = tensor_for_masking.clone()\n",
    "    binary_mask.data.copy_(torch.rand(tensor_for_masking.size()) > dropout_probability)\n",
    "    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)\n",
    "    return dropout_mask\n",
    "\n",
    "def block_orthogonal(tensor: torch.Tensor,\n",
    "                     split_sizes: List[int],\n",
    "                     gain: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    An initializer which allows initaliizing model parametes in \"block\".\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, Variable):\n",
    "    # in pytorch 4.0, Variable equals Tensor\n",
    "    #     block_orthogonal(tensor.data, split_sizes, gain)\n",
    "    # else:\n",
    "        sizes = list(tensor.size())\n",
    "        if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n",
    "            raise ConfigurationError(\n",
    "                \"tensor dimentions must be divisible by their respective \"\n",
    "                f\"split_sizes. Found size: {size} and split_sizes: {split_sizes}\")\n",
    "        indexes = [list(range(0, max_size, split))\n",
    "                   for max_size, split in zip(sizes, split_sizes)]\n",
    "        # Iterate over all possible blocks within the tensor.\n",
    "        for block_start_indices in itertools.product(*indexes):\n",
    "            index_and_step_tuples = zip(block_start_indices, split_sizes)\n",
    "            block_slice = tuple([slice(start_index, start_index + step)\n",
    "                                 for start_index, step in index_and_step_tuples])\n",
    "            tensor[block_slice] = nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)\n",
    "            \n",
    "def sort_batch_by_length(tensor: torch.autograd.Variable,\n",
    "                         sequence_lengths: torch.autograd.Variable):\n",
    "    if not isinstance(tensor, Variable) or not isinstance(sequence_lengths, Variable):\n",
    "        raise Exception(\"Both the tensor and sequence lengths must be torch.autograd.Variables.\")\n",
    "        \n",
    "    sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n",
    "    sorted_tensor = tensor.index_select(0, permutation_index)\n",
    "    \n",
    "    # This is ugly, but required - we are creating a new variable at runtime, so we\n",
    "    # must ensure it has the correct CUDA vs non-CUDA type. We do this by cloning and\n",
    "    # refilling one of the inputs to the function.\n",
    "    index_range = sequence_lengths.data.clone().copy_(torch.arange(0, len(sequence_lengths)))\n",
    "    # This is the equivalent of zipping with index, sorting by the original\n",
    "    # sequence lengths and returning the now sorted indices.\n",
    "    index_range = Variable(index_range.long())\n",
    "    _, reverse_mapping = permutation_index.sort(0, descending=False)\n",
    "    restoration_indices = index_range.index_select(0, reverse_mapping)\n",
    "    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아직 코드 리뷰안한 코드!\n",
    "from typing import Optional, Tuple, List, Callable, Union\n",
    "\n",
    "import h5py\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# We have two types here for the state, because storing the state in something\n",
    "# which is Iterable (like a tuple, below), is helpful for internal manipulation\n",
    "# - however, the states are consumed as either Tensors or a Tuple of Tensors, so\n",
    "# returning them in this format is unhelpful.\n",
    "RnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]  # pylint: disable=invalid-name\n",
    "RnnStateStorage = Tuple[torch.Tensor, ...]  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "class _EncoderBase(nn.Module):\n",
    "    # pyling: disable=abstract-method\n",
    "    \"\"\"\n",
    "    This abstract class serves as a base for the 3 ``Encoder`` abstractions in AllenNLP.\n",
    "    - :class:`~allennlp.modules.seq2seq_encoders.Seq2SeqEncoders`\n",
    "    - :class:`~allennlp.modules.seq2vec_encoders.Seq2VecEncoders`\n",
    "    Additionally, this class provides functionality for sorting sequences by length\n",
    "    so they can be consumed by Pytorch RNN classes, which require their inputs to be\n",
    "    sorted by length. Finally, it also provides optional statefulness to all of it's\n",
    "    subclasses by allowing the caching and retrieving of the hidden states of RNNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, stateful: bool = False) -> None:\n",
    "        super(_EncoderBase, self).__init__()\n",
    "        self.stateful = stateful\n",
    "        self._states: Optional[RnnStateStorage] = None\n",
    "\n",
    "    def sort_and_run_forward(self,\n",
    "                             module: Callable[[PackedSequence, Optional[RnnState]],\n",
    "                                              Tuple[Union[PackedSequence, torch.Tensor], RnnState]],\n",
    "                             inputs: torch.Tensor,\n",
    "                             mask: torch.Tensor,\n",
    "                             hidden_state: Optional[RnnState] = None):\n",
    "        \"\"\"\n",
    "        Pytorch RNNs는 input이 passing되기 전에 정렬되있어야 함\n",
    "        Seq2xxxEncoders가 이러한 기능을 모두 사용하기에 base class로 제공\n",
    "        \"\"\"\n",
    "        # In some circumstances you may have sequences of zero length. ``pack_padded_sequence``\n",
    "        # requires all sequence lengths to be > 0, so remove sequences of zero length before\n",
    "        # calling self._module, then fill with zeros.\n",
    "\n",
    "        # First count how many sequences are empty.\n",
    "        batch_size = mask.size(0)\n",
    "        num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "\n",
    "        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n",
    "            sort_batch_by_length(inputs, sequence_lengths)\n",
    "\n",
    "        # Now create a PackedSequence with only the non-empty, sorted sequences.\n",
    "        # pad token 제외, 유의미한 값들만 packing\n",
    "        packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                                     sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                                     batch_first=True)\n",
    "\n",
    "        # Prepare teh initial states.\n",
    "        if not self.stateful:\n",
    "            if hidden_state == None:\n",
    "                initial_states = hidden_state\n",
    "            elif isinstance(hidden_state, tuple):\n",
    "                initial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :]\n",
    "                                  for state in hidden_state]\n",
    "            else:\n",
    "                initial_stats = self._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "        else:\n",
    "            initial_states = self._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "\n",
    "        # Actually call the module on the sorted PackedSequence\n",
    "        module_output, final_states = module(packed_sequence_input, initial_states)\n",
    "\n",
    "        return module_output, final_states, restoration_indices\n",
    "\n",
    "    def _get_initial_states(self,\n",
    "                            batch_size: int,\n",
    "                            num_valid: int,\n",
    "                            sorting_indices: torch.LongTensor) -> Optional[RnnState]:\n",
    "        \"\"\"\n",
    "        RNN의 초기 상태를 반환\n",
    "        추가적으로, 이 메서드는 batch의 새로운 요소의 초기 상태를 추가하기 위해 상태를 변경하여(mutate)\n",
    "            호출시 batch size를 처리\n",
    "        또한 이 메서드는\n",
    "            1. 배치의 요소 seq. length로 상태를 정렬하는 것과\n",
    "            2. pad가 끝난 row 제거도 처리\n",
    "        중요한 것은 현재의 배치 크기가 이전에 호출되었을 때보다 더 크면 이 상태를 \"혼합\"하는 것이다.\n",
    "\n",
    "        이 메서드는 (1) 처음 호출되어 아무 상태가 없는 경우 (2) RNN이 heterogeneous state를 가질 때\n",
    "        의 경우를 처리해야 하기 때문에 return값이 복잡함\n",
    "\n",
    "        (1) module이 처음 호출됬을 때 ``module``의 타입이 무엇이든 ``None`` 반환\n",
    "        (2) Otherwise,\n",
    "            - LSTM의 경우 tuple of ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "                 and ``(num_layers, num_valid, memory_size)``\n",
    "            - GRU의 경우  single ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "        \"\"\"\n",
    "        # We don't know the state sizes the first time calling forward,\n",
    "        # so we let the module define what it's initial hidden state looks like.\n",
    "        if self._states is None:\n",
    "            return None\n",
    "\n",
    "        # Otherwise, we have some previous states.\n",
    "        if batch_size > self._states[0].size(1):\n",
    "            # This batch is larger than the all previous states.\n",
    "            # If so, resize the states.\n",
    "            num_states_to_concat = batch_size - self._states[0].size(1)\n",
    "            resized_states = []\n",
    "            # state has shape (num_layers, batch_size, hidden_size)\n",
    "            for state in self._states:\n",
    "                # This _must_ be inside the loop because some\n",
    "                # RNNs have states with different last dimension sizes.\n",
    "                zeros = state.data.new(state.size(0),\n",
    "                                       num_states_to_concat,\n",
    "                                       state.size(2)).fill_(0)\n",
    "                zeros = Variable(zeros)\n",
    "                resized_states.append(torch.cat([state, zeros], 1))\n",
    "            self._states = tuple(resized_states)\n",
    "            correctly_shaped_states = self._states\n",
    "        elif batch_size < self._states[0].size(1):\n",
    "            # This batch is smaller than the previous one.\n",
    "            correctly_shaped_states = tuple(staet[:, :batch_size, :] for state in self._states)\n",
    "        else:\n",
    "            correctly_shaped_states = self._states\n",
    "\n",
    "        # At this point, out states are of shape (num_layers, batch_size, hidden_size).\n",
    "        # However, the encoder uses sorted sequences and additionally removes elements\n",
    "        # of the batch which are fully padded. We need the states to match up to these\n",
    "        # sorted and filtered sequences, so we do that in the next two blocks before\n",
    "        # returning the states.\n",
    "        if len(self._states) == 1:\n",
    "            # GRU\n",
    "            correctly_shaped_state = correctly_shaped_states[0]\n",
    "            sorted_state = correctly_shaped_state.index_select(1, sorting_indices)\n",
    "            return sorted_state[:, :num_valid, :]\n",
    "        else:\n",
    "            # LSTM\n",
    "            sorted_states = [state.index_select(1, sorting_indices)\n",
    "                             for state in correctly_shaped_states]\n",
    "            return tuple(state[:, :num_valid, :] for state in sorted_states)\n",
    "\n",
    "    def _update_states(self,\n",
    "                       final_states: RnnStateStorage,\n",
    "                       restoration_indices: torch.LongTensor) -> None:\n",
    "        \"\"\"\n",
    "        RNN forward 동작 후에 state를 update\n",
    "        새로운 state로 update하며 몇 가지 book-keeping을 실시\n",
    "        즉, 상태를 해제하고 완전히 padding된 state가 업데이트되지 않도록 함\n",
    "        마지막으로 graph가 매 batch iteration후에 gc되도록 계산 그래프에서\n",
    "        state variable을 떼어냄.\n",
    "        \"\"\"\n",
    "        # TODO(Mark)L seems weird to sort here, but append zeros in the subclasses.\n",
    "        # which way around is best?\n",
    "        new_unsorted_states = [state.index_select(1, restoration_indices)\n",
    "                               for state in final_states]\n",
    "\n",
    "        if self._states is None:\n",
    "            # We don't already have states, so just set the\n",
    "            # ones we receive to be the current state.\n",
    "            self._states = tuple([Variable(state.data)\n",
    "                                  for state in new_unsorted_states])\n",
    "        else:\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            # Now we've sorted the states back so that they correspond to the original\n",
    "            # indices, we need to figure out what states we need to update, because if we\n",
    "            # didn't use a state for a particular row, we want to preserve its state.\n",
    "            # Thankfully, the rows which are all zero in the state correspond exactly\n",
    "            # to those which aren't used, so we create masks of shape (new_batch_size,),\n",
    "            # denoting which states were used in the RNN computation.\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            # Masks for the unused states of shape (1, new_batch_size, 1)\n",
    "            used_new_rows_mask = [(state[0, :, :].sum(-1)\n",
    "                                   != 0.0).float().view(1, new_state_batch_size, 1)\n",
    "                                  for state in new_unsorted_states]\n",
    "            new_states = []\n",
    "            if current_state_batch_size > new_state_batch_size:\n",
    "                # The new state is smaller than the old one,\n",
    "                # so just update the indices which we used.\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows in the previous state\n",
    "                    # which _were_ used in the current state.\n",
    "                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(old_state.data))\n",
    "            else:\n",
    "                # The states are the same size, so we just have to\n",
    "                # deal with the possibility that some rows weren't used.\n",
    "                new_states = []\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows which _were_ used in the current state.\n",
    "                    masked_old_state = old_state * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    new_state += masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(new_state.data))\n",
    "\n",
    "            # It looks like there should be another case handled here - when\n",
    "            # the current_state_batch_size < new_state_batch_size. However,\n",
    "            # this never happens, because the states themeselves are mutated\n",
    "            # by appending zeros when calling _get_inital_states, meaning that\n",
    "            # the new states are either of equal size, or smaller, in the case\n",
    "            # that there are some unused elements (zero-length) for the RNN computation.\n",
    "            self._states = tuple(new_states)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._states = None\n",
    "\n",
    "\n",
    "class ElmobiLm(_EncoderBase):\n",
    "    def __init__(self, config, use_cuda=False):\n",
    "        super(ElmobiLm, self).__init__(stateful=True)\n",
    "        self.config = config\n",
    "        self.use_cuda = use_cuda\n",
    "        input_size = config['encoder']['projection_dim']\n",
    "        hidden_size = config['encoder']['projection_dim']\n",
    "        cell_size = config['encoder']['dim']\n",
    "        num_layers = config['encoder']['n_layers']\n",
    "        memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "        state_projection_clip_value = config['encoder']['proj_clip']\n",
    "        recurrent_dropout_probability = config['dropout']\n",
    "        \n",
    "        print('ELMo biLM layer params')\n",
    "        print(f\"\\tinput_size = {input_size}\")\n",
    "        print(f\"\\thidden_size = {hidden_size}\")\n",
    "        print(f\"\\tcell_size = {cell_size}\")\n",
    "        print(f\"\\tnum_layers = {num_layers}\")\n",
    "        print(f\"\\tmemory_cell_clip_value = {memory_cell_clip_value}\")\n",
    "        print(f\"\\tstate_projection_clip_value = {state_projection_clip_value}\")\n",
    "#         print(f\"\\trecurrent_dropout_probability = {config['dropout']}\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        forward_layers = []\n",
    "        backward_layers = []\n",
    "\n",
    "        lstm_input_size = input_size\n",
    "        go_forward = True\n",
    "        for layer_index in range(num_layers):\n",
    "            forward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                                   hidden_size,\n",
    "                                                   cell_size,\n",
    "                                                   go_forward,\n",
    "                                                   recurrent_dropout_probability,\n",
    "                                                   memory_cell_clip_value,\n",
    "                                                   state_projection_clip_value).cuda()\n",
    "            backward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                                    hidden_size,\n",
    "                                                    cell_size,\n",
    "                                                    not go_forward,\n",
    "                                                    recurrent_dropout_probability,\n",
    "                                                    memory_cell_clip_value,\n",
    "                                                    state_projection_clip_value).cuda()\n",
    "            if use_cuda:\n",
    "                forward_layer = forward_layer.cuda()\n",
    "                backward_layer = backward_layer.cuda()\n",
    "            lstm_input_size = hidden_size\n",
    "\n",
    "            self.add_module('forward_layer_{}'.format(layer_index), forward_layer)\n",
    "            self.add_module('backward_layer_{}'.format(layer_index), backward_layer)\n",
    "            forward_layers.append(forward_layer)\n",
    "            backward_layers.append(backward_layer)\n",
    "        self.forward_layers = forward_layers\n",
    "        self.backward_layers = backward_layers\n",
    "        print(f\"forward_layers = {forward_layers}\")\n",
    "        print(f\"backward_layers = {backward_layers}\")\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        print('FORWARD!!!!**************')\n",
    "        batch_size, total_sequence_length = mask.size()\n",
    "        print(f\"batch_size = {batch_size}\")\n",
    "        print(f\"total_sequence_length = {total_sequence_length}\")\n",
    "        stacked_sequence_output, final_states, restoration_indices = \\\n",
    "            self.sort_and_run_forward(self._lstm_forward, inputs, mask)\n",
    "        print(f\"stacked_sequence_output = {stacked_sequence_output}\")\n",
    "        print(f\"final_states = {final_states}\")\n",
    "        print(f\"restoration_indices = {restoration_indices}\")\n",
    "        num_layers, num_valid, returned_timesteps, encoder_dim = stacked_sequence_output.size()\n",
    "        # Add back invalid rows which were removed in the call to sort_and_run_forward.\n",
    "        if num_valid < batch_size:\n",
    "            zeros = stacked_sequence_output.data.new(num_layers,\n",
    "                                                     batch_size - num_valid,\n",
    "                                                     returned_timesteps,\n",
    "                                                     encoder_dim).fill_(0)\n",
    "            zeros = Variable(zeros)\n",
    "            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 1)\n",
    "\n",
    "            # The states also need to have invalid rows added back.\n",
    "            new_states = []\n",
    "            for state in final_states:\n",
    "                state_dim = state.size(-1)\n",
    "                zeros = state.data.new(num_layers, batch_size - num_valid, state_dim).fill_(0)\n",
    "                zeros = Variable(zeros)\n",
    "                new_states.append(torch.cat([state, zeros], 1))\n",
    "            final_states = new_states\n",
    "\n",
    "        # It's possible to need to pass sequences which are padded to longer than the\n",
    "        # max length of the sequence to a Seq2StackEncoder. However, packing and unpacking\n",
    "        # the sequences mean that the returned tensor won't include these dimensions, because\n",
    "        # the RNN did not need to process them. We add them back on in the form of zeros here.\n",
    "        sequence_length_difference = total_sequence_length - returned_timesteps\n",
    "        if sequence_length_difference > 0:\n",
    "            zeros = stacked_sequence_output.data.new(num_layers,\n",
    "                                                     batch_size,\n",
    "                                                     sequence_length_difference,\n",
    "                                                     stacked_sequence_output[0].size(-1)).fill_(0)\n",
    "            zeros = Variable(zeros)\n",
    "            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)\n",
    "\n",
    "        self._update_states(final_states, restoration_indices)\n",
    "\n",
    "        # Restore the original indices and return the sequence.\n",
    "        # Has shape (num_layers, batch_size, sequence_length, hidden_size)\n",
    "        return stacked_sequence_output.index_select(1, restoration_indices)\n",
    "\n",
    "\n",
    "    def _lstm_forward(self,\n",
    "                      inputs: PackedSequence,\n",
    "                      initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> \\\n",
    "        Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        if initial_state is None:\n",
    "            hidden_states: List[Optional[Tuple[torch.Tensor,\n",
    "                                         torch.Tensor]]] = [None] * len(self.forward_layers)\n",
    "        elif initial_state[0].size()[0] != len(self.forward_layers):\n",
    "            raise Exception(\"Initial states were passed to forward() but the number of \"\n",
    "                            \"initial states does not match the number of layers.\")\n",
    "        else:\n",
    "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
    "                                     initial_state[1].split(1, 0)))\n",
    "\n",
    "        inputs, batch_lengths = pad_packed_sequence(inputs, batch_first=True)\n",
    "        forward_output_sequence = inputs\n",
    "        backward_output_sequence = inputs\n",
    "\n",
    "        final_states = []\n",
    "        sequence_outputs = []\n",
    "        for layer_index, state in enumerate(hidden_states):\n",
    "            forward_layer = getattr(self, 'forward_layer_{}'.format(layer_index))\n",
    "            backward_layer = getattr(self, 'backward_layer_{}'.format(layer_index))\n",
    "\n",
    "            forward_cache = forward_output_sequence\n",
    "            backward_cache = backward_output_sequence\n",
    "\n",
    "        if state is not None:\n",
    "            forward_hidden_state, backward_hidden_state = state[0].split(self.hidden_size, 2)\n",
    "            forward_memory_state, backward_memory_state = state[1].split(self.cell_size, 2)\n",
    "            forward_state = (forward_hidden_state, forward_memory_state)\n",
    "            backward_state = (backward_hidden_state, backward_memory_state)\n",
    "        else:\n",
    "            forward_state = None\n",
    "            backward_state = None\n",
    "\n",
    "        forward_output_sequence, forward_state = forward_layer(forward_output_sequence,\n",
    "                                                             batch_lengths,\n",
    "                                                             forward_state)\n",
    "        backward_output_sequence, backward_state = backward_layer(backward_output_sequence,\n",
    "                                                                batch_lengths,\n",
    "                                                                backward_state)\n",
    "        # Skip connections, just adding the input to the output.\n",
    "        if layer_index != 0:\n",
    "            forward_output_sequence += forward_cache\n",
    "            backward_output_sequence += backward_cache\n",
    "\n",
    "        sequence_outputs.append(torch.cat([forward_output_sequence,\n",
    "                                           backward_output_sequence], -1))\n",
    "        # Append the state tuples in a list, so that we can return\n",
    "        # the final states for all the layers.\n",
    "        final_states.append((torch.cat([forward_state[0], backward_state[0]], -1),\n",
    "                           torch.cat([forward_state[1], backward_state[1]], -1)))\n",
    "\n",
    "        stacked_sequence_outputs: torch.FloatTensor = torch.stack(sequence_outputs)\n",
    "        # Stack the hidden state and memory for each layer into 2 tensors of shape\n",
    "        # (num_layers, batch_size, hidden_size) and (num_layers, batch_size, cell_size)\n",
    "        # respectively.\n",
    "        final_hidden_states, final_memory_states = zip(*final_states)\n",
    "        final_state_tuple: Tuple[torch.FloatTensor,\n",
    "                                 torch.FloatTensor] = (torch.cat(final_hidden_states, 0),\n",
    "                                                       torch.cat(final_memory_states, 0))\n",
    "        return stacked_sequence_outputs, final_state_tuple\n",
    "    \n",
    "class LstmCellWithProjection(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 cell_size: int,\n",
    "                 use_cuda: bool = False,\n",
    "                 go_forward: bool = True,\n",
    "                 recurrent_dropout_probability: float = 0.0,\n",
    "                 memory_cell_clip_value: Optional[float] = None,\n",
    "                 state_projection_clip_value: Optional[float] = None) -> None:\n",
    "        super(LstmCellWithProjection, self).__init__()\n",
    "        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`.\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        self.go_forward = go_forward\n",
    "        self.state_projection_clip_value = state_projection_clip_value\n",
    "        self.memory_cell_clip_value = memory_cell_clip_value\n",
    "        self.recurrent_dropout_probability = recurrent_dropout_probability\n",
    "\n",
    "        # We do the projections for all the gates all at once.\n",
    "        self.input_linearity = nn.Linear(input_size, 4 * cell_size, bias=False)\n",
    "        self.state_linearity = nn.Linear(hidden_size, 4 * cell_size, bias=True)\n",
    "\n",
    "        # Additional projection matrix for making the hidden state smaller.\n",
    "        self.state_projection = nn.Linear(cell_size, hidden_size, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Use sensible default initializations for parameters.\n",
    "        block_orthogonal(self.input_linearity.weight.data, [self.cell_size, self.input_size])\n",
    "        block_orthogonal(self.state_linearity.weight.data, [self.cell_size, self.hidden_size])\n",
    "\n",
    "        self.state_linearity.bias.data.fill_(0.0)\n",
    "        # Initialize forget gate biases to 1.0 as per An Empirical\n",
    "        # Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n",
    "        self.state_linearity.bias.data[self.cell_size:2 * self.cell_size].fill_(1.0)\n",
    "\n",
    "    def forward(self,  # pylint: disable=arguments-differ\n",
    "                inputs: torch.FloatTensor,\n",
    "                batch_lengths: List[int],\n",
    "                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        batch_size = inputs.size()[0]\n",
    "        total_timesteps = inputs.size()[1]\n",
    "\n",
    "        # We have to use this '.data.new().fill_' pattern to create tensors with the correct\n",
    "        # type - forward has no knowledge of whether these are torch.Tensors or torch.cuda.Tensors.\n",
    "        output_accumulator = Variable(inputs.data.new(batch_size,\n",
    "                                                      total_timesteps,\n",
    "                                                      self.hidden_size).fill_(0))\n",
    "        if initial_state is None:\n",
    "            full_batch_previous_memory = Variable(inputs.data.new(batch_size,\n",
    "                                                                  self.cell_size).fill_(0))\n",
    "            full_batch_previous_state = Variable(inputs.data.new(batch_size,\n",
    "                                                                 self.hidden_size).fill_(0))\n",
    "        else:\n",
    "            full_batch_previous_state = initial_state[0].squeeze(0)\n",
    "            full_batch_previous_memory = initial_state[1].squeeze(0)\n",
    "\n",
    "        current_length_index = batch_size - 1 if self.go_forward else 0\n",
    "        if self.recurrent_dropout_probability > 0.0 and self.training:\n",
    "            dropout_mask = get_dropout_mask(self.recurrent_dropout_probability,\n",
    "                                            full_batch_previous_state)\n",
    "        else:\n",
    "            dropout_mask = None\n",
    "\n",
    "        for timestep in range(total_timesteps):\n",
    "            # The index depends on which end we start.\n",
    "            index = timestep if self.go_forward else total_timesteps - timestep - 1\n",
    "\n",
    "            # What we are doing here is finding the index into the batch dimension\n",
    "            # which we need to use for this timestep, because the sequences have\n",
    "            # variable length, so once the index is greater than the length of this\n",
    "            # particular batch sequence, we no longer need to do the computation for\n",
    "            # this sequence. The key thing to recognise here is that the batch inputs\n",
    "            # must be _ordered_ by length from longest (first in batch) to shortest\n",
    "            # (last) so initially, we are going forwards with every sequence and as we\n",
    "            # pass the index at which the shortest elements of the batch finish,\n",
    "            # we stop picking them up for the computation.\n",
    "            if self.go_forward:\n",
    "                while batch_lengths[current_length_index] <= index:\n",
    "                    current_length_index -= 1\n",
    "            # If we're going backwards, we are _picking up_ more indices.\n",
    "            else:\n",
    "                # First conditional: Are we already at the maximum number of elements in the batch?\n",
    "                # Second conditional: Does the next shortest sequence beyond the current batch\n",
    "                # index require computation use this timestep?\n",
    "                while current_length_index < (len(batch_lengths) - 1) and \\\n",
    "                                batch_lengths[current_length_index + 1] > index:\n",
    "                    current_length_index += 1\n",
    "\n",
    "            # Actually get the slices of the batch which we\n",
    "            # need for the computation at this timestep.\n",
    "            # shape (batch_size, cell_size)\n",
    "            previous_memory = full_batch_previous_memory[0: current_length_index + 1].clone()\n",
    "            # Shape (batch_size, hidden_size)\n",
    "            previous_state = full_batch_previous_state[0: current_length_index + 1].clone()\n",
    "            # Shape (batch_size, input_size)\n",
    "            timestep_input = inputs[0: current_length_index + 1, index]\n",
    "\n",
    "            # Do the projections for all the gates all at once.\n",
    "            # Both have shape (batch_size, 4 * cell_size)\n",
    "            projected_input = self.input_linearity(timestep_input)\n",
    "            projected_state = self.state_linearity(previous_state)\n",
    "\n",
    "            # Main LSTM equations using relevant chunks of the big linear\n",
    "            # projections of the hidden state and inputs.\n",
    "            input_gate = torch.sigmoid(projected_input[:, (0 * self.cell_size):(1 * self.cell_size)] +\n",
    "                                       projected_state[:, (0 * self.cell_size):(1 * self.cell_size)])\n",
    "            forget_gate = torch.sigmoid(projected_input[:, (1 * self.cell_size):(2 * self.cell_size)] +\n",
    "                                        projected_state[:, (1 * self.cell_size):(2 * self.cell_size)])\n",
    "            memory_init = torch.tanh(projected_input[:, (2 * self.cell_size):(3 * self.cell_size)] +\n",
    "                                     projected_state[:, (2 * self.cell_size):(3 * self.cell_size)])\n",
    "            output_gate = torch.sigmoid(projected_input[:, (3 * self.cell_size):(4 * self.cell_size)] +\n",
    "                                        projected_state[:, (3 * self.cell_size):(4 * self.cell_size)])\n",
    "            memory = input_gate * memory_init + forget_gate * previous_memory\n",
    "\n",
    "            # Here is the non-standard part of this LSTM cell; first, we clip the\n",
    "            # memory cell, then we project the output of the timestep to a smaller size\n",
    "            # and again clip it.\n",
    "\n",
    "            if self.memory_cell_clip_value:\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                memory = torch.clamp(memory, -self.memory_cell_clip_value, self.memory_cell_clip_value)\n",
    "\n",
    "            # shape (current_length_index, cell_size)\n",
    "            pre_projection_timestep_output = output_gate * torch.tanh(memory)\n",
    "\n",
    "            # shape (current_length_index, hidden_size)\n",
    "            timestep_output = self.state_projection(pre_projection_timestep_output)\n",
    "            if self.state_projection_clip_value:\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                timestep_output = torch.clamp(timestep_output,\n",
    "                                              -self.state_projection_clip_value,\n",
    "                                              self.state_projection_clip_value)\n",
    "\n",
    "            # Only do dropout if the dropout prob is > 0.0 and we are in training mode.\n",
    "            if dropout_mask is not None:\n",
    "                timestep_output = timestep_output * dropout_mask[0: current_length_index + 1]\n",
    "\n",
    "            # We've been doing computation with less than the full batch, so here we create a new\n",
    "            # variable for the the whole batch at this timestep and insert the result for the\n",
    "            # relevant elements of the batch into it.\n",
    "            full_batch_previous_memory = Variable(full_batch_previous_memory.data.clone())\n",
    "            full_batch_previous_state = Variable(full_batch_previous_state.data.clone())\n",
    "            full_batch_previous_memory[0:current_length_index + 1] = memory\n",
    "            full_batch_previous_state[0:current_length_index + 1] = timestep_output\n",
    "            output_accumulator[0:current_length_index + 1, index] = timestep_output\n",
    "\n",
    "        # Mimic the pytorch API by returning state in the following shape:\n",
    "        # (num_layers * num_directions, batch_size, ...). As this\n",
    "        # LSTM cell cannot be stacked, the first dimension here is just 1.\n",
    "        final_state = (full_batch_previous_state.unsqueeze(0),\n",
    "                       full_batch_previous_memory.unsqueeze(0))\n",
    "\n",
    "        return output_accumulator, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo biLM layer params\n",
      "\tinput_size = 512\n",
      "\thidden_size = 512\n",
      "\tcell_size = 4096\n",
      "\tnum_layers = 2\n",
      "\tmemory_cell_clip_value = 3\n",
      "\tstate_projection_clip_value = 3\n",
      "forward_layers = [LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "), LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      ")]\n",
      "backward_layers = [LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "), LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "encoder = ElmobiLm(config, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(encoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD!!!!**************\n",
      "batch_size = 3\n",
      "total_sequence_length = 10\n",
      "stacked_sequence_output = tensor([[[[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
      "          [ 0.0230, -0.0824,  0.0369,  ..., -0.0135,  0.0166, -0.0028],\n",
      "          [ 0.0258, -0.0843,  0.0384,  ..., -0.0171,  0.0255,  0.0179],\n",
      "          ...,\n",
      "          [ 0.0468, -0.0994,  0.0035,  ..., -0.0169,  0.0224,  0.0065],\n",
      "          [ 0.0604, -0.0773,  0.0357,  ..., -0.0188,  0.0194,  0.0118],\n",
      "          [ 0.0247, -0.1005,  0.0432,  ..., -0.0369,  0.0135,  0.0009]],\n",
      "\n",
      "         [[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
      "          [ 0.0433, -0.1056,  0.0312,  ..., -0.0326,  0.0076, -0.0091],\n",
      "          [ 0.0530, -0.0870,  0.0403,  ..., -0.0145,  0.0064,  0.0179],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
      "          [ 0.0273, -0.0663,  0.0425,  ..., -0.0234,  0.0174, -0.0203],\n",
      "          [ 0.0412, -0.0818,  0.0333,  ..., -0.0363,  0.0524,  0.0140],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
      "       device='cuda:0', grad_fn=<StackBackward>)\n",
      "final_states = (tensor([[[0., 0., -0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., -0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., -0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward>), tensor([[[ 0.0349,  0.0161, -0.0015,  ..., -0.0316,  0.0153,  0.0112],\n",
      "         [ 0.0318,  0.0116, -0.0021,  ..., -0.0259,  0.0193,  0.0145],\n",
      "         [ 0.0248,  0.0094, -0.0015,  ..., -0.0303,  0.0160,  0.0151]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(token_embedding, Variable(masks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
       "          [ 0.0433, -0.1056,  0.0312,  ..., -0.0326,  0.0076, -0.0091],\n",
       "          [ 0.0530, -0.0870,  0.0403,  ..., -0.0145,  0.0064,  0.0179],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
       "          [ 0.0273, -0.0663,  0.0425,  ..., -0.0234,  0.0174, -0.0203],\n",
       "          [ 0.0412, -0.0818,  0.0333,  ..., -0.0363,  0.0524,  0.0140],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0282, -0.0787,  0.0546,  ..., -0.0039,  0.0065,  0.0014],\n",
       "          [ 0.0230, -0.0824,  0.0369,  ..., -0.0135,  0.0166, -0.0028],\n",
       "          [ 0.0258, -0.0843,  0.0384,  ..., -0.0171,  0.0255,  0.0179],\n",
       "          ...,\n",
       "          [ 0.0468, -0.0994,  0.0035,  ..., -0.0169,  0.0224,  0.0065],\n",
       "          [ 0.0604, -0.0773,  0.0357,  ..., -0.0188,  0.0194,  0.0118],\n",
       "          [ 0.0247, -0.1005,  0.0432,  ..., -0.0369,  0.0135,  0.0009]]]],\n",
       "       device='cuda:0', grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD!!!!**************\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Initial states were passed to forward() but the number of initial states does not match the number of layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-9fbbcd1b4ffe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c0d740eb1e9e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sequence_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mstacked_sequence_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_and_run_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lstm_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturned_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstacked_sequence_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c0d740eb1e9e>\u001b[0m in \u001b[0;36msort_and_run_forward\u001b[1;34m(self, module, inputs, mask, hidden_state)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# Actually call the module on the sorted PackedSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mmodule_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_sequence_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c0d740eb1e9e>\u001b[0m in \u001b[0;36m_lstm_forward\u001b[1;34m(self, inputs, initial_state)\u001b[0m\n\u001b[0;32m    336\u001b[0m                                          torch.Tensor]]] = [None] * len(self.forward_layers)\n\u001b[0;32m    337\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m             raise Exception(\"Initial states were passed to forward() but the number of \"\n\u001b[0m\u001b[0;32m    339\u001b[0m                             \"initial states does not match the number of layers.\")\n\u001b[0;32m    340\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Initial states were passed to forward() but the number of initial states does not match the number of layers."
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(token_embedding, Variable(masks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 10, 1024])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
