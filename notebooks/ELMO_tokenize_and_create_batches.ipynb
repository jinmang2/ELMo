{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import random\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'C:/workspace/ELMo/161/'\n",
    "args2 = dict2namedtuple(json.load(codecs.open(\n",
    "    os.path.join(model_dir, 'config.json'), 'r', \n",
    "    encoding='utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, args2.config_path), 'r') as fin:\n",
    "    config = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [['今', '天', '天氣', '真', '好', '阿'],\n",
    "['潮水', '退', '了', '就', '知道', '誰', '沒', '穿', '褲子']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chars = 50\n",
    "dataset, textset = [], []\n",
    "for sent in sents:\n",
    "    data = ['<bos>'] # begin of sentence\n",
    "    text = []\n",
    "    for token in sent:\n",
    "        text.append(token)\n",
    "        if max_chars is not None and len(token) + 2 > max_chars:\n",
    "            token = token[:max_chars - 2]\n",
    "        data.append(token)\n",
    "    data.append('<eos>') # end of sentence\n",
    "    dataset.append(data)\n",
    "    textset.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', '今', '天', '天氣', '真', '好', '阿', '<eos>'],\n",
       " ['<bos>', '潮水', '退', '了', '就', '知道', '誰', '沒', '穿', '褲子', '<eos>']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['今', '天', '天氣', '真', '好', '阿'],\n",
       " ['潮水', '退', '了', '就', '知道', '誰', '沒', '穿', '褲子']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder': {'name': 'elmo',\n",
       "  'projection_dim': 512,\n",
       "  'cell_clip': 3,\n",
       "  'proj_clip': 3,\n",
       "  'dim': 4096,\n",
       "  'n_layers': 2},\n",
       " 'token_embedder': {'name': 'cnn',\n",
       "  'activation': 'relu',\n",
       "  'filters': [[1, 32],\n",
       "   [2, 32],\n",
       "   [3, 64],\n",
       "   [4, 128],\n",
       "   [5, 256],\n",
       "   [6, 512],\n",
       "   [7, 1024]],\n",
       "  'n_highway': 2,\n",
       "  'word_dim': 100,\n",
       "  'char_dim': 50,\n",
       "  'max_characters_per_token': 50},\n",
       " 'classifier': {'name': 'sampled_softmax', 'n_samples': 8192},\n",
       " 'dropout': 0.1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the model trained with character-based word encoder.\n",
    "if config['token_embedder']['char_dim'] > 0:\n",
    "    char_lexicon = {}\n",
    "    with codecs.open(os.path.join(model_dir, 'char.dic'), 'r', encoding='utf-8') as fpi:\n",
    "        for line in fpi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            if len(tokens) == 1:\n",
    "                tokens.insert(0, '\\u3000')\n",
    "            token, i = tokens\n",
    "            char_lexicon[token] = int(i)\n",
    "#     char_emb_layer = EmbeddingLayer(\n",
    "#         config['token_embedder']['char_dim'], char_lexicon, fix_emb=False, embs=None)\n",
    "#     logging.info('char embedding size: ' +\n",
    "#                 str(len(char_emb_layer.word2id)))\n",
    "else:\n",
    "    char_lexicon = None\n",
    "    char_emb_layer = None\n",
    "\n",
    "# For the model trained with word form word encoder.\n",
    "if config['token_embedder']['word_dim'] > 0:\n",
    "    word_lexicon = {}\n",
    "    with codecs.open(os.path.join(model_dir, 'word.dic'), 'r', encoding='utf-8') as fpi:\n",
    "        for line in fpi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            if len(tokens) == 1:\n",
    "                tokens.insert(0, '\\u3000')\n",
    "            token, i = tokens\n",
    "            word_lexicon[token] = int(i)\n",
    "#     word_emb_layer = EmbeddingLayer(\n",
    "#         config['token_embedder']['word_dim'], word_lexicon, fix_emb=False, embs=None)\n",
    "#     logging.info('word embedding size: ' +\n",
    "#                 str(len(word_emb_layer.word2id)))\n",
    "else:\n",
    "    word_lexicon = None\n",
    "    word_emb_layer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = word_lexicon\n",
    "char2id = char_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset\n",
    "text = textset\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "x = test\n",
    "perm = None\n",
    "shuffle = False\n",
    "sort = True\n",
    "\n",
    "ind = list(range(len(x)))\n",
    "lst = perm or ind\n",
    "print(lst)\n",
    "if shuffle:\n",
    "    random.shuffle(lst)\n",
    "    \n",
    "if sort:\n",
    "    lst.sort(key=lambda l: -len(x[l]))\n",
    "    print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', '潮水', '退', '了', '就', '知道', '誰', '沒', '穿', '褲子', '<eos>'], ['<bos>', '今', '天', '天氣', '真', '好', '阿', '<eos>']]\n",
      "[0, 1]\n",
      "[['潮水', '退', '了', '就', '知道', '誰', '沒', '穿', '褲子'], ['今', '天', '天氣', '真', '好', '阿']]\n"
     ]
    }
   ],
   "source": [
    "x = [x[i] for i in lst]\n",
    "ind = [ind[i] for i in lst]\n",
    "if text is not None:\n",
    "    text = [text[i] for i in lst]\n",
    "\n",
    "print(x)\n",
    "print(ind)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_len = 0.0\n",
    "batches_w, batches_c, batches_lens, batches_masks, batches_text, batches_ind = [], [], [], [], [], []\n",
    "size = batch_size\n",
    "nbatch = (len(x) - 1) // size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov='<oov>'\n",
    "pad='<pad>'\n",
    "\n",
    "for i in range(nbatch):\n",
    "    start_id, end_id = i * size, (i + 1) * size\n",
    "    # Create one_batch---------------------------------------\n",
    "    x_b = x[start_id: end_id]\n",
    "    batch_size = len(x_b)\n",
    "    lst = list(range(batch_size))\n",
    "    if sort:\n",
    "        lst.sort(key=lambda l: -len(x[l]))\n",
    "    # shuffle the sentences by\n",
    "    x_b = [x_b[i] for i in lst]\n",
    "    lens = [len(x_b[i]) for i in lst]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # get a batch of word id whose size is (batch x max_len)\n",
    "    if word2id is not None:\n",
    "        oov_id = word2id.get(oov, None)\n",
    "        pad_id = word2id.get(pad, None)\n",
    "        assert oov_id is not None and pad_id is not None\n",
    "        batch_w = torch.LongTensor(batch_size, max_len).fill_(pad_id)\n",
    "        for i, x_i in enumerate(x_b):\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_w[i][j] = word2id.get(x_ij, oov_id)\n",
    "    else:\n",
    "        batch_w = None\n",
    "    \n",
    "    # get a batch of character id whose size is (batch x max_chars)\n",
    "    if char2id is not None:\n",
    "        bow_id, eow_id, oov_id, pad_id = [\n",
    "            char2id.get(key, None) \n",
    "            for key in ('<eow>', '<bow>', oov, pad)\n",
    "        ]\n",
    "        assert ((bow_id is not None) and \n",
    "                (eow_id is not None) and\n",
    "                (oov_id is not None) and\n",
    "                (pad_id is not None))\n",
    "        if config['token_embedder']['name'].lower() == 'cnn':\n",
    "            max_chars = config['token_embedder']['max_characters_per_token']\n",
    "            assert max([len(w) for i in lst for w in x_b[i]]) + 2 <= max_chars\n",
    "        elif config['token_embedder']['name'].lower() == 'lstm':\n",
    "            max_chars = max([len(w) for i in lst for w in x_b[i]]) + 2\n",
    "        else:\n",
    "            raise ValueError('Unknown token_embedder: {0}'.format(config['token_embedder']['name']))\n",
    "        batch_c = torch.LongTensor(batch_size, max_len, max_chars).fill_(pad_id)\n",
    "        for i, x_i in enumerate(x_b):\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_c[i][j][0] = bow_id\n",
    "                if x_ij in ['<bos>', '<eos>']:\n",
    "                    batch_c[i][j][1] = char2id.get(x_ij)\n",
    "                    batch_c[i][j][2] = eow_id\n",
    "                else:\n",
    "                    for k, c in enumerate(x_ij):\n",
    "                        batch_c[i][j][k+1] = char2id.get(c, oov_id)\n",
    "                    batch_c[i][j][len(x_ij)+1] = eow_id\n",
    "    else:\n",
    "        batch_c = None\n",
    "        \n",
    "    masks = [torch.LongTensor(batch_size, max_len).fill_(0), [], []]\n",
    "    \n",
    "    for i, x_i in enumerate(x_b):\n",
    "        for j in range(len(x_i)):\n",
    "            masks[0][i][j] = 1\n",
    "            if j + 1 < len(x_i):\n",
    "                masks[1].append(i * max_len + j)\n",
    "            if j > 0:\n",
    "                masks[2].append(i * max_len + j)\n",
    "\n",
    "    assert len(masks[1]) <= batch_size * max_len\n",
    "    assert len(masks[2]) <= batch_size * max_len\n",
    "\n",
    "    masks[1] = torch.LongTensor(masks[1])\n",
    "    masks[2] = torch.LongTensor(masks[2])                            \n",
    "    # -------------------------------------------------------\n",
    "    bw, bc, blens, bmasks = batch_w, batch_c, lens, masks\n",
    "    sum_len += sum(blens)\n",
    "    batches_w.append(bw)\n",
    "    batches_c.append(bc)\n",
    "    batches_lens.append(blens)\n",
    "    batches_masks.append(bmasks)\n",
    "    batches_ind.append(ind[start_id: end_id])\n",
    "    if text is not None:\n",
    "        batches_text.append(text[start_id: end_id])\n",
    "        \n",
    "if sort:\n",
    "    perm = list(range(nbatch))\n",
    "    random.shuffle(perm)\n",
    "    batches_w = [batches_w[i] for i in perm]\n",
    "    batches_c = [batches_c[i] for i in perm]\n",
    "    batches_lens = [batches_lens[i] for i in perm]\n",
    "    batches_masks = [batches_masks[i] for i in perm]\n",
    "    batches_ind = [batches_ind[i] for i in perm]\n",
    "    if text is not None:\n",
    "        batches_text = [batches_text[i] for i in perm]\n",
    "\n",
    "logging.info(\"{} batches, avg len: {:.1f}\".format(\n",
    "    nbatch, sum_len / len(x)))\n",
    "recover_ind = [item for sublist in batches_ind for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[     1,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      2],\n",
       "         [     1,      0, 194137,      0,      0,      0,      0,      2,      3,\n",
       "               3,      3]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,  4069,  1674,  ..., 17682, 17682, 17682],\n",
       "          [17684,  3498, 17683,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17684,  2735, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684, 17681,  1725,  ..., 17682, 17682, 17682],\n",
       "          [17684, 17680, 17683,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,  3826, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,  2716, 17683,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 8]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]),\n",
       "  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 17]),\n",
       "  tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 17, 18])]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['潮水', '退', '了', '就', '知道', '誰', '沒', '穿', '褲子'],\n",
       "  ['今', '天', '天氣', '真', '好', '阿']]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recover_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "f\"{len(a)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_d = 5\n",
    "embvecs = [[1,2,3],[4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNINGS] n_d (5) != word vector size (3). Use 3 for embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(f\"[WARNINGS] n_d ({n_d}) != word vector size ({len(embvecs[0])}). \"\n",
    "f\"Use {len(embvecs[0])} for embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Embedding()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
