{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo bi-LM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import h5py\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# *~ coding convention ~*\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable, Union\n",
    "from overrides import overrides\n",
    "\n",
    "# Python Standard Library\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import codecs\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Python Installed Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction: dict to namedtuple\n",
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)\n",
    "\n",
    "# input your directories path\n",
    "model_dir = 'C:\\workspace\\implement_elmo\\elmo\\configs'\n",
    "args2 = dict2namedtuple(\n",
    "    json.load(\n",
    "        codecs.open(\n",
    "            os.path.join(model_dir, 'config.json'), \n",
    "            'r', encoding='utf-8')\n",
    "    )\n",
    ")\n",
    "\n",
    "# args2.config_path == 'cnn_50_100_512_4096_sample.json'\n",
    "\n",
    "# load config\n",
    "with open(os.path.join(model_dir, args2.config_path), 'r') as fin:\n",
    "    config = json.load(fin)\n",
    "    \n",
    "token_embedding = torch.load('token_embedding.pt') \n",
    "masks = [torch.load(f'mask[{ix}].pt') for ix in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size = 512\n",
      "hidden_size = 512\n",
      "cell_size = 4096\n",
      "num_layers = 2\n",
      "memory_cell_clip_value = 3\n",
      "state_projection_clip_value = 3\n",
      "recurrent_dropout_probability = 0.1\n"
     ]
    }
   ],
   "source": [
    "stateful = False\n",
    "_states = None\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "input_size = config['encoder']['projection_dim']\n",
    "hidden_size = config['encoder']['projection_dim']\n",
    "cell_size = config['encoder']['dim']\n",
    "num_layers = config['encoder']['n_layers']\n",
    "memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "state_projection_clip_value = config['encoder']['proj_clip']\n",
    "recurrent_dropout_probability = config['dropout']\n",
    "\n",
    "print(f\"input_size = {input_size}\")\n",
    "print(f\"hidden_size = {hidden_size}\")\n",
    "print(f\"cell_size = {cell_size}\")\n",
    "print(f\"num_layers = {num_layers}\")\n",
    "print(f\"memory_cell_clip_value = {memory_cell_clip_value}\")\n",
    "print(f\"state_projection_clip_value = {state_projection_clip_value}\")\n",
    "print(f\"recurrent_dropout_probability = {config['dropout']}\")\n",
    "\n",
    "forward_layers = []\n",
    "backward_layers = []\n",
    "\n",
    "lstm_input_size = input_size\n",
    "go_forward = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCellWithProjection(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 cell_size: int,\n",
    "                 go_forward: bool = True,\n",
    "                 recurrent_dropout_probability: float = 0.0,\n",
    "                 memory_cell_clip_value: Optional[float] = None,\n",
    "                 state_projection_clip_value: Optional[float] = None) -> None:\n",
    "        super(LstmCellWithProjection, self).__init__()\n",
    "        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`.\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        self.go_forward = go_forward\n",
    "        self.state_projection_clip_value = state_projection_clip_value\n",
    "        self.memory_cell_clip_value = memory_cell_clip_value\n",
    "        self.recurrent_dropout_probability = recurrent_dropout_probability\n",
    "\n",
    "        # We do the projections for all the gates all at once.\n",
    "        self.input_linearity = nn.Linear(input_size, 4 * cell_size, bias=False)\n",
    "        self.state_linearity = nn.Linear(hidden_size, 4 * cell_size, bias=True)\n",
    "\n",
    "        # Additional projection matrix for making the hidden state smaller.\n",
    "        self.state_projection = nn.Linear(cell_size, hidden_size, bias=False)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Use sensible default initializations for parameters.\n",
    "        block_orthogonal(self.input_linearity.weight.data, [self.cell_size, self.input_size])\n",
    "        block_orthogonal(self.state_linearity.weight.data, [self.cell_size, self.hidden_size])\n",
    "\n",
    "        self.state_linearity.bias.data.fill_(0.0)\n",
    "        # Initialize forget gate biases to 1.0 as per An Empirical\n",
    "        # Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n",
    "        self.state_linearity.bias.data[self.cell_size:2 * self.cell_size].fill_(1.0)\n",
    "        \n",
    "def block_orthogonal(tensor: torch.Tensor,\n",
    "                     split_sizes: List[int],\n",
    "                     gain: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    An initializer which allows initaliizing model parametes in \"block\".\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, Variable):\n",
    "    # in pytorch 4.0, Variable equals Tensor\n",
    "    #     block_orthogonal(tensor.data, split_sizes, gain)\n",
    "    # else:\n",
    "        sizes = list(tensor.size())\n",
    "        if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n",
    "            raise ConfigurationError(\n",
    "                \"tensor dimentions must be divisible by their respective \"\n",
    "                f\"split_sizes. Found size: {size} and split_sizes: {split_sizes}\")\n",
    "        indexes = [list(range(0, max_size, split))\n",
    "                   for max_size, split in zip(sizes, split_sizes)]\n",
    "        # Iterate over all possible blocks within the tensor.\n",
    "        for block_start_indices in itertools.product(*indexes):\n",
    "            index_and_step_tuples = zip(block_start_indices, split_sizes)\n",
    "            block_slice = tuple([slice(start_index, start_index + step)\n",
    "                                 for start_index, step in index_and_step_tuples])\n",
    "            tensor[block_slice] = nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_index in range(num_layers):\n",
    "    forward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                          hidden_size,\n",
    "                                          cell_size,\n",
    "                                          go_forward,\n",
    "                                          recurrent_dropout_probability,\n",
    "                                          memory_cell_clip_value,\n",
    "                                          state_projection_clip_value)\n",
    "    backward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                           hidden_size,\n",
    "                                           cell_size,\n",
    "                                           not go_forward,\n",
    "                                           recurrent_dropout_probability,\n",
    "                                           memory_cell_clip_value,\n",
    "                                           state_projection_clip_value)\n",
    "    if use_cuda:\n",
    "        forward_layer = forward_layer.cuda()\n",
    "        backward_layer = backward_layer.cuda()\n",
    "    lstm_input_size = hidden_size\n",
    "    forward_layers.append(forward_layer)\n",
    "    backward_layers.append(backward_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LstmCellWithProjection(\n",
       "   (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "   (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "   (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       " ), LstmCellWithProjection(\n",
       "   (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "   (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "   (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       " )]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LstmCellWithProjection(\n",
       "   (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "   (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "   (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       " ), LstmCellWithProjection(\n",
       "   (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "   (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "   (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       " )]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가보자~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = token_embedding\n",
    "mask = masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "           1.9630e-02, -2.7469e-02],\n",
       "         [ 2.9012e-02,  4.7038e-03, -2.2742e-02,  ...,  1.6245e-02,\n",
       "           2.9220e-02, -1.6054e-02],\n",
       "         [-5.4546e-03, -2.7518e-02, -1.8246e-02,  ...,  9.4597e-03,\n",
       "           2.9380e-02, -9.5604e-03],\n",
       "         ...,\n",
       "         [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "          -5.3617e-03, -1.8110e-02],\n",
       "         [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "          -5.3617e-03, -1.8110e-02],\n",
       "         [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "          -5.3617e-03, -1.8110e-02]],\n",
       "\n",
       "        [[-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "           1.9630e-02, -2.7469e-02],\n",
       "         [ 5.6490e-03, -2.6852e-02, -2.1564e-02,  ..., -4.3684e-03,\n",
       "           5.7293e-02, -4.5267e-02],\n",
       "         [ 2.6103e-02, -4.5548e-03, -1.5987e-02,  ...,  3.1253e-02,\n",
       "           1.0739e-02, -5.7272e-02],\n",
       "         ...,\n",
       "         [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "          -5.3617e-03, -1.8110e-02],\n",
       "         [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "          -5.3617e-03, -1.8110e-02],\n",
       "         [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "          -5.3617e-03, -1.8110e-02]],\n",
       "\n",
       "        [[-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "           1.9630e-02, -2.7469e-02],\n",
       "         [ 7.7665e-03, -2.5998e-02, -2.0474e-02,  ...,  1.4450e-02,\n",
       "           5.0952e-02, -1.0182e-02],\n",
       "         [ 4.3900e-02, -2.0010e-02, -2.2308e-02,  ...,  1.1431e-02,\n",
       "           4.4527e-02, -5.6586e-02],\n",
       "         ...,\n",
       "         [ 3.9943e-02, -6.3473e-04, -2.4333e-02,  ...,  1.9435e-02,\n",
       "           5.1162e-02, -3.3509e-02],\n",
       "         [ 2.2468e-02,  2.0974e-03, -7.3369e-03,  ...,  3.1688e-02,\n",
       "           3.0309e-02, -4.4693e-02],\n",
       "         [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
       "           3.8271e-02,  8.0397e-05]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 512]), torch.Size([3, 10]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 3\n",
      "total_sequence_length = 10\n"
     ]
    }
   ],
   "source": [
    "batch_size, total_sequence_length = mask.size()\n",
    "print(f\"batch_size = {batch_size}\")\n",
    "print(f\"total_sequence_length = {total_sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_valid = 3\n",
      "sequence_lengths = tensor([ 7,  6, 10], device='cuda:0')\n",
      "Sorting lengths by descending...\n",
      "sorted_length_and_permIx = torch.return_types.sort(\n",
      "\ttensor([10,  7,  6], device='cuda:0')\t# sorted_sequence_lengths\n",
      "\ttensor([2, 0, 1], device='cuda:0')\t# sorting_indices\n",
      ")\n",
      "Sorting tensor...\n",
      "index_range = tensor([0, 1, 2], device='cuda:0')\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# sorting\n",
    "num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "print(f\"num_valid = {num_valid}\")\n",
    "\n",
    "# get_lengths_from_binary_sequence_mask\n",
    "sequence_lengths = mask.long().sum(-1)\n",
    "print(f\"sequence_lengths = {sequence_lengths}\")\n",
    "\n",
    "# sort_batch_by_length\n",
    "assert (isinstance(inputs, Variable) and\n",
    "        isinstance(sequence_lengths, Variable)), \\\n",
    "        \"Both the tensor and sequence lengths must be \"\\\n",
    "        \"torch.autograd.Variables.\"\n",
    "\n",
    "print('Sorting lengths by descending...')\n",
    "sorted_length_and_permIx = sequence_lengths.sort(0, \n",
    "                                                 descending=True)\n",
    "print(\"sorted_length_and_permIx = torch.return_types.sort(\"\n",
    "      f\"\\n\\t{sorted_length_and_permIx[0]}\\t# sorted_sequence_lengths\"\n",
    "      f\"\\n\\t{sorted_length_and_permIx[1]}\\t# sorting_indices\\n)\")\n",
    "sorted_sequence_lengths, sorting_indices = sorted_length_and_permIx\n",
    "\n",
    "print(\"Sorting tensor...\")\n",
    "sorted_inputs = inputs.index_select(0, sorting_indices)\n",
    "\n",
    "index_range = sequence_lengths.data.clone().copy_(\n",
    "    torch.arange(0, len(sequence_lengths)))\n",
    "print(f\"index_range = {index_range}\")\n",
    "\n",
    "_, reverse_mapping = permutation_index.sort(0, descending=False)\n",
    "restoration_indices = index_range.index_select(0, reverse_mapping)\n",
    "print(f\"restoration_indices = {restoration_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "            1.9630e-02, -2.7469e-02],\n",
       "          [ 7.7665e-03, -2.5998e-02, -2.0474e-02,  ...,  1.4450e-02,\n",
       "            5.0952e-02, -1.0182e-02],\n",
       "          [ 4.3900e-02, -2.0010e-02, -2.2308e-02,  ...,  1.1431e-02,\n",
       "            4.4527e-02, -5.6586e-02],\n",
       "          ...,\n",
       "          [ 3.9943e-02, -6.3473e-04, -2.4333e-02,  ...,  1.9435e-02,\n",
       "            5.1162e-02, -3.3509e-02],\n",
       "          [ 2.2468e-02,  2.0974e-03, -7.3369e-03,  ...,  3.1688e-02,\n",
       "            3.0309e-02, -4.4693e-02],\n",
       "          [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
       "            3.8271e-02,  8.0397e-05]],\n",
       " \n",
       "         [[-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "            1.9630e-02, -2.7469e-02],\n",
       "          [ 2.9012e-02,  4.7038e-03, -2.2742e-02,  ...,  1.6245e-02,\n",
       "            2.9220e-02, -1.6054e-02],\n",
       "          [-5.4546e-03, -2.7518e-02, -1.8246e-02,  ...,  9.4597e-03,\n",
       "            2.9380e-02, -9.5604e-03],\n",
       "          ...,\n",
       "          [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "           -5.3617e-03, -1.8110e-02],\n",
       "          [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "           -5.3617e-03, -1.8110e-02],\n",
       "          [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "           -5.3617e-03, -1.8110e-02]],\n",
       " \n",
       "         [[-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "            1.9630e-02, -2.7469e-02],\n",
       "          [ 5.6490e-03, -2.6852e-02, -2.1564e-02,  ..., -4.3684e-03,\n",
       "            5.7293e-02, -4.5267e-02],\n",
       "          [ 2.6103e-02, -4.5548e-03, -1.5987e-02,  ...,  3.1253e-02,\n",
       "            1.0739e-02, -5.7272e-02],\n",
       "          ...,\n",
       "          [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "           -5.3617e-03, -1.8110e-02],\n",
       "          [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "           -5.3617e-03, -1.8110e-02],\n",
       "          [ 2.1840e-02, -2.4943e-02, -3.1549e-02,  ...,  2.9098e-02,\n",
       "           -5.3617e-03, -1.8110e-02]]], device='cuda:0',\n",
       "        grad_fn=<IndexSelectBackward>), torch.Size([3, 10, 512]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_inputs, sorted_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  7,  6], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restoration_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorting_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "          1.9630e-02, -2.7469e-02],\n",
       "        [-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "          1.9630e-02, -2.7469e-02],\n",
       "        [-3.3597e-03,  9.7976e-03,  1.4402e-03,  ...,  3.7601e-02,\n",
       "          1.9630e-02, -2.7469e-02],\n",
       "        ...,\n",
       "        [ 3.9943e-02, -6.3473e-04, -2.4333e-02,  ...,  1.9435e-02,\n",
       "          5.1162e-02, -3.3509e-02],\n",
       "        [ 2.2468e-02,  2.0974e-03, -7.3369e-03,  ...,  3.1688e-02,\n",
       "          3.0309e-02, -4.4693e-02],\n",
       "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
       "          3.8271e-02,  8.0397e-05]], device='cuda:0',\n",
       "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pack_padded_sequence\n",
    "packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                             sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                             batch_first=True)\n",
    "packed_sequence_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 512])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_sequence_input.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 처음일 경우, 아래와 같이 `initial_state`를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 초기화한 상태를 가지고 lstm_forward를 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if initial_state is None:\n",
    "    hidden_states = [None] * len(forward_layers)\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, batch_lengths = pad_packed_sequence(packed_sequence_input, \n",
    "                                            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 512]), tensor([10,  7,  6]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, batch_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_output_sequence = inputs\n",
    "backward_output_sequence = inputs\n",
    "\n",
    "final_states = []\n",
    "sequence_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_index, state in enumerate(hidden_states):\n",
    "    forward_layer = forward_layers[layer_index]\n",
    "    backward_layer = backward_layers[layer_index]\n",
    "    \n",
    "    forward_cache = forward_output_sequence\n",
    "    backward_cache = backward_output_sequence\n",
    "    \n",
    "    # 맨 처음 실시될 당시에는 state == None임!\n",
    "    forward_state = None\n",
    "    backward_state = None\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
