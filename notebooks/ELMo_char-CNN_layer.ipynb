{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo Char-CNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# *~ coding convention ~*\n",
    "from overrides import overrides\n",
    "from typing import Callable\n",
    "\n",
    "# Python Standard Library\n",
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Python Installed Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "with open(os.path.join(model_dir, args2.config_path), 'r') as fin:\n",
    "    config = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction: dict to namedtuple\n",
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)\n",
    "\n",
    "# input your directories path\n",
    "model_dir = 'C:/workspace/ELMo/161/'\n",
    "args2 = dict2namedtuple(\n",
    "    json.load(\n",
    "        codecs.open(\n",
    "            os.path.join(model_dir, 'config.json'), \n",
    "            'r', encoding='utf-8')\n",
    "    )\n",
    ")\n",
    "\n",
    "# args2.config_path == 'cnn_50_100_512_4096_sample.json'\n",
    "\n",
    "# load config\n",
    "with open(os.path.join(model_dir, args2.config_path), 'r') as fin:\n",
    "    config = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder': {'name': 'elmo',\n",
       "  'projection_dim': 512,\n",
       "  'cell_clip': 3,\n",
       "  'proj_clip': 3,\n",
       "  'dim': 4096,\n",
       "  'n_layers': 2},\n",
       " 'token_embedder': {'name': 'cnn',\n",
       "  'activation': 'relu',\n",
       "  'filters': [[1, 32],\n",
       "   [2, 32],\n",
       "   [3, 64],\n",
       "   [4, 128],\n",
       "   [5, 256],\n",
       "   [6, 512],\n",
       "   [7, 1024]],\n",
       "  'n_highway': 2,\n",
       "  'word_dim': 100,\n",
       "  'char_dim': 50,\n",
       "  'max_characters_per_token': 50},\n",
       " 'classifier': {'name': 'sampled_softmax', 'n_samples': 8192},\n",
       " 'dropout': 0.1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\t발\t없는\t말이\t천리\t간다\t<EOS>\t<PAD>\t<PAD>\t<PAD>\t\n",
      "<BOS>\t다시\t사랑한다\t말\t할까\t<EOS>\t<PAD>\t<PAD>\t<PAD>\t<PAD>\t\n",
      "<BOS>\t유독\t너와\t헤어지다\t싫다\t밤\t집\t으로\t돌아가다\t<EOS>\t\n"
     ]
    }
   ],
   "source": [
    "sents = [['<BOS>', '발', '없는', '말이', '천리', '간다', '<EOS>', '<PAD>', '<PAD>', '<PAD>'],\n",
    "         ['<BOS>', '다시', '사랑한다', '말', '할까', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'],\n",
    "         ['<BOS>', '유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다', '<EOS>']]\n",
    "for sent in sents:\n",
    "    for i in sent:\n",
    "        print(i, end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Inputs\n",
    "sents = [['발', '없는', '말이', '천리', '간다'],\n",
    "         ['다시', '사랑한다', '말', '할까'],\n",
    "         ['유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다']]\n",
    "\n",
    "# Set maximum number of characters\n",
    "max_chars = 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, textset = [], []\n",
    "for sent in sents:\n",
    "    # Add begin of sentence(bos)\n",
    "    data = ['<bos>']\n",
    "    text = []\n",
    "    for token in sent:\n",
    "        text.append(token)\n",
    "        # ELMo's input is character\n",
    "        # Since ElMo uses char-CNN, input_dim must be SAME\n",
    "        # if numChars+2 < max_chars: why +2? bos & eos\n",
    "        #     pad values to pad_id\n",
    "        # else:\n",
    "        #     cut token:= token[:max_chars - 2]\n",
    "        if max_chars is not None and len(token) + 2 > max_chars:\n",
    "            token = token[:max_chars - 2]\n",
    "        data.append(token)\n",
    "    # Add end of sentence(eos)\n",
    "    data.append('<eos>')\n",
    "    dataset.append(data)\n",
    "    textset.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', '발', '없는', '말이', '천리', '간다', '<eos>'],\n",
       " ['<bos>', '다시', '사랑한다', '말', '할까', '<eos>'],\n",
       " ['<bos>', '유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다', '<eos>']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['발', '없는', '말이', '천리', '간다'],\n",
       " ['다시', '사랑한다', '말', '할까'],\n",
       " ['유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If GPU is available, use_cuda:= True\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    EmbeddingLayer\n",
    "    \n",
    "    두 가지 역할을 수행\n",
    "    1. word/character를 사전 규칙에 따라 index로 변환\n",
    "    2. config['token_embedder']['char_dim']으로 차원을 축소\n",
    "    \"\"\"\n",
    "    def __init__(self, n_d, word2id, embs=None, fix_emb=True, oov='<oov>', pad='<pad>', normalize=True):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        if embs is not None:\n",
    "            embwords, embvecs = embs\n",
    "            logging.info(f\"{len(word2id)} pre-trained word embeddings loaded.\")\n",
    "            if n_d != len(embvecs[0]):\n",
    "                logging.warning(f\"[WARNINGS] n_d ({n_d}) != word vector size ({len(embvecs[0])}). \"\n",
    "                                f\"Use {len(embvecs[0])} for embeddings.\")\n",
    "                n_d = len(embvecs[0])\n",
    "        self.word2id = word2id\n",
    "        self.id2word = {i: word for word, i in word2id.items()}\n",
    "        self.n_V, self.n_d = len(word2id), n_d\n",
    "        self.oovid = word2id[oov]\n",
    "        self.padid = word2id[pad]\n",
    "        # n_V -> n_d, 차원 축소\n",
    "        self.embedding = nn.Embedding(self.n_V, n_d, padding_idx=self.padid)\n",
    "        self.embedding.weight.data.uniform_(-0.25, 0.25)\n",
    "        \n",
    "        if embs is not None:\n",
    "            weight = self.embedding.weight\n",
    "            weight.data[:len(embwords)].copy_(torch.from_numpy(embvecs))\n",
    "            logging.info(\"embedding shape: {}\".format(weight.size()))\n",
    "            \n",
    "        if normalize:\n",
    "            weight = self.embedding.weight\n",
    "            norms = weight.data.norm(2, 1)\n",
    "            if norms.dim() == 1:\n",
    "                norms = norms.unsqueeze(1)\n",
    "            weight.data.div_(norms.expand_as(weight.data))\n",
    "            \n",
    "        if fix_emb:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            \n",
    "    def forward(self, input_):\n",
    "        return self.embedding(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the model trained with character-based word encoder.\n",
    "if config['token_embedder']['char_dim'] > 0:\n",
    "    char_lexicon = {}\n",
    "    with codecs.open(os.path.join(model_dir, 'char.dic'), 'r', encoding='utf-8') as fpi:\n",
    "        \"\"\"\n",
    "        # char.dic\n",
    "        <\t0\n",
    "        b\t1\n",
    "        ...\n",
    "        특\t18\n",
    "        별\t19\n",
    "        기\t20\n",
    "        고\t21\n",
    "        ...\n",
    "        ữ\t17675\n",
    "        븟\t17676\n",
    "        铸\t17677\n",
    "        鋳\t17678\n",
    "        <bos>\t17679\n",
    "        <eos>\t17680\n",
    "        <oov>\t17681\n",
    "        <pad>\t17682\n",
    "        <bow>\t17683\n",
    "        <eow>\t17684\n",
    "        \"\"\"\n",
    "        for line in fpi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            if len(tokens) == 1:\n",
    "                tokens.insert(0, '\\u3000')\n",
    "            token, i = tokens\n",
    "            char_lexicon[token] = int(i)\n",
    "    char_emb_layer = EmbeddingLayer(\n",
    "        config['token_embedder']['char_dim'], char_lexicon, fix_emb=False, embs=None)\n",
    "    if use_cuda:\n",
    "        char_emb_layer = char_emb_layer.cuda()\n",
    "    logging.info('char embedding size: ' +\n",
    "                str(len(char_emb_layer.word2id)))\n",
    "else:\n",
    "    char_lexicon = None\n",
    "    char_emb_layer = None\n",
    "\n",
    "# For the model trained with word form word encoder.\n",
    "if config['token_embedder']['word_dim'] > 0:\n",
    "    word_lexicon = {}\n",
    "    with codecs.open(os.path.join(model_dir, 'word.dic'), 'r', encoding='utf-8') as fpi:\n",
    "        \"\"\"\n",
    "        <oov>\t0\n",
    "        <bos>\t1\n",
    "        <eos>\t2\n",
    "        <pad>\t3\n",
    "        ,\t4\n",
    "        .\t5\n",
    "        호텔\t6\n",
    "        ...\n",
    "        (Penobscot\t427840\n",
    "        Tornesch\t427841\n",
    "        Wodociągi\t427842\n",
    "        피트리\t427843\n",
    "        ArmeniaThe\t427844\n",
    "        Cascade에서\t427845\n",
    "        Retrophilia\t427846\n",
    "        kmCala\t427847\n",
    "        노스다코타Dickinson\t427848\n",
    "        \"\"\"\n",
    "        for line in fpi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            if len(tokens) == 1:\n",
    "                tokens.insert(0, '\\u3000')\n",
    "            token, i = tokens\n",
    "            word_lexicon[token] = int(i)\n",
    "    word_emb_layer = EmbeddingLayer(\n",
    "        config['token_embedder']['word_dim'], word_lexicon, fix_emb=False, embs=None)\n",
    "    if use_cuda:\n",
    "        word_emb_layer = word_emb_layer.cuda()\n",
    "    logging.info('word embedding size: ' +\n",
    "                str(len(word_emb_layer.word2id)))\n",
    "else:\n",
    "    word_lexicon = None\n",
    "    word_emb_layer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer(\n",
       "  (embedding): Embedding(17685, 50, padding_idx=17682)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer(\n",
       "  (embedding): Embedding(427849, 100, padding_idx=3)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = word_lexicon\n",
    "char2id = char_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset\n",
    "text = textset\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "x = test\n",
    "perm = None\n",
    "shuffle = False\n",
    "# sort = True\n",
    "sort = False\n",
    "\n",
    "ind = list(range(len(x)))\n",
    "lst = perm or ind\n",
    "print(lst)\n",
    "if shuffle:\n",
    "    random.shuffle(lst)\n",
    "    \n",
    "if sort:\n",
    "    lst.sort(key=lambda l: -len(x[l]))\n",
    "    print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', '발', '없는', '말이', '천리', '간다', '<eos>'], ['<bos>', '다시', '사랑한다', '말', '할까', '<eos>'], ['<bos>', '유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다', '<eos>']]\n",
      "[0, 1, 2]\n",
      "[['발', '없는', '말이', '천리', '간다'], ['다시', '사랑한다', '말', '할까'], ['유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다']]\n"
     ]
    }
   ],
   "source": [
    "x = [x[i] for i in lst]\n",
    "ind = [ind[i] for i in lst]\n",
    "if text is not None:\n",
    "    text = [text[i] for i in lst]\n",
    "\n",
    "print(x)\n",
    "print(ind)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_len = 0.0\n",
    "batches_w, batches_c, batches_lens, batches_masks, batches_text, batches_ind = [], [], [], [], [], []\n",
    "size = batch_size\n",
    "nbatch = (len(x) - 1) // size + 1\n",
    "\n",
    "nbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xec\\x82\\xac\\xeb\\x9e\\x91\\xed\\x95\\x9c\\xeb\\x8b\\xa4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'사랑한다'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> : 17679\n",
      "<eos> : 17680\n",
      "<oov> : 17681\n",
      "<pad> : 17682\n",
      "<bow> : 17683\n",
      "<eow> : 17684\n"
     ]
    }
   ],
   "source": [
    "for i in ['<bos>', '<eos>', '<oov>', '<pad>', '<bow>', '<eow>']:\n",
    "    print(f\"{i} : {char2id[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 문장:\n",
      "tensor([[17684, 17679, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   217, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   186,    31, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   134,    53, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   419,    40, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,    65,    92, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684, 17680, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682]])\n",
      "2번째 문장:\n",
      "tensor([[17684, 17679, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,    92,    42, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,    43,   580,    59,    92, 17683, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   134, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   185,    78, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684, 17680, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682]])\n",
      "3번째 문장:\n",
      "tensor([[17684, 17679, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,    87,   416, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   183,   223, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   478,    27,    76,    92, 17683, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,  1209,    92, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,  1217, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   439, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   198,    57, 17683, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684,   774,   136,    32,    92, 17683, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682],\n",
      "        [17684, 17680, 17683, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682,\n",
      "         17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682, 17682]])\n"
     ]
    }
   ],
   "source": [
    "oov='<oov>'\n",
    "pad='<pad>'\n",
    "\n",
    "# Create batch\n",
    "for i in range(nbatch):\n",
    "    start_id, end_id = i * size, (i + 1) * size\n",
    "    # Create one_batch---------------------------------------\n",
    "    x_b = x[start_id: end_id]\n",
    "    batch_size = len(x_b)\n",
    "    lst = list(range(batch_size))\n",
    "    if sort:\n",
    "        lst.sort(key=lambda l: -len(x[l]))\n",
    "    # shuffle the sentences by\n",
    "    x_b = [x_b[i] for i in lst]\n",
    "    lens = [len(x_b[i]) for i in lst]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # get a batch of word id whose size is (batch x max_len)\n",
    "    if word2id is not None:\n",
    "        oov_id = word2id.get(oov, None)\n",
    "        pad_id = word2id.get(pad, None)\n",
    "        assert oov_id is not None and pad_id is not None\n",
    "        batch_w = torch.LongTensor(batch_size, max_len).fill_(pad_id)\n",
    "        for i, x_i in enumerate(x_b):\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_w[i][j] = word2id.get(x_ij, oov_id)\n",
    "    else:\n",
    "        batch_w = None\n",
    "    \n",
    "    # get a batch of character id whose size is (batch x max_chars)\n",
    "    if char2id is not None:\n",
    "        bow_id, eow_id, oov_id, pad_id = [\n",
    "            char2id.get(key, None) \n",
    "            for key in ('<eow>', '<bow>', oov, pad)\n",
    "        ] # 왜 거꾸로 받지???ㄷㄷ;;\n",
    "        assert ((bow_id is not None) and \n",
    "                (eow_id is not None) and\n",
    "                (oov_id is not None) and\n",
    "                (pad_id is not None))\n",
    "        if config['token_embedder']['name'].lower() == 'cnn':\n",
    "            max_chars = config['token_embedder']['max_characters_per_token']\n",
    "            assert max([len(w) for i in lst for w in x_b[i]]) + 2 <= max_chars\n",
    "        elif config['token_embedder']['name'].lower() == 'lstm':\n",
    "            max_chars = max([len(w) for i in lst for w in x_b[i]]) + 2\n",
    "        else:\n",
    "            raise ValueError('Unknown token_embedder: {0}'.format(config['token_embedder']['name']))\n",
    "        batch_c = torch.LongTensor(batch_size, max_len, max_chars).fill_(pad_id)\n",
    "        for i, x_i in enumerate(x_b):\n",
    "            print(f\"{i+1}번째 문장:\")\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_c[i][j][0] = bow_id\n",
    "                if x_ij in ['<bos>', '<eos>']:\n",
    "                    batch_c[i][j][1] = char2id.get(x_ij)\n",
    "                    batch_c[i][j][2] = eow_id\n",
    "                else:\n",
    "                    for k, c in enumerate(x_ij):\n",
    "                        batch_c[i][j][k+1] = char2id.get(c, oov_id)\n",
    "                    batch_c[i][j][len(x_ij)+1] = eow_id\n",
    "            print(batch_c[i])\n",
    "    else:\n",
    "        batch_c = None\n",
    "        \n",
    "    masks = [torch.LongTensor(batch_size, max_len).fill_(0), [], []]\n",
    "    \n",
    "    for i, x_i in enumerate(x_b):\n",
    "        for j in range(len(x_i)):\n",
    "            masks[0][i][j] = 1\n",
    "            if j + 1 < len(x_i):\n",
    "                masks[1].append(i * max_len + j)\n",
    "            if j > 0:\n",
    "                masks[2].append(i * max_len + j)\n",
    "\n",
    "    assert len(masks[1]) <= batch_size * max_len\n",
    "    assert len(masks[2]) <= batch_size * max_len\n",
    "\n",
    "    masks[1] = torch.LongTensor(masks[1])\n",
    "    masks[2] = torch.LongTensor(masks[2])                            \n",
    "    # -------------------------------------------------------\n",
    "    bw, bc, blens, bmasks = batch_w, batch_c, lens, masks\n",
    "    sum_len += sum(blens)\n",
    "    batches_w.append(bw)\n",
    "    batches_c.append(bc)\n",
    "    batches_lens.append(blens)\n",
    "    batches_masks.append(bmasks)\n",
    "    batches_ind.append(ind[start_id: end_id])\n",
    "    if text is not None:\n",
    "        batches_text.append(text[start_id: end_id])\n",
    "        \n",
    "if sort:\n",
    "    perm = list(range(nbatch))\n",
    "    random.shuffle(perm)\n",
    "    batches_w = [batches_w[i] for i in perm]\n",
    "    batches_c = [batches_c[i] for i in perm]\n",
    "    batches_lens = [batches_lens[i] for i in perm]\n",
    "    batches_masks = [batches_masks[i] for i in perm]\n",
    "    batches_ind = [batches_ind[i] for i in perm]\n",
    "    if text is not None:\n",
    "        batches_text = [batches_text[i] for i in perm]\n",
    "\n",
    "logging.info(\"{} batches, avg len: {:.1f}\".format(\n",
    "    nbatch, sum_len / len(x)))\n",
    "recover_ind = [item for sublist in batches_ind for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[     1,   1820,    325,   3232, 345792,   7127,      2,      3,      3,\n",
       "               3],\n",
       "         [     1,    237,  50660,   1489,  13000,      2,      3,      3,      3,\n",
       "               3],\n",
       "         [     1,  36081,  26437,      0,  65226,   1973,   2607,   3650,      0,\n",
       "               2]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,   217, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,   186,    31,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,    92,    42,  ..., 17682, 17682, 17682],\n",
       "          [17684,    43,   580,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,    87,   416,  ..., 17682, 17682, 17682],\n",
       "          [17684,   183,   223,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17684,   198,    57,  ..., 17682, 17682, 17682],\n",
       "          [17684,   774,   136,  ..., 17682, 17682, 17682],\n",
       "          [17684, 17680, 17683,  ..., 17682, 17682, 17682]]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 6, 10]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       "  tensor([ 0,  1,  2,  3,  4,  5, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25, 26,\n",
       "          27, 28]),\n",
       "  tensor([ 1,  2,  3,  4,  5,  6, 11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 26, 27,\n",
       "          28, 29])]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['발', '없는', '말이', '천리', '간다'],\n",
       "  ['다시', '사랑한다', '말', '할까'],\n",
       "  ['유독', '너와', '헤어지다', '싫다', '밤', '집', '으로', '돌아가다']]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recover_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    batches_w[0] = batches_w[0].cuda()\n",
    "    batches_c[0] = batches_c[0].cuda()\n",
    "    batches_masks[0] = [mask.cuda() for mask in batches_masks[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[     1,   1820,    325,   3232, 345792,   7127,      2,      3,      3,\n",
       "               3],\n",
       "         [     1,    237,  50660,   1489,  13000,      2,      3,      3,      3,\n",
       "               3],\n",
       "         [     1,  36081,  26437,      0,  65226,   1973,   2607,   3650,      0,\n",
       "               2]], device='cuda:0')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,   217, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,   186,    31,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,    92,    42,  ..., 17682, 17682, 17682],\n",
       "          [17684,    43,   580,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "          [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       " \n",
       "         [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "          [17684,    87,   416,  ..., 17682, 17682, 17682],\n",
       "          [17684,   183,   223,  ..., 17682, 17682, 17682],\n",
       "          ...,\n",
       "          [17684,   198,    57,  ..., 17682, 17682, 17682],\n",
       "          [17684,   774,   136,  ..., 17682, 17682, 17682],\n",
       "          [17684, 17680, 17683,  ..., 17682, 17682, 17682]]], device='cuda:0')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       "  tensor([ 0,  1,  2,  3,  4,  5, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25, 26,\n",
       "          27, 28], device='cuda:0'),\n",
       "  tensor([ 1,  2,  3,  4,  5,  6, 11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 26, 27,\n",
       "          28, 29], device='cuda:0')]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이어서 계속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w, test_c, test_lens, test_masks, test_text, recover_ind = (\n",
    "    batches_w, batches_c, batches_lens, batches_masks, batches_text, recover_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, c, lens, masks, texts = next(zip(test_w, test_c, test_lens, test_masks, test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output = self.model.forward(w, c, masks)\n",
    "# token_embedder = ConvTokenEmbedder(\n",
    "#     config, word_emb_layer, char_emb_layer, use_cuda)\n",
    "\n",
    "emb_dim = 0\n",
    "output_dim = config['encoder']['projection_dim']\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if word_emb_layer is not None:\n",
    "    emb_dim += word_emb_layer.n_d\n",
    "emb_dim    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cnn',\n",
       " 'activation': 'relu',\n",
       " 'filters': [[1, 32],\n",
       "  [2, 32],\n",
       "  [3, 64],\n",
       "  [4, 128],\n",
       "  [5, 256],\n",
       "  [6, 512],\n",
       "  [7, 1024]],\n",
       " 'n_highway': 2,\n",
       " 'word_dim': 100,\n",
       " 'char_dim': 50,\n",
       " 'max_characters_per_token': 50}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['token_embedder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = config['token_embedder']['filters']\n",
    "char_embed_dim = config['token_embedder']['char_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutions = []\n",
    "\n",
    "for i, (width, num) in enumerate(filters):\n",
    "    conv = nn.Conv1d(\n",
    "        in_channels=char_embed_dim,\n",
    "        out_channels=num, # 문자를 몇 개나 볼 것인지\n",
    "        kernel_size=width,\n",
    "        bias=True\n",
    "    )\n",
    "    if use_cuda:\n",
    "        conv = conv.cuda()\n",
    "    convolutions.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv1d(50, 32, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(50, 32, kernel_size=(2,), stride=(1,)),\n",
       " Conv1d(50, 64, kernel_size=(3,), stride=(1,)),\n",
       " Conv1d(50, 128, kernel_size=(4,), stride=(1,)),\n",
       " Conv1d(50, 256, kernel_size=(5,), stride=(1,)),\n",
       " Conv1d(50, 512, kernel_size=(6,), stride=(1,)),\n",
       " Conv1d(50, 1024, kernel_size=(7,), stride=(1,))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
       "  (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
       "  (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
       "  (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
       "  (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
       "  (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
       "  (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolutions = nn.ModuleList(convolutions)\n",
    "convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_filters = sum(f[1] for f in filters)\n",
    "n_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_highway = config['token_embedder']['n_highway']\n",
    "n_highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 num_layers: int = 1,\n",
    "                 activation: Callable[[torch.Tensor], torch.Tensor] = torch.nn.functional.relu) -> None:\n",
    "        super(Highway, self).__init__()\n",
    "        self._input_dim = input_dim\n",
    "        self._layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, input_dim * 2).cuda() if use_cuda \n",
    "             else nn.Linear(input_dim, input_dim * 2).cuda()\n",
    "             for _ in range(num_layers)])\n",
    "        self._activation = activation\n",
    "        for layer in self._layers:\n",
    "            # We should bias the highway layer to just carry its input forward.  We do that by\n",
    "            # setting the bias on `B(x)` to be positive, because that means `g` will be biased to\n",
    "            # be high, to we will carry the input forward.  The bias on `B(x)` is the second half\n",
    "            # of the bias vector in each Linear layer.\n",
    "            layer.bias[input_dim:].data.fill_(1)\n",
    "        \n",
    "    @overrides\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ\n",
    "        current_input = inputs\n",
    "        for layer in self._layers:\n",
    "            projected_input = layer(current_input)\n",
    "            linear_part = current_input\n",
    "            # NOTE: if you modify this, think about whether you should modify the initialization\n",
    "            # above, too.\n",
    "            nonlinear_part = projected_input[:, (0 * self._input_dim):(1 * self._input_dim)]\n",
    "            gate = projected_input[:, (1 * self._input_dim):(2 * self._input_dim)]\n",
    "            nonlinear_part = self._activation(nonlinear_part)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            current_input = gate * linear_part + (1 - gate) * nonlinear_part\n",
    "        return current_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "highways = Highway(n_filters, n_highway, torch.nn.functional.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2148"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim += n_filters\n",
    "emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = nn.Linear(emb_dim, output_dim, bias=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.forward(w, c, masks)\n",
    "# token_embedder.forward()\n",
    "\n",
    "word_inp = w\n",
    "\n",
    "chars_package = c\n",
    "chars_inp = chars_package\n",
    "\n",
    "mask_package = masks\n",
    "shape = mask_package[0].size(0), mask_package[0].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     1,   1820,    325,   3232, 345792,   7127,      2,      3,      3,\n",
       "              3],\n",
       "        [     1,    237,  50660,   1489,  13000,      2,      3,      3,      3,\n",
       "              3],\n",
       "        [     1,  36081,  26437,      0,  65226,   1973,   2607,   3650,      0,\n",
       "              2]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "         [17684,   217, 17683,  ..., 17682, 17682, 17682],\n",
       "         [17684,   186,    31,  ..., 17682, 17682, 17682],\n",
       "         ...,\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       "\n",
       "        [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "         [17684,    92,    42,  ..., 17682, 17682, 17682],\n",
       "         [17684,    43,   580,  ..., 17682, 17682, 17682],\n",
       "         ...,\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682],\n",
       "         [17682, 17682, 17682,  ..., 17682, 17682, 17682]],\n",
       "\n",
       "        [[17684, 17679, 17683,  ..., 17682, 17682, 17682],\n",
       "         [17684,    87,   416,  ..., 17682, 17682, 17682],\n",
       "         [17684,   183,   223,  ..., 17682, 17682, 17682],\n",
       "         ...,\n",
       "         [17684,   198,    57,  ..., 17682, 17682, 17682],\n",
       "         [17684,   774,   136,  ..., 17682, 17682, 17682],\n",
       "         [17684, 17680, 17683,  ..., 17682, 17682, 17682]]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = []\n",
    "batch_size, seq_len = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.1412, -0.0004,  0.0330,  ...,  0.0924, -0.0193, -0.0614],\n",
       "          [-0.1320,  0.1018,  0.0645,  ..., -0.0490, -0.1527, -0.0221],\n",
       "          [ 0.0084,  0.1108, -0.0935,  ..., -0.1273,  0.0898,  0.1059],\n",
       "          ...,\n",
       "          [-0.1228,  0.0211,  0.0883,  ...,  0.0661, -0.0490,  0.0166],\n",
       "          [-0.1228,  0.0211,  0.0883,  ...,  0.0661, -0.0490,  0.0166],\n",
       "          [-0.1228,  0.0211,  0.0883,  ...,  0.0661, -0.0490,  0.0166]],\n",
       " \n",
       "         [[-0.1412, -0.0004,  0.0330,  ...,  0.0924, -0.0193, -0.0614],\n",
       "          [ 0.0106,  0.0417,  0.0964,  ..., -0.0115,  0.0068, -0.0888],\n",
       "          [ 0.0636,  0.0399, -0.0362,  ...,  0.0624, -0.1155,  0.1449],\n",
       "          ...,\n",
       "          [-0.1228,  0.0211,  0.0883,  ...,  0.0661, -0.0490,  0.0166],\n",
       "          [-0.1228,  0.0211,  0.0883,  ...,  0.0661, -0.0490,  0.0166],\n",
       "          [-0.1228,  0.0211,  0.0883,  ...,  0.0661, -0.0490,  0.0166]],\n",
       " \n",
       "         [[-0.1412, -0.0004,  0.0330,  ...,  0.0924, -0.0193, -0.0614],\n",
       "          [ 0.1143, -0.1426, -0.1394,  ...,  0.0209, -0.0017,  0.1168],\n",
       "          [ 0.1416, -0.0663, -0.0329,  ...,  0.1446,  0.0578,  0.1379],\n",
       "          ...,\n",
       "          [-0.0339,  0.0241,  0.0033,  ..., -0.0926, -0.1095, -0.0251],\n",
       "          [-0.0832,  0.1084,  0.0430,  ...,  0.0638,  0.1192, -0.1032],\n",
       "          [ 0.1499, -0.0948, -0.1176,  ...,  0.0578,  0.1528,  0.1038]]],\n",
       "        device='cuda:0', grad_fn=<EmbeddingBackward>)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if word_emb_layer is not None:\n",
    "    batch_size, seq_len = word_inp.size()\n",
    "    variable = Variable(word_inp)\n",
    "    if use_cuda:\n",
    "        variable = variable.cuda()\n",
    "    word_emb = word_emb_layer(variable)\n",
    "    embs.append(word_emb)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 100])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb_layer is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 50])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 50])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_inp = chars_inp.view(batch_size * seq_len, -1)\n",
    "chars_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(17685, 50, padding_idx=17682)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb_layer.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_embedding = char_emb_layer(\n",
    "    Variable(chars_inp).cuda() if use_cuda\n",
    "    else Variable(chars_inp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 50, 50])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 50, 50])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_embedding = torch.transpose(character_embedding, 1, 2)\n",
    "character_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = getattr(torch.nn.functional, \n",
    "                     config['token_embedder']['activation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = convolutions[i](character_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 32, 50])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = torch.max(convolved, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 32])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = activation(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs.append(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([30, 32, 49])\n",
      "torch.Size([30, 32])\n",
      "torch.Size([30, 32])\n",
      "2\n",
      "torch.Size([30, 64, 48])\n",
      "torch.Size([30, 64])\n",
      "torch.Size([30, 64])\n",
      "3\n",
      "torch.Size([30, 128, 47])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128])\n",
      "4\n",
      "torch.Size([30, 256, 46])\n",
      "torch.Size([30, 256])\n",
      "torch.Size([30, 256])\n",
      "5\n",
      "torch.Size([30, 512, 45])\n",
      "torch.Size([30, 512])\n",
      "torch.Size([30, 512])\n",
      "6\n",
      "torch.Size([30, 1024, 44])\n",
      "torch.Size([30, 1024])\n",
      "torch.Size([30, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(convolutions)):\n",
    "    print(i)\n",
    "    convolved = convolutions[i](character_embedding)\n",
    "    print(convolved.shape)\n",
    "    convolved = torch.max(convolved, dim=-1)[0]\n",
    "    print(convolved.shape)\n",
    "    convolved = activation(convolved)\n",
    "    print(convolved.shape)\n",
    "    convs.append(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([30, 32]),\n",
       " torch.Size([30, 32]),\n",
       " torch.Size([30, 64]),\n",
       " torch.Size([30, 128]),\n",
       " torch.Size([30, 256]),\n",
       " torch.Size([30, 512]),\n",
       " torch.Size([30, 1024])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[conv.shape for conv in convs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2048])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb = torch.cat(convs, dim=-1)\n",
    "char_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2048])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb = highways(char_emb)\n",
    "char_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 2048])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_emb_ = char_emb.view(batch_size, -1, n_filters)\n",
    "chat_emb_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs.append(chat_emb_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 100]), torch.Size([3, 10, 2048]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[0].shape, embs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 2148])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding = torch.cat(embs, dim=2)\n",
    "token_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 512])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding = projection(token_embedding)\n",
    "token_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "         [ 0.0499,  0.0233, -0.0560,  ..., -0.0119, -0.0294,  0.0098],\n",
       "         [ 0.0358,  0.0143, -0.0375,  ...,  0.0022, -0.0452, -0.0209],\n",
       "         ...,\n",
       "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
       "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
       "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240]],\n",
       "\n",
       "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "         [ 0.0426,  0.0086, -0.0403,  ..., -0.0172, -0.0399,  0.0101],\n",
       "         [ 0.0151,  0.0227, -0.0583,  ...,  0.0283, -0.0380, -0.0247],\n",
       "         ...,\n",
       "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
       "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
       "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240]],\n",
       "\n",
       "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "         [ 0.0363,  0.0383, -0.0691,  ..., -0.0122, -0.0327, -0.0238],\n",
       "         [ 0.0329,  0.0252, -0.0403,  ..., -0.0023,  0.0078,  0.0058],\n",
       "         ...,\n",
       "         [ 0.0634,  0.0281, -0.0232,  ..., -0.0151, -0.0302, -0.0067],\n",
       "         [ 0.0193,  0.0227, -0.0390,  ...,  0.0225, -0.0381,  0.0253],\n",
       "         [ 0.0401,  0.0508, -0.0588,  ..., -0.0385, -0.0220,  0.0049]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이어서 계속\n",
    "- `Model.forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " tensor([ 0,  1,  2,  3,  4,  5, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28], device='cuda:0'),\n",
       " tensor([ 1,  2,  3,  4,  5,  6, 11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28, 29], device='cuda:0')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = Variable(mask_package[0]).cuda()\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ElmobiLM`을 이해하기 위해선\n",
    "- `PackedSequence, pad_packed_sequence, pack_padded_sequence`\n",
    "- `_EncoderBase`\n",
    "- `LSTMCellWithPorjection`\n",
    "\n",
    "을 이해해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules/utils.py\n",
    "\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([ 7,  6, 10], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_lengths_from_binary_sequence_mask(mask: torch.Tensor):\n",
    "    return mask.long().sum(-1)\n",
    "\n",
    "print(mask)\n",
    "print(get_lengths_from_binary_sequence_mask(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activate Function...\n",
      "\n",
      "sequence_lengths, and it's size = tensor([ 7,  6, 10], device='cuda:0') torch.Size([3])\n",
      "input tensor's size = torch.Size([3, 10, 512])\n",
      "Is datatype is Variable? True\n",
      "* sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "* permutation_index = tensor([2, 0, 1], device='cuda:0')\n",
      "Index Sorting...\n",
      "* sorted_tensor = \n",
      " tensor([[[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0363,  0.0383, -0.0691,  ..., -0.0122, -0.0327, -0.0238],\n",
      "         [ 0.0329,  0.0252, -0.0403,  ..., -0.0023,  0.0078,  0.0058],\n",
      "         ...,\n",
      "         [ 0.0634,  0.0281, -0.0232,  ..., -0.0151, -0.0302, -0.0067],\n",
      "         [ 0.0193,  0.0227, -0.0390,  ...,  0.0225, -0.0381,  0.0253],\n",
      "         [ 0.0401,  0.0508, -0.0588,  ..., -0.0385, -0.0220,  0.0049]],\n",
      "\n",
      "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0499,  0.0233, -0.0560,  ..., -0.0119, -0.0294,  0.0098],\n",
      "         [ 0.0358,  0.0143, -0.0375,  ...,  0.0022, -0.0452, -0.0209],\n",
      "         ...,\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240]],\n",
      "\n",
      "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0426,  0.0086, -0.0403,  ..., -0.0172, -0.0399,  0.0101],\n",
      "         [ 0.0151,  0.0227, -0.0583,  ...,  0.0283, -0.0380, -0.0247],\n",
      "         ...,\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240]]],\n",
      "       device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "index_range = tensor([0, 1, 2], device='cuda:0'), torch.int64, <class 'torch.Tensor'>\n",
      "<warning!> 왜 바로 아래 작업을 해주는지 이해 불가... 바뀌는게 없는거 같은데?\n",
      "index_range to Variable:long...\n",
      "index_range = tensor([0, 1, 2], device='cuda:0'), torch.int64, <class 'torch.Tensor'>\n",
      "reverse_mapping = tensor([1, 2, 0], device='cuda:0')\n",
      "* restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "\n",
      "Return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index\n"
     ]
    }
   ],
   "source": [
    "def sort_batch_by_length(tensor: torch.autograd.Variable,\n",
    "                         sequence_lengths: torch.autograd.Variable):\n",
    "    if not isinstance(tensor, Variable) or not isinstance(sequence_lengths, Variable):\n",
    "        raise Exception(\"Both the tensor and sequence lengths must be torch.autograd.Variables.\")\n",
    "        \n",
    "    sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n",
    "    sorted_tensor = tensor.index_select(0, permutation_index)\n",
    "    \n",
    "    # This is ugly, but required - we are creating a new variable at runtime, so we\n",
    "    # must ensure it has the correct CUDA vs non-CUDA type. We do this by cloning and\n",
    "    # refilling one of the inputs to the function.\n",
    "    index_range = sequence_lengths.data.clone().copy_(torch.arange(0, len(sequence_lengths)))\n",
    "    # This is the equivalent of zipping with index, sorting by the original\n",
    "    # sequence lengths and returning the now sorted indices.\n",
    "    index_range = Variable(index_range.long())\n",
    "    _, reverse_mapping = permutation_index.sort(0, descending=False)\n",
    "    restoration_indices = index_range.index_select(0, reverse_mapping)\n",
    "    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index\n",
    "\n",
    "# sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "# sort_batch_by_length(token_embedding, sequence_lengths)\n",
    "print('Activate Function...', end='\\n\\n')\n",
    "sequence_lengths = mask.long().sum(-1)\n",
    "print(\"sequence_lengths, and it's size = \", end='')\n",
    "print(sequence_lengths, sequence_lengths.size())\n",
    "print(\"input tensor's size = \", end='')\n",
    "print(token_embedding.size())\n",
    "print(\"Is datatype is Variable? \", end='')\n",
    "print(not (not isinstance(token_embedding, Variable) or \n",
    "       not isinstance(sequence_lengths, Variable)))\n",
    "\n",
    "sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n",
    "print('* sorted_sequence_lengths =', sorted_sequence_lengths)\n",
    "print('* permutation_index =', permutation_index)\n",
    "print(\"Index Sorting...\")\n",
    "sorted_tensor = token_embedding.index_select(0, permutation_index)\n",
    "print('* sorted_tensor = \\n', sorted_tensor)\n",
    "index_range = sequence_lengths.data.clone().copy_(torch.arange(0, len(sequence_lengths)))\n",
    "print(f'index_range = {index_range}, {index_range.dtype}, {type(index_range)}')\n",
    "print(\"<warning!> 왜 바로 아래 작업을 해주는지 이해 불가... 바뀌는게 없는거 같은데?\")\n",
    "print(\"index_range to Variable:long...\")\n",
    "index_range = Variable(index_range.long())\n",
    "print(f'index_range = {index_range}, {index_range.dtype}, {type(index_range)}')\n",
    "_, reverse_mapping = permutation_index.sort(0, descending=False)\n",
    "print(f\"reverse_mapping = {reverse_mapping}\")\n",
    "restoration_indices = index_range.index_select(0, reverse_mapping)\n",
    "print(f\"* restoration_indices = {restoration_indices}\", end=\"\\n\\n\")\n",
    "print('Return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules.encoder_base.py\n",
    "from typing import Tuple, Union, Optional, Callable\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "RnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]  # pylint: disable=invalid-name\n",
    "RnnStateStorage = Tuple[torch.Tensor, ...]  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _EncoderBase(nn.Module):\n",
    "    # pyling: disable=abstract-method\n",
    "    \"\"\"\n",
    "    This abstract class serves as a base for the 3 ``Encoder`` abstractions in AllenNLP.\n",
    "    - :class:`~allennlp.modules.seq2seq_encoders.Seq2SeqEncoders`\n",
    "    - :class:`~allennlp.modules.seq2vec_encoders.Seq2VecEncoders`\n",
    "    Additionally, this class provides functionality for sorting sequences by length\n",
    "    so they can be consumed by Pytorch RNN classes, which require their inputs to be\n",
    "    sorted by length. Finally, it also provides optional statefulness to all of it's\n",
    "    subclasses by allowing the caching and retrieving of the hidden states of RNNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, stateful: bool = False) -> None:\n",
    "        super(_EncoderBase, self).__init__()\n",
    "        self.stateful = stateful\n",
    "        self._states: Optional[RnnStateStorage] = None\n",
    "    \n",
    "    def sort_and_run_forward(self,\n",
    "                             module: Callable[[PackedSequence, Optional[RnnState]],\n",
    "                                              Tuple[Union[PackedSequence, torch.Tensor], RnnState]],\n",
    "                             inputs: torch.Tensor,\n",
    "                             mask: torch.Tensor,\n",
    "                             hidden_state: Optional[RnnState] = None):\n",
    "        \"\"\"\n",
    "        Pytorch RNNs는 input이 passing되기 전에 정렬되있어야 함\n",
    "        Seq2xxxEncoders가 이러한 기능을 모두 사용하기에 base class로 제공\n",
    "        \"\"\"\n",
    "        # In some circumstances you may have sequences of zero length. ``pack_padded_sequence``\n",
    "        # requires all sequence lengths to be > 0, so remove sequences of zero length before\n",
    "        # calling self._module, then fill with zeros.\n",
    "        \n",
    "        # First count how many sequences are empty.\n",
    "        batch_size = mask.size(0)\n",
    "        num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "        \n",
    "        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n",
    "            sort_batch_by_length(inputs, sequence_lengths)\n",
    "        \n",
    "        # Now create a PackedSequence with only the non-empty, sorted sequences.\n",
    "        # pad token 제외, 유의미한 값들만 packing\n",
    "        packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                                     sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                                     batch_first=True)\n",
    "        \n",
    "        # Prepare teh initial states.\n",
    "        if not self.stateful:\n",
    "            if hidden_state == None:\n",
    "                initial_states = hidden_state\n",
    "            elif isinstance(hidden_state, tuple):\n",
    "                initial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :]\n",
    "                                  for state in hidden_state]\n",
    "            else:\n",
    "                initial_stats = self._get_initial_states(batch_size, num_valid, sorting_indices)    \n",
    "        else:\n",
    "            initial_states = selt._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "            \n",
    "        # Actually call the module on the sorted PackedSequence\n",
    "        module_output, final_states = module(packed_sequence_input, initial_states)\n",
    "        \n",
    "        return module_output, final_states, restoration_indices\n",
    "    \n",
    "    def _get_initial_states(self,\n",
    "                            batch_size: int,\n",
    "                            num_valid: int,\n",
    "                            sorting_indices: torch.LongTensor) -> Optional[RnnState]:\n",
    "        \"\"\"\n",
    "        RNN의 초기 상태를 반환\n",
    "        추가적으로, 이 메서드는 batch의 새로운 요소의 초기 상태를 추가하기 위해 상태를 변경하여(mutate)\n",
    "            호출시 batch size를 처리\n",
    "        또한 이 메서드는 \n",
    "            1. 배치의 요소 seq. length로 상태를 정렬하는 것과\n",
    "            2. pad가 끝난 row 제거도 처리\n",
    "        중요한 것은 현재의 배치 크기가 이전에 호출되었을 때보다 더 크면 이 상태를 \"혼합\"하는 것이다.\n",
    "        \n",
    "        이 메서드는 (1) 처음 호출되어 아무 상태가 없는 경우 (2) RNN이 heterogeneous state를 가질 때\n",
    "        의 경우를 처리해야 하기 때문에 return값이 복잡함\n",
    "        \n",
    "        (1) module이 처음 호출됬을 때 ``module``의 타입이 무엇이든 ``None`` 반환\n",
    "        (2) Otherwise, \n",
    "            - LSTM의 경우 tuple of ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "                 and ``(num_layers, num_valid, memory_size)``\n",
    "            - GRU의 경우  single ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "        \"\"\"\n",
    "        # We don't know the state sizes the first time calling forward,\n",
    "        # so we let the module define what it's initial hidden state looks like.\n",
    "        if self._states is None:\n",
    "            return None\n",
    "        \n",
    "        # Otherwise, we have some previous states.\n",
    "        if batch_size > self._states[0].size(1):\n",
    "            # This batch is larger than the all previous states.\n",
    "            # If so, resize the states.\n",
    "            num_states_to_concat = batch_size - self._states[0].size(1)\n",
    "            resized_states = []\n",
    "            # state has shape (num_layers, batch_size, hidden_size)\n",
    "            for state in self._states:\n",
    "                # This _must_ be inside the loop because some\n",
    "                # RNNs have states with different last dimension sizes.\n",
    "                zeros = state.data.new(state.size(0),\n",
    "                                       num_states_to_concat,\n",
    "                                       state.size(2)).fill_(0)\n",
    "                zeros = Variable(zeros)\n",
    "                resized_states.append(torch.cat([state, zeros], 1))\n",
    "            self._states = tuple(resized_states)\n",
    "            correctly_shaped_states = self._states\n",
    "        elif batch_size < self._states[0].size(1):\n",
    "            # This batch is smaller than the previous one.\n",
    "            correctly_shaped_states = tuple(staet[:, :batch_size, :] for state in self._states)\n",
    "        else:\n",
    "            correctly_shaped_states = self._states\n",
    "            \n",
    "        # At this point, out states are of shape (num_layers, batch_size, hidden_size).\n",
    "        # However, the encoder uses sorted sequences and additionally removes elements\n",
    "        # of the batch which are fully padded. We need the states to match up to these\n",
    "        # sorted and filtered sequences, so we do that in the next two blocks before\n",
    "        # returning the states.\n",
    "        if len(self._states) == 1:\n",
    "            # GRU\n",
    "            correctly_shaped_state = correctly_shaped_states[0]\n",
    "            sorted_state = correctly_shaped_state.index_select(1, sorting_indices)\n",
    "            return sorted_state[:, :num_valid, :]\n",
    "        else:\n",
    "            # LSTM\n",
    "            sorted_states = [state.index_select(1, sorting_indices)\n",
    "                             for state in correctly_shaped_states]\n",
    "            return tuple(state[:, :num_valid, :] for state in sorted_stest)\n",
    "        \n",
    "    def _update_states(self,\n",
    "                       final_states: RnnStateStorage,\n",
    "                       restoration_indices: torch.LongTensor) -> None:\n",
    "        \"\"\"\n",
    "        RNN forward 동작 후에 state를 update\n",
    "        새로운 state로 update하며 몇 가지 book-keeping을 실시\n",
    "        즉, 상태를 해제하고 완전히 padding된 state가 업데이트되지 않도록 함\n",
    "        마지막으로 graph가 매 batch iteration후에 gc되도록 계산 그래프에서 \n",
    "        state variable을 떼어냄.\n",
    "        \"\"\"\n",
    "        # TODO(Mark)L seems weird to sort here, but append zeros in the subclasses.\n",
    "        # which way around is best?\n",
    "        new_unsorted_states = [state.index_select(1, restoration_indices)\n",
    "                               for state in final_states]\n",
    "        \n",
    "        if self._states is None:\n",
    "            # We don't already have states, so just set the\n",
    "            # ones we receive to be the current state.\n",
    "            self._states = tuple([Variable(state.data) \n",
    "                                  for state in new_unsorted_states])\n",
    "        else:\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            # Now we've sorted the states back so that they correspond to the original\n",
    "            # indices, we need to figure out what states we need to update, because if we\n",
    "            # didn't use a state for a particular row, we want to preserve its state.\n",
    "            # Thankfully, the rows which are all zero in the state correspond exactly\n",
    "            # to those which aren't used, so we create masks of shape (new_batch_size,),\n",
    "            # denoting which states were used in the RNN computation.\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            # Masks for the unused states of shape (1, new_batch_size, 1)\n",
    "            used_new_rows_mask = [(state[0, :, :].sum(-1)\n",
    "                                   != 0.0).float().view(1, new_state_batch_size, 1)\n",
    "                                  for state in new_unsorted_states]\n",
    "            new_states = []\n",
    "            if current_state_batch_size > new_state_batch_size:\n",
    "                # The new state is smaller than the old one,\n",
    "                # so just update the indices which we used.\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows in the previous state\n",
    "                    # which _were_ used in the current state.\n",
    "                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(old_state.data))\n",
    "            else:\n",
    "                # The states are the same size, so we just have to\n",
    "                # deal with the possibility that some rows weren't used.\n",
    "                new_states = []\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows which _were_ used in the current state.\n",
    "                    masked_old_state = old_state * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    new_state += masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(new_state.data))\n",
    "\n",
    "            # It looks like there should be another case handled here - when\n",
    "            # the current_state_batch_size < new_state_batch_size. However,\n",
    "            # this never happens, because the states themeselves are mutated\n",
    "            # by appending zeros when calling _get_inital_states, meaning that\n",
    "            # the new states are either of equal size, or smaller, in the case\n",
    "            # that there are some unused elements (zero-length) for the RNN computation.\n",
    "            self._states = tuple(new_states)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 3, num_valid = 3\n",
      "sequence_lengths = tensor([ 7,  6, 10], device='cuda:0')\n",
      "1. sorted_inputs = \n",
      "tensor([[[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0363,  0.0383, -0.0691,  ..., -0.0122, -0.0327, -0.0238],\n",
      "         [ 0.0329,  0.0252, -0.0403,  ..., -0.0023,  0.0078,  0.0058],\n",
      "         ...,\n",
      "         [ 0.0634,  0.0281, -0.0232,  ..., -0.0151, -0.0302, -0.0067],\n",
      "         [ 0.0193,  0.0227, -0.0390,  ...,  0.0225, -0.0381,  0.0253],\n",
      "         [ 0.0401,  0.0508, -0.0588,  ..., -0.0385, -0.0220,  0.0049]],\n",
      "\n",
      "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0499,  0.0233, -0.0560,  ..., -0.0119, -0.0294,  0.0098],\n",
      "         [ 0.0358,  0.0143, -0.0375,  ...,  0.0022, -0.0452, -0.0209],\n",
      "         ...,\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240]],\n",
      "\n",
      "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0426,  0.0086, -0.0403,  ..., -0.0172, -0.0399,  0.0101],\n",
      "         [ 0.0151,  0.0227, -0.0583,  ...,  0.0283, -0.0380, -0.0247],\n",
      "         ...,\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240]]],\n",
      "       device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "2. sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "3. restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "4. sorting_indices = tensor([2, 0, 1], device='cuda:0')\n",
      "             sorted_inputs.shape = torch.Size([3, 10, 512])\n",
      "packed_sequence_input.data.shape = torch.Size([23, 512])\n",
      "packed_sequence_input.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# .sort_and_run_forward()\n",
    "batch_size = mask.size(0)\n",
    "num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "print(f\"batch_size = {batch_size}, num_valid = {num_valid}\")\n",
    "\n",
    "sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "print(f\"sequence_lengths = {sequence_lengths}\")\n",
    "\n",
    "sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n",
    "    sort_batch_by_length(token_embedding, sequence_lengths)\n",
    "print(f\"1. sorted_inputs = \\n{sorted_inputs}\")\n",
    "print(f\"2. sorted_sequence_lengths = {sorted_sequence_lengths}\")\n",
    "print(f\"3. restoration_indices = {restoration_indices}\")\n",
    "print(f\"4. sorting_indices = {sorting_indices}\")\n",
    "packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                             sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                             batch_first=True)\n",
    "print(f\"             sorted_inputs.shape = {sorted_inputs.shape}\")\n",
    "print(f\"packed_sequence_input.data.shape = {packed_sequence_input.data.shape}\")\n",
    "print(f\"packed_sequence_input.batch_sizes = {packed_sequence_input.batch_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0396, 0.0363, 0.0329, 0.0104, 0.0173, 0.0526, 0.0410, 0.0634, 0.0193,\n",
       "         0.0401],\n",
       "        [0.0396, 0.0499, 0.0358, 0.0151, 0.0293, 0.0485, 0.0401, 0.0274, 0.0274,\n",
       "         0.0274],\n",
       "        [0.0396, 0.0426, 0.0151, 0.0124, 0.0608, 0.0401, 0.0274, 0.0274, 0.0274,\n",
       "         0.0274]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_inputs[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "        [ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "        [ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "        ...,\n",
       "        [ 0.0634,  0.0281, -0.0232,  ..., -0.0151, -0.0302, -0.0067],\n",
       "        [ 0.0193,  0.0227, -0.0390,  ...,  0.0225, -0.0381,  0.0253],\n",
       "        [ 0.0401,  0.0508, -0.0588,  ..., -0.0385, -0.0220,  0.0049]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_sequence_input.data[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 by 10 matrix with pad token which has value 0.0064\n",
    "```\n",
    "[\n",
    "    [0.0435, 0.0346, 0.0437, 0.0677, 0.0203, 0.0358, 0.0473, 0.0363, 0.0597, 0.0351],\n",
    "    [0.0435, 0.0684, 0.0423, 0.0844, 0.0396, 0.0619, 0.0351, 0.0064, 0.0064, 0.0064],\n",
    "    [0.0435, 0.0618, 0.0441, 0.0633, 0.0835, 0.0351, 0.0064, 0.0064, 0.0064, 0.0064]\n",
    "]\n",
    "```\n",
    "- pad token visualization\n",
    "```\n",
    "[\n",
    "    [0.0435, 0.0346, 0.0437, 0.0677, 0.0203, 0.0358, 0.0473, 0.0363, 0.0597, 0.0351],\n",
    "    [0.0435, 0.0684, 0.0423, 0.0844, 0.0396, 0.0619, 0.0351, -.----, -.----, -.----],\n",
    "    [0.0435, 0.0618, 0.0441, 0.0633, 0.0835, 0.0351, -.----, -.----, -.----, -.----]\n",
    "]\n",
    "```\n",
    "- count non-padding value (batch_sizes)\n",
    "```\n",
    "[\n",
    "    [-----3, -----3, -----3, -----3, -----3, -----3, -----2, -----1, -----1, -----1]\n",
    "]\n",
    "```\n",
    "- Extract pad token, and then merge data\n",
    "```\n",
    "[\n",
    "    0.0435, 0.0435, 0.0435,   # 3\n",
    "    0.0346, 0.0684, 0.0618,   # 3\n",
    "    0.0437, 0.0423, 0.0441,   # 3\n",
    "    0.0677, 0.0844, 0.0633,   # 3\n",
    "    0.0203, 0.0396, 0.0835,   # 3\n",
    "    0.0358, 0.0619, 0.0351,   # 3\n",
    "    0.0473, 0.0351,           # 2\n",
    "    0.0363,                   # 1\n",
    "    0.0597,                   # 1\n",
    "    0.0351                    # 1\n",
    "]\n",
    "```\n",
    "- \\* 512 Dimensions\n",
    "```\n",
    "tensor([[ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
    "        [ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
    "        [ 0.0435, -0.0570, -0.0156,  ...,  0.0872, -0.0066,  0.0430],\n",
    "        ...,\n",
    "        [ 0.0363, -0.0648,  0.0206,  ...,  0.1120, -0.0184,  0.0414],\n",
    "        [ 0.0597, -0.0749,  0.0128,  ...,  0.1011, -0.0258,  0.0474],\n",
    "        [ 0.0351, -0.0268,  0.0019,  ...,  0.0816,  0.0207,  0.0365]],\n",
    "       device='cuda:0', grad_fn=<SliceBackward>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'elmo',\n",
       " 'projection_dim': 512,\n",
       " 'cell_clip': 3,\n",
       " 'proj_clip': 3,\n",
       " 'dim': 4096,\n",
       " 'n_layers': 2}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout_mask(dropout_probability: float,\n",
    "                     tensor_for_masking: Variable):\n",
    "    binary_mask = tensor_for_masking.clone()\n",
    "    binary_mask.data.copy_(torch.rand(tensor_for_masking.size()) > dropout_probability)\n",
    "    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)\n",
    "    return dropout_mask\n",
    "\n",
    "def block_orthogonal(tensor: torch.Tensor,\n",
    "                     split_sizes: List[int],\n",
    "                     gain: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    An initializer which allows initaliizing model parametes in \"block\".\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, Variable):\n",
    "    # in pytorch 4.0, Variable equals Tensor\n",
    "    #     block_orthogonal(tensor.data, split_sizes, gain)\n",
    "    # else:\n",
    "        sizes = list(tensor.size())\n",
    "        if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n",
    "            raise ConfigurationError(\n",
    "                \"tensor dimentions must be divisible by their respective \"\n",
    "                f\"split_sizes. Found size: {size} and split_sizes: {split_sizes}\")\n",
    "        indexes = [list(range(0, max_size, split))\n",
    "                   for max_size, split in zip(sizes, split_sizes)]\n",
    "        # Iterate over all possible blocks within the tensor.\n",
    "        for block_start_indices in itertools.product(*indexes):\n",
    "            index_and_step_tuples = zip(block_start_indices, split_sizes)\n",
    "            block_slice = tuple([slice(start_index, start_index + step)\n",
    "                                 for start_index, step in index_and_step_tuples])\n",
    "            tensor[block_slice] = nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)\n",
    "            \n",
    "class LstmCellWithProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM with Recurrent Dropout and a projected and \n",
    "    clipped hidden state and memory.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 cell_size: int,\n",
    "                 go_forward: bool = True,\n",
    "                 recurrent_dropout_probability: float = 0.0,\n",
    "                 memory_cell_clip_value: Optional[float] = None,\n",
    "                 state_projection_clip_value: Optional[float] = None) -> None:\n",
    "        super(LstmCellWithProjection, self).__init__()\n",
    "        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        self.go_forward = go_forward\n",
    "        self.state_projection_clip_value = state_projection_clip_value\n",
    "        self.memory_cell_clip_value = memory_cell_clip_value\n",
    "        self.recurrent_dropout_probability = recurrent_dropout_probability\n",
    "\n",
    "        # We do the projections for all the gates all at once.\n",
    "        self.input_linearity = nn.Linear(input_size, 4 * cell_size, bias=False)\n",
    "        self.state_linearity = nn.Linear(hidden_size, 4 * cell_size, bias=True)\n",
    "\n",
    "        # Additional projection matrix for making the hidden state smaller.\n",
    "        self.state_projection = nn.Linear(cell_size, hidden_size, bias=False)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Use sensible default initializations for parameters.\n",
    "        block_orthogonal(self.input_linearity.weight.data, [self.cell_size, self.input_size])\n",
    "        block_orthogonal(self.state_linearity.weight.data, [self.cell_size, self.hidden_size])\n",
    "        \n",
    "        self.state_linearity.bias.data.fill_(0.0)\n",
    "        # Initialize forget gate biases to 1.0 as per An Empirical\n",
    "        # Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n",
    "        self.state_linearity.bias.data[self.cell_size:2 * self.cell_size].fill_(1.0)\n",
    "        \n",
    "    def forward(self,  # pylint: disabled=arguments-differ\n",
    "                inputs: torch.FloatTensor,\n",
    "                batch_lengths: List[int],\n",
    "                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        batch_size = inputs.size()[0]\n",
    "        total_timesteps = inputs.size()[1]\n",
    "        \n",
    "        output_accumulator = Variable(inputs.data.new(batch_size,\n",
    "                                                      total_timesteps,\n",
    "                                                      self.hidden_size).fill_(0))\n",
    "        if initial_state is None:\n",
    "            full_batch_previous_memory = Variable(inputs.data.new(batch_size,\n",
    "                                                                  self.cell_size).fill_(0))\n",
    "            full_batch_previous_state = Variable(inputs.data.new(batch_size,\n",
    "                                                                  self.hidden_size).fill_(0))\n",
    "        else:\n",
    "            full_batch_previous_state = initial_state[0].squeeze(0)\n",
    "            full_batch_previous_memory = initial_state[1].squeeze(0)\n",
    "        \n",
    "        current_length_index = batch_size - 1 if self.go_forward else 0\n",
    "        if self.recurrent_dropout_probability > 0.0 and self.training:\n",
    "            dropout_mask = get_dropout_mask(self.recurrent_dropout_probability,\n",
    "                                            full_batch_previous_state)\n",
    "        else:\n",
    "            dropout_mask = None\n",
    "            \n",
    "        for timestep in range(total_timesteps):\n",
    "            index = timestep if self.go_forward else total_timesteps - timestep - 1\n",
    "            \n",
    "            if self.go_forward:\n",
    "                while batch_lengths[current_length_index] <= index:\n",
    "                    current_length_index -= 1\n",
    "            else:\n",
    "                while (current_length_index < (len(batch_lengths) - 1) and\n",
    "                       batch_lengths[current_length_index + 1] > index):\n",
    "                    current_length_index += 1\n",
    "            \n",
    "            # shape (batch_size, cell_size)\n",
    "            previous_memory = full_batch_previous_memory[:current_length_index+1].clone()\n",
    "            # shape (batch_size, cell_size)\n",
    "            previous_state = full_batch_previous_state[:current_length_index+1].clone()\n",
    "            # shape (batch_size, input_size)\n",
    "            timestep_input = inputs[:current_length_index+1, index]\n",
    "            \n",
    "            # Do the projections for all the gates all at once.\n",
    "            # Both have shape (batch_size, 4 * cell_size)\n",
    "            projected_input = self.input_linearity(timestep_input)\n",
    "            projected_state = self.state_linearity(previous_state)\n",
    "            \n",
    "            # Main LSTM equations using relevant chunks of the big linear\n",
    "            # projections of the hidden state and inputs.\n",
    "            input_gate = torch.sigmoid(projected_input[:, (0 * self.cell_size):(1 * self.cell_size)] +\n",
    "                                       projected_state[:, (0 * self.cell_size):(1 * self.cell_size)])\n",
    "            forget_gate = torch.sigmoid(projected_input[:, (1 * self.cell_size):(2 * self.cell_size)] +\n",
    "                                        projected_state[:, (1 * self.cell_size):(2 * self.cell_size)])\n",
    "            memory_init = torch.tanh(projected_input[:, (2 * self.cell_size):(3 * self.cell_size)] +\n",
    "                                     projected_input[:, (2 * self.cell_size):(3 * self.cell_size)])\n",
    "            output_gate = torch.sigmoid(projected_input[:, (3 * self.cell_size):(4 * self.cell_size)] + \n",
    "                                        projected_input[:, (3 * self.cell_size):(4 * self.cell_size)])\n",
    "            memory = input_gate * memory_init + forget_gate * previous_memory\n",
    "            \n",
    "            # Here is the non-standard part of this LSTM cell;\n",
    "            # First, we clip the memory cell,\n",
    "            # Then we project the output of the timestep to a smaller size\n",
    "            # and again clip it.\n",
    "            \n",
    "            if self.memory_cell_clip_valie:\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                memory = torch.clamp(memory, -self.memory_cell_clip_value, self.memory_cell_clip_value)\n",
    "                \n",
    "            # shape (current_length_index, cell_size)\n",
    "            pre_projection_timestep_output = output_gate * torch.tanh(memory)\n",
    "            \n",
    "            # shape (current_length_index, hidden_size)\n",
    "            timestep_output = self.state_projection(pre_projection_timestep_output)\n",
    "            if self.state_projection_clip_value:\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                timestep_output = torch.clamp(timestep_output,\n",
    "                                              -self.state_projection_clip_value,\n",
    "                                              self.state_projection_clip_value)\n",
    "                \n",
    "            # Only do dropout if the dropout prob is > 0.0 and we are in training mode.\n",
    "            if dropout_mask is not None:\n",
    "                timestep_output = timestep_output * dropout_mask[0: current_length_index + 1]\n",
    "                \n",
    "            full_batch_previous_memory = Variable(full_batch_previous_memory.data.clone())\n",
    "            full_batch_previous_state = Variable(full_batch_previous_state.data.clone())\n",
    "            full_batch_previous_memory[0:current_length_index + 1] = memory\n",
    "            full_batch_previous_state[0:current_length_index + 1] = timestep_output\n",
    "            output_accumulator[0:current_length_index + 1, index] = timestep_output\n",
    "            \n",
    "        # Mimic the pytorch API by returning state in the following shape:\n",
    "        # (num_layers * num_directions, batch_size, ...). As this\n",
    "        # LSTM cell cannot be stacked, the first dimension here is just 1.\n",
    "        final_state = (full_batch_previous_state.unsqueeze(0),\n",
    "                       full_batch_previous_memory.unsqueeze(0))\n",
    "\n",
    "        return output_accumulator, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super.__init__()\n",
    "stateful = True\n",
    "_states = None\n",
    "config = config\n",
    "use_cuda = use_cuda\n",
    "input_size = config['encoder']['projection_dim']\n",
    "hidden_size = config['encoder']['projection_dim']\n",
    "cell_size = config['encoder']['dim']\n",
    "num_layers = config['encoder']['n_layers']\n",
    "memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "state_projection_clilp_value = config['encoder']['proj_clip']\n",
    "recurrent_dropout_probability = config['dropout']\n",
    "\n",
    "forward_layers = []\n",
    "backward_layers = []\n",
    "\n",
    "lstm_input_size = input_size\n",
    "go_forward = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_linearity = nn.Linear(input_size, 4 * cell_size, bias=False).cuda()\n",
    "state_linearity = nn.Linear(input_size, 4 * cell_size, bias=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=16384, bias=False)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=16384, bias=True)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0125, -0.0367, -0.0396,  ...,  0.0316,  0.0382,  0.0386],\n",
       "        [ 0.0068, -0.0066,  0.0311,  ...,  0.0317,  0.0357, -0.0033],\n",
       "        [ 0.0354, -0.0289, -0.0034,  ..., -0.0021, -0.0140, -0.0085],\n",
       "        ...,\n",
       "        [ 0.0037,  0.0243, -0.0260,  ..., -0.0266, -0.0075,  0.0020],\n",
       "        [-0.0262,  0.0256, -0.0024,  ...,  0.0221,  0.0126, -0.0210],\n",
       "        [-0.0272, -0.0062,  0.0064,  ..., -0.0015,  0.0244, -0.0199]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_linearity.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_sizes = [4096, 512]\n",
      "sizes = [16384, 512]\n",
      "indexes = [[0, 4096, 8192, 12288], [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0), (4096, 0), (8192, 0), (12288, 0)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = input_linearity.weight.data\n",
    "split_sizes = [cell_size, input_size]\n",
    "print(f\"split_sizes = {split_sizes}\")\n",
    "sizes = list(tensor.size())\n",
    "print(f\"sizes = {sizes}\")\n",
    "indexes = [list(range(0, max_size, split))\n",
    "           for max_size, split in zip(sizes, split_sizes)]\n",
    "print(f\"indexes = {indexes}\")\n",
    "list(itertools.product(*indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 4096, cols = 512\n",
      "q = torch.Size([4096, 512]), r = torch.Size([512, 512])\n",
      "tensor([[-0.0125, -0.0367, -0.0396,  ...,  0.0316,  0.0382,  0.0386],\n",
      "        [ 0.0068, -0.0066,  0.0311,  ...,  0.0317,  0.0357, -0.0033],\n",
      "        [ 0.0354, -0.0289, -0.0034,  ..., -0.0021, -0.0140, -0.0085],\n",
      "        ...,\n",
      "        [ 0.0102, -0.0014,  0.0340,  ...,  0.0309,  0.0014,  0.0085],\n",
      "        [-0.0130,  0.0344,  0.0049,  ...,  0.0133,  0.0186,  0.0099],\n",
      "        [ 0.0319,  0.0087,  0.0203,  ..., -0.0413,  0.0372, -0.0247]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0256, -0.0093,  0.0088,  ...,  0.0242,  0.0246,  0.0182],\n",
      "        [-0.0087,  0.0250,  0.0071,  ..., -0.0050,  0.0180, -0.0267],\n",
      "        [ 0.0198, -0.0035, -0.0167,  ...,  0.0059,  0.0213,  0.0045],\n",
      "        ...,\n",
      "        [ 0.0043, -0.0083, -0.0325,  ...,  0.0012,  0.0110, -0.0048],\n",
      "        [-0.0033,  0.0061,  0.0254,  ...,  0.0062, -0.0063, -0.0116],\n",
      "        [-0.0039,  0.0285,  0.0224,  ..., -0.0008,  0.0007, -0.0126]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0256, -0.0093,  0.0088,  ...,  0.0242,  0.0246,  0.0182],\n",
      "        [-0.0087,  0.0250,  0.0071,  ..., -0.0050,  0.0180, -0.0267],\n",
      "        [ 0.0198, -0.0035, -0.0167,  ...,  0.0059,  0.0213,  0.0045],\n",
      "        ...,\n",
      "        [ 0.0043, -0.0083, -0.0325,  ...,  0.0012,  0.0110, -0.0048],\n",
      "        [-0.0033,  0.0061,  0.0254,  ...,  0.0062, -0.0063, -0.0116],\n",
      "        [-0.0039,  0.0285,  0.0224,  ..., -0.0008,  0.0007, -0.0126]],\n",
      "       device='cuda:0')\n",
      "rows = 4096, cols = 512\n",
      "q = torch.Size([4096, 512]), r = torch.Size([512, 512])\n",
      "tensor([[-0.0117, -0.0205,  0.0122,  ...,  0.0124, -0.0201,  0.0428],\n",
      "        [-0.0070,  0.0279,  0.0324,  ...,  0.0230, -0.0213, -0.0095],\n",
      "        [ 0.0204,  0.0253,  0.0134,  ..., -0.0236, -0.0053,  0.0253],\n",
      "        ...,\n",
      "        [-0.0439, -0.0224,  0.0344,  ..., -0.0210, -0.0123,  0.0345],\n",
      "        [ 0.0004,  0.0017, -0.0270,  ..., -0.0422, -0.0295, -0.0033],\n",
      "        [-0.0349,  0.0215,  0.0249,  ..., -0.0271, -0.0165,  0.0007]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0173,  0.0334, -0.0102,  ...,  0.0255,  0.0189,  0.0006],\n",
      "        [-0.0080, -0.0178,  0.0247,  ..., -0.0104,  0.0089, -0.0248],\n",
      "        [ 0.0228, -0.0223,  0.0061,  ...,  0.0192, -0.0162, -0.0086],\n",
      "        ...,\n",
      "        [-0.0184, -0.0026, -0.0218,  ...,  0.0068,  0.0018,  0.0239],\n",
      "        [-0.0179,  0.0271, -0.0077,  ...,  0.0145,  0.0021,  0.0080],\n",
      "        [-0.0336, -0.0071, -0.0088,  ..., -0.0247, -0.0015, -0.0105]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0173,  0.0334, -0.0102,  ...,  0.0255,  0.0189,  0.0006],\n",
      "        [-0.0080, -0.0178,  0.0247,  ..., -0.0104,  0.0089, -0.0248],\n",
      "        [ 0.0228, -0.0223,  0.0061,  ...,  0.0192, -0.0162, -0.0086],\n",
      "        ...,\n",
      "        [-0.0184, -0.0026, -0.0218,  ...,  0.0068,  0.0018,  0.0239],\n",
      "        [-0.0179,  0.0271, -0.0077,  ...,  0.0145,  0.0021,  0.0080],\n",
      "        [-0.0336, -0.0071, -0.0088,  ..., -0.0247, -0.0015, -0.0105]],\n",
      "       device='cuda:0')\n",
      "rows = 4096, cols = 512\n",
      "q = torch.Size([4096, 512]), r = torch.Size([512, 512])\n",
      "tensor([[ 0.0400,  0.0133, -0.0176,  ...,  0.0402, -0.0159,  0.0203],\n",
      "        [-0.0434, -0.0278, -0.0363,  ..., -0.0130,  0.0131, -0.0151],\n",
      "        [ 0.0059,  0.0354, -0.0423,  ..., -0.0303, -0.0397, -0.0273],\n",
      "        ...,\n",
      "        [-0.0083,  0.0331, -0.0076,  ...,  0.0070, -0.0126,  0.0421],\n",
      "        [ 0.0339,  0.0164, -0.0241,  ...,  0.0113,  0.0114, -0.0317],\n",
      "        [ 0.0133, -0.0169,  0.0121,  ..., -0.0102, -0.0080,  0.0032]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0028,  0.0059,  0.0058,  ...,  0.0017,  0.0052, -0.0002],\n",
      "        [ 0.0076,  0.0002, -0.0026,  ..., -0.0031, -0.0025,  0.0037],\n",
      "        [ 0.0043, -0.0002, -0.0251,  ...,  0.0004,  0.0196,  0.0070],\n",
      "        ...,\n",
      "        [ 0.0079,  0.0066, -0.0120,  ...,  0.0240,  0.0103, -0.0063],\n",
      "        [-0.0069, -0.0111, -0.0215,  ...,  0.0111,  0.0078,  0.0104],\n",
      "        [-0.0063,  0.0020, -0.0045,  ...,  0.0089,  0.0035, -0.0163]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0028,  0.0059,  0.0058,  ...,  0.0017,  0.0052, -0.0002],\n",
      "        [ 0.0076,  0.0002, -0.0026,  ..., -0.0031, -0.0025,  0.0037],\n",
      "        [ 0.0043, -0.0002, -0.0251,  ...,  0.0004,  0.0196,  0.0070],\n",
      "        ...,\n",
      "        [ 0.0079,  0.0066, -0.0120,  ...,  0.0240,  0.0103, -0.0063],\n",
      "        [-0.0069, -0.0111, -0.0215,  ...,  0.0111,  0.0078,  0.0104],\n",
      "        [-0.0063,  0.0020, -0.0045,  ...,  0.0089,  0.0035, -0.0163]],\n",
      "       device='cuda:0')\n",
      "rows = 4096, cols = 512\n",
      "q = torch.Size([4096, 512]), r = torch.Size([512, 512])\n",
      "tensor([[ 0.0058, -0.0272,  0.0129,  ...,  0.0324, -0.0334,  0.0393],\n",
      "        [ 0.0060,  0.0047,  0.0230,  ..., -0.0209, -0.0260, -0.0124],\n",
      "        [ 0.0329, -0.0146, -0.0362,  ...,  0.0111,  0.0216, -0.0328],\n",
      "        ...,\n",
      "        [ 0.0037,  0.0243, -0.0260,  ..., -0.0266, -0.0075,  0.0020],\n",
      "        [-0.0262,  0.0256, -0.0024,  ...,  0.0221,  0.0126, -0.0210],\n",
      "        [-0.0272, -0.0062,  0.0064,  ..., -0.0015,  0.0244, -0.0199]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0029, -0.0127, -0.0053,  ..., -0.0061, -0.0204,  0.0066],\n",
      "        [ 0.0046, -0.0367, -0.0160,  ...,  0.0050,  0.0255,  0.0066],\n",
      "        [-0.0006,  0.0183, -0.0104,  ..., -0.0172, -0.0139, -0.0078],\n",
      "        ...,\n",
      "        [-0.0122,  0.0173, -0.0311,  ...,  0.0209, -0.0011, -0.0071],\n",
      "        [-0.0056, -0.0181, -0.0036,  ...,  0.0211, -0.0177,  0.0132],\n",
      "        [ 0.0112, -0.0225, -0.0079,  ..., -0.0372,  0.0141, -0.0045]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0029, -0.0127, -0.0053,  ..., -0.0061, -0.0204,  0.0066],\n",
      "        [ 0.0046, -0.0367, -0.0160,  ...,  0.0050,  0.0255,  0.0066],\n",
      "        [-0.0006,  0.0183, -0.0104,  ..., -0.0172, -0.0139, -0.0078],\n",
      "        ...,\n",
      "        [-0.0122,  0.0173, -0.0311,  ...,  0.0209, -0.0011, -0.0071],\n",
      "        [-0.0056, -0.0181, -0.0036,  ...,  0.0211, -0.0177,  0.0132],\n",
      "        [ 0.0112, -0.0225, -0.0079,  ..., -0.0372,  0.0141, -0.0045]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "gain = 1.0\n",
    "for block_start_indices in itertools.product(*indexes):\n",
    "    index_and_step_tuples = zip(block_start_indices, split_sizes)\n",
    "    block_slice = tuple([slice(start_index, start_index + step)\n",
    "                     for start_index, step in index_and_step_tuples])\n",
    "    tensor_ = tensor[block_slice].contiguous()\n",
    "\n",
    "    rows = tensor_.size(0)\n",
    "    cols = tensor_.numel() // rows\n",
    "    print(f\"rows = {rows}, cols = {cols}\")\n",
    "\n",
    "    flattened = tensor_.new(rows, cols).normal_(0, 1)\n",
    "    if rows < cols:\n",
    "        print('Transposing...')\n",
    "        flattened.t_() # transpose\n",
    "\n",
    "    # Compute the qr factorization\n",
    "    q, r = torch.qr(flattened)\n",
    "    print(f\"q = {q.shape}, r = {r.shape}\")\n",
    "\n",
    "    # Make Q uniform according to https://arvix.arg/pdf/math-ph/0609050.pdf\n",
    "    d = torch.diag(r, 0)\n",
    "    ph = d.sign()\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        print('Transposing...')\n",
    "        q.t_()\n",
    "\n",
    "    print(tensor_)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tensor_.view_as(q).copy_(q)\n",
    "        print(tensor_)\n",
    "        tensor_.mul_(gain)\n",
    "        print(tensor_)\n",
    "        \n",
    "    tensor[block_slice] = tensor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0256, -0.0093,  0.0088,  ...,  0.0242,  0.0246,  0.0182],\n",
       "        [-0.0087,  0.0250,  0.0071,  ..., -0.0050,  0.0180, -0.0267],\n",
       "        [ 0.0198, -0.0035, -0.0167,  ...,  0.0059,  0.0213,  0.0045],\n",
       "        ...,\n",
       "        [-0.0122,  0.0173, -0.0311,  ...,  0.0209, -0.0011, -0.0071],\n",
       "        [-0.0056, -0.0181, -0.0036,  ...,  0.0211, -0.0177,  0.0132],\n",
       "        [ 0.0112, -0.0225, -0.0079,  ..., -0.0372,  0.0141, -0.0045]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_linearity.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_sizes = [4096, 512]\n",
      "sizes = [16384, 512]\n",
      "indexes = [[0, 4096, 8192, 12288], [0]]\n",
      "rows = 4096, cols = 512\n",
      "q = torch.Size([4096, 512]), r = torch.Size([512, 512])\n",
      "tensor([[ 0.0401,  0.0197, -0.0396,  ..., -0.0049,  0.0370,  0.0252],\n",
      "        [-0.0369, -0.0122,  0.0224,  ..., -0.0375, -0.0038,  0.0171],\n",
      "        [-0.0437,  0.0173, -0.0043,  ..., -0.0433, -0.0338, -0.0372],\n",
      "        ...,\n",
      "        [ 0.0360,  0.0189, -0.0399,  ..., -0.0028, -0.0402,  0.0200],\n",
      "        [-0.0176, -0.0133, -0.0099,  ...,  0.0413,  0.0417, -0.0329],\n",
      "        [ 0.0229, -0.0113, -0.0234,  ..., -0.0361,  0.0274, -0.0082]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0041, -0.0165,  0.0173,  ...,  0.0098,  0.0182,  0.0195],\n",
      "        [ 0.0005,  0.0054, -0.0018,  ..., -0.0112, -0.0138, -0.0080],\n",
      "        [-0.0131,  0.0186, -0.0121,  ...,  0.0028,  0.0146,  0.0051],\n",
      "        ...,\n",
      "        [-0.0069, -0.0264, -0.0254,  ..., -0.0117,  0.0115, -0.0074],\n",
      "        [ 0.0241, -0.0021, -0.0039,  ...,  0.0119, -0.0062,  0.0135],\n",
      "        [-0.0003,  0.0114, -0.0234,  ..., -0.0193, -0.0039,  0.0200]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0041, -0.0165,  0.0173,  ...,  0.0098,  0.0182,  0.0195],\n",
      "        [ 0.0005,  0.0054, -0.0018,  ..., -0.0112, -0.0138, -0.0080],\n",
      "        [-0.0131,  0.0186, -0.0121,  ...,  0.0028,  0.0146,  0.0051],\n",
      "        ...,\n",
      "        [-0.0069, -0.0264, -0.0254,  ..., -0.0117,  0.0115, -0.0074],\n",
      "        [ 0.0241, -0.0021, -0.0039,  ...,  0.0119, -0.0062,  0.0135],\n",
      "        [-0.0003,  0.0114, -0.0234,  ..., -0.0193, -0.0039,  0.0200]],\n",
      "       device='cuda:0')\n",
      "rows = 4096, cols = 512\n",
      "q = torch.Size([4096, 512]), r = torch.Size([512, 512])\n",
      "tensor([[ 0.0338,  0.0219, -0.0361,  ...,  0.0372,  0.0079, -0.0097],\n",
      "        [-0.0393, -0.0192,  0.0374,  ...,  0.0279, -0.0133,  0.0433],\n",
      "        [-0.0318, -0.0300, -0.0090,  ..., -0.0215, -0.0162, -0.0401],\n",
      "        ...,\n",
      "        [-0.0073, -0.0093,  0.0367,  ...,  0.0017,  0.0126,  0.0100],\n",
      "        [-0.0411,  0.0362,  0.0306,  ...,  0.0146, -0.0293,  0.0300],\n",
      "        [-0.0442,  0.0143, -0.0308,  ..., -0.0279,  0.0292,  0.0127]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0115, -0.0160, -0.0016,  ...,  0.0155,  0.0158, -0.0064],\n",
      "        [ 0.0022, -0.0148,  0.0030,  ..., -0.0195, -0.0066, -0.0145],\n",
      "        [-0.0043,  0.0160,  0.0091,  ...,  0.0109,  0.0194, -0.0151],\n",
      "        ...,\n",
      "        [ 0.0206,  0.0107, -0.0031,  ...,  0.0083, -0.0134, -0.0226],\n",
      "        [-0.0175,  0.0115, -0.0290,  ..., -0.0346, -0.0037, -0.0167],\n",
      "        [ 0.0176, -0.0265,  0.0104,  ...,  0.0156,  0.0278,  0.0237]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0115, -0.0160, -0.0016,  ...,  0.0155,  0.0158, -0.0064],\n",
      "        [ 0.0022, -0.0148,  0.0030,  ..., -0.0195, -0.0066, -0.0145],\n",
      "        [-0.0043,  0.0160,  0.0091,  ...,  0.0109,  0.0194, -0.0151],\n",
      "        ...,\n",
      "        [ 0.0206,  0.0107, -0.0031,  ...,  0.0083, -0.0134, -0.0226],\n",
      "        [-0.0175,  0.0115, -0.0290,  ..., -0.0346, -0.0037, -0.0167],\n",
      "        [ 0.0176, -0.0265,  0.0104,  ...,  0.0156,  0.0278,  0.0237]],\n",
      "       device='cuda:0')\n",
      "rows = 4096, cols = 512\n",
      "q = torch.Size([4096, 512]), r = torch.Size([512, 512])\n",
      "tensor([[ 0.0125, -0.0135,  0.0263,  ...,  0.0195,  0.0251,  0.0409],\n",
      "        [-0.0252,  0.0089, -0.0210,  ...,  0.0069,  0.0432, -0.0357],\n",
      "        [-0.0168,  0.0311,  0.0110,  ...,  0.0394, -0.0012, -0.0273],\n",
      "        ...,\n",
      "        [ 0.0252, -0.0059,  0.0119,  ...,  0.0192,  0.0303,  0.0145],\n",
      "        [-0.0029,  0.0194,  0.0406,  ...,  0.0178,  0.0326,  0.0384],\n",
      "        [ 0.0226, -0.0026, -0.0310,  ..., -0.0167,  0.0070, -0.0202]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0057, -0.0019,  0.0145,  ...,  0.0032, -0.0039,  0.0138],\n",
      "        [ 0.0223,  0.0022, -0.0197,  ..., -0.0053, -0.0032,  0.0093],\n",
      "        [ 0.0106,  0.0018, -0.0053,  ..., -0.0090, -0.0057,  0.0142],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0140,  0.0157,  ..., -0.0018,  0.0190,  0.0122],\n",
      "        [-0.0110,  0.0097, -0.0471,  ..., -0.0159, -0.0155,  0.0159],\n",
      "        [ 0.0125, -0.0202,  0.0003,  ...,  0.0153,  0.0148,  0.0071]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0057, -0.0019,  0.0145,  ...,  0.0032, -0.0039,  0.0138],\n",
      "        [ 0.0223,  0.0022, -0.0197,  ..., -0.0053, -0.0032,  0.0093],\n",
      "        [ 0.0106,  0.0018, -0.0053,  ..., -0.0090, -0.0057,  0.0142],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0140,  0.0157,  ..., -0.0018,  0.0190,  0.0122],\n",
      "        [-0.0110,  0.0097, -0.0471,  ..., -0.0159, -0.0155,  0.0159],\n",
      "        [ 0.0125, -0.0202,  0.0003,  ...,  0.0153,  0.0148,  0.0071]],\n",
      "       device='cuda:0')\n",
      "rows = 4096, cols = 512\n",
      "q = torch.Size([4096, 512]), r = torch.Size([512, 512])\n",
      "tensor([[ 0.0052,  0.0206, -0.0393,  ..., -0.0144,  0.0082, -0.0398],\n",
      "        [ 0.0132, -0.0333,  0.0261,  ...,  0.0064, -0.0346,  0.0133],\n",
      "        [-0.0242,  0.0398, -0.0159,  ...,  0.0187, -0.0186,  0.0420],\n",
      "        ...,\n",
      "        [-0.0202, -0.0300,  0.0411,  ...,  0.0057,  0.0324, -0.0125],\n",
      "        [ 0.0038,  0.0214,  0.0060,  ..., -0.0365,  0.0106, -0.0094],\n",
      "        [ 0.0017, -0.0128, -0.0092,  ...,  0.0262,  0.0425,  0.0429]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0215,  0.0037,  0.0070,  ...,  0.0059, -0.0209, -0.0037],\n",
      "        [-0.0123, -0.0258,  0.0201,  ..., -0.0324, -0.0152,  0.0218],\n",
      "        [-0.0027, -0.0178, -0.0191,  ...,  0.0094, -0.0013,  0.0140],\n",
      "        ...,\n",
      "        [-0.0233,  0.0033,  0.0041,  ..., -0.0005, -0.0108, -0.0150],\n",
      "        [-0.0098,  0.0055, -0.0159,  ..., -0.0111, -0.0057,  0.0096],\n",
      "        [-0.0284, -0.0272, -0.0015,  ...,  0.0031,  0.0232, -0.0123]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0215,  0.0037,  0.0070,  ...,  0.0059, -0.0209, -0.0037],\n",
      "        [-0.0123, -0.0258,  0.0201,  ..., -0.0324, -0.0152,  0.0218],\n",
      "        [-0.0027, -0.0178, -0.0191,  ...,  0.0094, -0.0013,  0.0140],\n",
      "        ...,\n",
      "        [-0.0233,  0.0033,  0.0041,  ..., -0.0005, -0.0108, -0.0150],\n",
      "        [-0.0098,  0.0055, -0.0159,  ..., -0.0111, -0.0057,  0.0096],\n",
      "        [-0.0284, -0.0272, -0.0015,  ...,  0.0031,  0.0232, -0.0123]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor = state_linearity.weight.data\n",
    "split_sizes = [cell_size, input_size]\n",
    "print(f\"split_sizes = {split_sizes}\")\n",
    "sizes = list(tensor.size())\n",
    "print(f\"sizes = {sizes}\")\n",
    "indexes = [list(range(0, max_size, split))\n",
    "           for max_size, split in zip(sizes, split_sizes)]\n",
    "print(f\"indexes = {indexes}\")\n",
    "list(itertools.product(*indexes))\n",
    "\n",
    "gain = 1.0\n",
    "for block_start_indices in itertools.product(*indexes):\n",
    "    index_and_step_tuples = zip(block_start_indices, split_sizes)\n",
    "    block_slice = tuple([slice(start_index, start_index + step)\n",
    "                     for start_index, step in index_and_step_tuples])\n",
    "    tensor_ = tensor[block_slice].contiguous()\n",
    "\n",
    "    rows = tensor_.size(0)\n",
    "    cols = tensor_.numel() // rows\n",
    "    print(f\"rows = {rows}, cols = {cols}\")\n",
    "\n",
    "    flattened = tensor_.new(rows, cols).normal_(0, 1)\n",
    "    if rows < cols:\n",
    "        print('Transposing...')\n",
    "        flattened.t_() # transpose\n",
    "\n",
    "    # Compute the qr factorization\n",
    "    q, r = torch.qr(flattened)\n",
    "    print(f\"q = {q.shape}, r = {r.shape}\")\n",
    "\n",
    "    # Make Q uniform according to https://arvix.arg/pdf/math-ph/0609050.pdf\n",
    "    d = torch.diag(r, 0)\n",
    "    ph = d.sign()\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        print('Transposing...')\n",
    "        q.t_()\n",
    "\n",
    "    print(tensor_)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tensor_.view_as(q).copy_(q)\n",
    "        print(tensor_)\n",
    "        tensor_.mul_(gain)\n",
    "        print(tensor_)\n",
    "        \n",
    "    tensor[block_slice] = tensor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0041, -0.0165,  0.0173,  ...,  0.0098,  0.0182,  0.0195],\n",
       "        [ 0.0005,  0.0054, -0.0018,  ..., -0.0112, -0.0138, -0.0080],\n",
       "        [-0.0131,  0.0186, -0.0121,  ...,  0.0028,  0.0146,  0.0051],\n",
       "        ...,\n",
       "        [-0.0233,  0.0033,  0.0041,  ..., -0.0005, -0.0108, -0.0150],\n",
       "        [-0.0098,  0.0055, -0.0159,  ..., -0.0111, -0.0057,  0.0096],\n",
       "        [-0.0284, -0.0272, -0.0015,  ...,  0.0031,  0.0232, -0.0123]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_linearity.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_linearity.bias.data.fill_(0.0)\n",
    "# Initialize forget gate biases to 1.0 as per An Empirical\n",
    "# Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n",
    "state_linearity.bias.data[cell_size:2*cell_size].fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_index in range(num_layers):\n",
    "    forward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                           hidden_size,\n",
    "                                           cell_size,\n",
    "                                           go_forward,\n",
    "                                           recurrent_dropout_probability,\n",
    "                                           memory_cell_clip_value,\n",
    "                                           state_projection_clilp_value).cuda()\n",
    "    backward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                            hidden_size,\n",
    "                                            cell_size,\n",
    "                                            not go_forward,\n",
    "                                            recurrent_dropout_probability,\n",
    "                                            memory_cell_clip_value,\n",
    "                                            state_projection_clilp_value).cuda()\n",
    "    lstm_input_size = hidden_size\n",
    "    \n",
    "#     add_module(f'forward_layer_{layer_index}', forward_layer)\n",
    "#     add_module(f'backward_layer_{layer_index}', backward_layer)\n",
    "    forward_layers.append(forward_layer)\n",
    "    backward_layers.append(backward_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = token_embedding.clone()\n",
    "mask = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 10\n"
     ]
    }
   ],
   "source": [
    "batch_size, total_sequence_length = mask.size()\n",
    "print(batch_size, total_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 3, num_valid = 3\n",
      "sequence_lengths = tensor([ 7,  6, 10], device='cuda:0')\n",
      "1. sorted_inputs = \n",
      "tensor([[[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0363,  0.0383, -0.0691,  ..., -0.0122, -0.0327, -0.0238],\n",
      "         [ 0.0329,  0.0252, -0.0403,  ..., -0.0023,  0.0078,  0.0058],\n",
      "         ...,\n",
      "         [ 0.0634,  0.0281, -0.0232,  ..., -0.0151, -0.0302, -0.0067],\n",
      "         [ 0.0193,  0.0227, -0.0390,  ...,  0.0225, -0.0381,  0.0253],\n",
      "         [ 0.0401,  0.0508, -0.0588,  ..., -0.0385, -0.0220,  0.0049]],\n",
      "\n",
      "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0499,  0.0233, -0.0560,  ..., -0.0119, -0.0294,  0.0098],\n",
      "         [ 0.0358,  0.0143, -0.0375,  ...,  0.0022, -0.0452, -0.0209],\n",
      "         ...,\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240]],\n",
      "\n",
      "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
      "         [ 0.0426,  0.0086, -0.0403,  ..., -0.0172, -0.0399,  0.0101],\n",
      "         [ 0.0151,  0.0227, -0.0583,  ...,  0.0283, -0.0380, -0.0247],\n",
      "         ...,\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240],\n",
      "         [ 0.0274,  0.0296, -0.0446,  ..., -0.0120, -0.0218,  0.0240]]],\n",
      "       device='cuda:0', grad_fn=<IndexSelectBackward>)\n",
      "2. sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "3. restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "4. sorting_indices = tensor([2, 0, 1], device='cuda:0')\n",
      "             sorted_inputs.shape = torch.Size([3, 10, 512])\n",
      "packed_sequence_input.data.shape = torch.Size([23, 512])\n",
      "packed_sequence_input.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# .sort_and_run_forward()\n",
    "batch_size = mask.size(0)\n",
    "num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "print(f\"batch_size = {batch_size}, num_valid = {num_valid}\")\n",
    "\n",
    "sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "print(f\"sequence_lengths = {sequence_lengths}\")\n",
    "\n",
    "sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n",
    "    sort_batch_by_length(inputs, sequence_lengths)\n",
    "print(f\"1. sorted_inputs = \\n{sorted_inputs}\")\n",
    "print(f\"2. sorted_sequence_lengths = {sorted_sequence_lengths}\")\n",
    "print(f\"3. restoration_indices = {restoration_indices}\")\n",
    "print(f\"4. sorting_indices = {sorting_indices}\")\n",
    "packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                             sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                             batch_first=True)\n",
    "print(f\"             sorted_inputs.shape = {sorted_inputs.shape}\")\n",
    "print(f\"packed_sequence_input.data.shape = {packed_sequence_input.data.shape}\")\n",
    "print(f\"packed_sequence_input.batch_sizes = {packed_sequence_input.batch_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 input이 들어간 경우\n",
    "initial_states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if initial_states is None:\n",
    "    hidden_states: List[Optional[Tuple[torch.Tensor,\n",
    "                                       torch.Tensor]]] = [None] * len(forward_layers)\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, batch_lengths = pad_packed_sequence(packed_sequence_input, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "         [ 0.0363,  0.0383, -0.0691,  ..., -0.0122, -0.0327, -0.0238],\n",
       "         [ 0.0329,  0.0252, -0.0403,  ..., -0.0023,  0.0078,  0.0058],\n",
       "         ...,\n",
       "         [ 0.0634,  0.0281, -0.0232,  ..., -0.0151, -0.0302, -0.0067],\n",
       "         [ 0.0193,  0.0227, -0.0390,  ...,  0.0225, -0.0381,  0.0253],\n",
       "         [ 0.0401,  0.0508, -0.0588,  ..., -0.0385, -0.0220,  0.0049]],\n",
       "\n",
       "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "         [ 0.0499,  0.0233, -0.0560,  ..., -0.0119, -0.0294,  0.0098],\n",
       "         [ 0.0358,  0.0143, -0.0375,  ...,  0.0022, -0.0452, -0.0209],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0396,  0.0479, -0.0287,  ..., -0.0203, -0.0359, -0.0038],\n",
       "         [ 0.0426,  0.0086, -0.0403,  ..., -0.0172, -0.0399,  0.0101],\n",
       "         [ 0.0151,  0.0227, -0.0583,  ...,  0.0283, -0.0380, -0.0247],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  7,  6])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_output_sequence = inputs\n",
    "backward_output_sequence = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #2 'mat2' in call to _th_mm",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-261-574087a0568a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     forward_output_sequence, forward_state = forward_layer(forward_output_sequence,\n\u001b[0;32m     15\u001b[0m                                                            \u001b[0mbatch_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                                                            forward_state)\n\u001b[0m\u001b[0;32m     17\u001b[0m     backward_output_sequence, backward_state = backward_layer(backward_output_sequence,\n\u001b[0;32m     18\u001b[0m                                                               \u001b[0mbatch_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-203-e32069715fd3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, batch_lengths, initial_state)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;31m# Do the projections for all the gates all at once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;31m# Both have shape (batch_size, 4 * cell_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mprojected_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_linearity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestep_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mprojected_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_linearity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevious_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #2 'mat2' in call to _th_mm"
     ]
    }
   ],
   "source": [
    "final_states = []\n",
    "sequence_outputs = []\n",
    "for layer_index, state in enumerate(hidden_states):\n",
    "    forward_layer = forward_layers[layer_index]\n",
    "    backward_layer = backward_layers[layer_index]\n",
    "    \n",
    "    forward_cache = forward_output_sequence\n",
    "    backward_cache = backward_output_sequence\n",
    "    \n",
    "    # Since state is None,\n",
    "    forward_state = None\n",
    "    backward_state = None\n",
    "    \n",
    "    forward_output_sequence, forward_state = forward_layer(forward_output_sequence,\n",
    "                                                           batch_lengths,\n",
    "                                                           forward_state)\n",
    "    backward_output_sequence, backward_state = backward_layer(backward_output_sequence,\n",
    "                                                              batch_lengths,\n",
    "                                                              backward_state)\n",
    "    # Skip connections, just adding the input to the output.\n",
    "    if layer_index != 0:\n",
    "        forward_output_sequence += forward_cache\n",
    "        backward_output_sequence += backward_cache\n",
    "        \n",
    "    sequence_outputs.append(torch.cat([forward_output_sequence,\n",
    "                                       backward_output_sequence], -1))\n",
    "    # Append the state tuples in a list, so that we can return\n",
    "    # the final states for all the layers.\n",
    "    final_states.append((torch.cat([forward_state[0], backward_state[0]], -1),\n",
    "                         torch.cat([forward_state[1], backward_state[1]], -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Prepare teh initial states.\n",
    "        if not self.stateful:\n",
    "            if hidden_state == None:\n",
    "                initial_states = hidden_state\n",
    "            elif isinstance(hidden_state, tuple):\n",
    "                initial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :]\n",
    "                                  for state in hidden_state]\n",
    "            else:\n",
    "                initial_stats = self._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "        else:\n",
    "            initial_states = selt._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "            \n",
    "        # Actually call the module on the sorted PackedSequence\n",
    "        module_output, final_states = module(packed_sequence_input, initial_states)\n",
    "        \n",
    "        return module_output, final_states, restoration_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules.elmo.py\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
