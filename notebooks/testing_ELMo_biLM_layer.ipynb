{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# *~ coding convention ~*\n",
    "from overrides import overrides\n",
    "from typing import Callable\n",
    "\n",
    "# Python Standard Library\n",
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Python Installed Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction: dict to namedtuple\n",
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)\n",
    "\n",
    "# input your directories path\n",
    "model_dir = 'C:\\workspace\\implement_elmo\\elmo\\configs'\n",
    "args2 = dict2namedtuple(\n",
    "    json.load(\n",
    "        codecs.open(\n",
    "            os.path.join(model_dir, 'config.json'), \n",
    "            'r', encoding='utf-8')\n",
    "    )\n",
    ")\n",
    "\n",
    "# args2.config_path == 'cnn_50_100_512_4096_sample.json'\n",
    "\n",
    "# load config\n",
    "with open(os.path.join(model_dir, args2.config_path), 'r') as fin:\n",
    "    config = json.load(fin)\n",
    "    \n",
    "token_embedding = torch.load('token_embedding.pt') \n",
    "masks = [torch.load(f'mask[{ix}].pt') for ix in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size = 512\n",
      "hidden_size = 512\n",
      "cell_size = 4096\n",
      "num_layers = 2\n",
      "memory_cell_clip_value = 3\n",
      "state_projection_clip_value = 3\n",
      "recurrent_dropout_probability = 0.1\n"
     ]
    }
   ],
   "source": [
    "# _EncoderBase\n",
    "stateful = False\n",
    "_states = None\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "input_size = config['encoder']['projection_dim']\n",
    "hidden_size = config['encoder']['projection_dim']\n",
    "cell_size = config['encoder']['dim']\n",
    "num_layers = config['encoder']['n_layers']\n",
    "memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "state_projection_clip_value = config['encoder']['proj_clip']\n",
    "recurrent_dropout_probability = config['dropout']\n",
    "\n",
    "print(f\"input_size = {input_size}\")\n",
    "print(f\"hidden_size = {hidden_size}\")\n",
    "print(f\"cell_size = {cell_size}\")\n",
    "print(f\"num_layers = {num_layers}\")\n",
    "print(f\"memory_cell_clip_value = {memory_cell_clip_value}\")\n",
    "print(f\"state_projection_clip_value = {state_projection_clip_value}\")\n",
    "print(f\"recurrent_dropout_probability = {config['dropout']}\")\n",
    "\n",
    "forward_layers = []\n",
    "backward_layers = []\n",
    "\n",
    "lstm_input_size = input_size\n",
    "go_forward = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable\n",
    "import logging\n",
    "import itertools\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def get_lengths_from_binary_sequence_mask(mask: torch.Tensor):\n",
    "    return mask.long().sum(-1)\n",
    "\n",
    "def get_dropout_mask(dropout_probability: float,\n",
    "                     tensor_for_masking: Variable):\n",
    "    print('*-*** get_dropout_mask ***-*')\n",
    "    binary_mask = tensor_for_masking.clone()\n",
    "    print('binary_mask', binary_mask)\n",
    "    binary_mask.data.copy_(torch.rand(tensor_for_masking.size()) > dropout_probability)\n",
    "    print(f'binary_mask = {torch.rand(tensor_for_masking.size()) > dropout_probability}')\n",
    "    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)\n",
    "    print(f\"Calc 1.0 / (1 - p) or 0.0\")\n",
    "    print(f\"dropout_mask = {dropout_mask}\")\n",
    "    print('*-*** ---------------- ***-*')\n",
    "    return dropout_mask\n",
    "\n",
    "def block_orthogonal(tensor: torch.Tensor,\n",
    "                     split_sizes: List[int],\n",
    "                     gain: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    An initializer which allows initaliizing model parametes in \"block\".\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, Variable):\n",
    "    # in pytorch 4.0, Variable equals Tensor\n",
    "    #     block_orthogonal(tensor.data, split_sizes, gain)\n",
    "    # else:\n",
    "        sizes = list(tensor.size())\n",
    "        if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n",
    "            raise ConfigurationError(\n",
    "                \"tensor dimentions must be divisible by their respective \"\n",
    "                f\"split_sizes. Found size: {size} and split_sizes: {split_sizes}\")\n",
    "        indexes = [list(range(0, max_size, split))\n",
    "                   for max_size, split in zip(sizes, split_sizes)]\n",
    "        # Iterate over all possible blocks within the tensor.\n",
    "        for block_start_indices in itertools.product(*indexes):\n",
    "            index_and_step_tuples = zip(block_start_indices, split_sizes)\n",
    "            block_slice = tuple([slice(start_index, start_index + step)\n",
    "                                 for start_index, step in index_and_step_tuples])\n",
    "            tensor[block_slice] = nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)\n",
    "            \n",
    "def sort_batch_by_length(tensor: torch.autograd.Variable,\n",
    "                         sequence_lengths: torch.autograd.Variable):\n",
    "    if not isinstance(tensor, Variable) or not isinstance(sequence_lengths, Variable):\n",
    "        raise Exception(\"Both the tensor and sequence lengths must be torch.autograd.Variables.\")\n",
    "        \n",
    "    sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n",
    "    sorted_tensor = tensor.index_select(0, permutation_index)\n",
    "    \n",
    "    # This is ugly, but required - we are creating a new variable at runtime, so we\n",
    "    # must ensure it has the correct CUDA vs non-CUDA type. We do this by cloning and\n",
    "    # refilling one of the inputs to the function.\n",
    "    index_range = sequence_lengths.data.clone().copy_(torch.arange(0, len(sequence_lengths)))\n",
    "    # This is the equivalent of zipping with index, sorting by the original\n",
    "    # sequence lengths and returning the now sorted indices.\n",
    "    index_range = Variable(index_range.long())\n",
    "    _, reverse_mapping = permutation_index.sort(0, descending=False)\n",
    "    restoration_indices = index_range.index_select(0, reverse_mapping)\n",
    "    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아직 코드 리뷰안한 코드!\n",
    "from typing import Optional, Tuple, List, Callable, Union\n",
    "\n",
    "import h5py\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# We have two types here for the state, because storing the state in something\n",
    "# which is Iterable (like a tuple, below), is helpful for internal manipulation\n",
    "# - however, the states are consumed as either Tensors or a Tuple of Tensors, so\n",
    "# returning them in this format is unhelpful.\n",
    "RnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]  # pylint: disable=invalid-name\n",
    "RnnStateStorage = Tuple[torch.Tensor, ...]  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "class _EncoderBase(nn.Module):\n",
    "    # pyling: disable=abstract-method\n",
    "    \"\"\"\n",
    "    This abstract class serves as a base for the 3 ``Encoder`` abstractions in AllenNLP.\n",
    "    - :class:`~allennlp.modules.seq2seq_encoders.Seq2SeqEncoders`\n",
    "    - :class:`~allennlp.modules.seq2vec_encoders.Seq2VecEncoders`\n",
    "    Additionally, this class provides functionality for sorting sequences by length\n",
    "    so they can be consumed by Pytorch RNN classes, which require their inputs to be\n",
    "    sorted by length. Finally, it also provides optional statefulness to all of it's\n",
    "    subclasses by allowing the caching and retrieving of the hidden states of RNNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, stateful: bool = False) -> None:\n",
    "        super(_EncoderBase, self).__init__()\n",
    "        self.stateful = stateful\n",
    "        self._states: Optional[RnnStateStorage] = None\n",
    "\n",
    "    def sort_and_run_forward(self,\n",
    "                             module: Callable[[PackedSequence, Optional[RnnState]],\n",
    "                                              Tuple[Union[PackedSequence, torch.Tensor], RnnState]],\n",
    "                             inputs: torch.Tensor,\n",
    "                             mask: torch.Tensor,\n",
    "                             hidden_state: Optional[RnnState] = None):\n",
    "        \"\"\"\n",
    "        Pytorch RNNs는 input이 passing되기 전에 정렬되있어야 함\n",
    "        Seq2xxxEncoders가 이러한 기능을 모두 사용하기에 base class로 제공\n",
    "        \"\"\"\n",
    "        # In some circumstances you may have sequences of zero length. ``pack_padded_sequence``\n",
    "        # requires all sequence lengths to be > 0, so remove sequences of zero length before\n",
    "        # calling self._module, then fill with zeros.\n",
    "\n",
    "        # First count how many sequences are empty.\n",
    "        batch_size = mask.size(0)\n",
    "        num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "        print(f\"\\tbatch_size = {batch_size}, num_valid = {num_valid}\")\n",
    "\n",
    "        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "        print(f\"\\tsequence_lengths = {sequence_lengths}\")\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n",
    "            sort_batch_by_length(inputs, sequence_lengths)\n",
    "        print(f\"\\t1. sorted_inputs.shape = {sorted_inputs.shape}\")\n",
    "        print(f\"\\t2. sorted_sequence_lengths = {sorted_sequence_lengths}\")\n",
    "        print(f\"\\t3. restoration_indices = {restoration_indices}\")\n",
    "        print(f\"\\t4. sorting_indices = {sorting_indices}\")\n",
    "        # Now create a PackedSequence with only the non-empty, sorted sequences.\n",
    "        # pad token 제외, 유의미한 값들만 packing\n",
    "        packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                                     sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                                     batch_first=True)\n",
    "        print(f\"\\t             sorted_inputs.shape  = {sorted_inputs.shape}\")\n",
    "        print(f\"\\tpacked_sequence_input.data.shape  = {packed_sequence_input.data.shape}\")\n",
    "        print(f\"\\tpacked_sequence_input.batch_sizes = {packed_sequence_input.batch_sizes}\")\n",
    "        # Prepare teh initial states.\n",
    "        print(f\"\\tself.stateful is {self.stateful}\")\n",
    "        if not self.stateful:\n",
    "            print(\"\\tstateful is False,\", end='')\n",
    "            print(\"If hidden_state is \", end='')\n",
    "            if hidden_state == None:\n",
    "                print(\"None,\\n\\t\\tinitial_states = hidden_state\")\n",
    "                initial_states = hidden_state\n",
    "            elif isinstance(hidden_state, tuple):\n",
    "                print(\"tuple,\\n\\t\\tinitial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :] for state in hidden_state]\")\n",
    "                initial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :]\n",
    "                                  for state in hidden_state]\n",
    "            else:\n",
    "                print(\"not both None and tuple,\\n\\t\\tConduct `_get_initial_states`\")\n",
    "                initial_stats = self._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "        else:\n",
    "            print(\"\\tstateful is True,\\n\\t\\tConduct `_get_initial_states`\")\n",
    "            initial_states = self._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "\n",
    "        # Actually call the module on the sorted PackedSequence\n",
    "        print(\"\\tRUN `_lstm_forward`... by initial_states\")\n",
    "        module_output, final_states = module(packed_sequence_input, initial_states)\n",
    "        \n",
    "\n",
    "        return module_output, final_states, restoration_indices\n",
    "\n",
    "    def _get_initial_states(self,\n",
    "                            batch_size: int,\n",
    "                            num_valid: int,\n",
    "                            sorting_indices: torch.LongTensor) -> Optional[RnnState]:\n",
    "        \"\"\"\n",
    "        RNN의 초기 상태를 반환\n",
    "        추가적으로, 이 메서드는 batch의 새로운 요소의 초기 상태를 추가하기 위해 상태를 변경하여(mutate)\n",
    "            호출시 batch size를 처리\n",
    "        또한 이 메서드는\n",
    "            1. 배치의 요소 seq. length로 상태를 정렬하는 것과\n",
    "            2. pad가 끝난 row 제거도 처리\n",
    "        중요한 것은 현재의 배치 크기가 이전에 호출되었을 때보다 더 크면 이 상태를 \"혼합\"하는 것이다.\n",
    "\n",
    "        이 메서드는 (1) 처음 호출되어 아무 상태가 없는 경우 (2) RNN이 heterogeneous state를 가질 때\n",
    "        의 경우를 처리해야 하기 때문에 return값이 복잡함\n",
    "\n",
    "        (1) module이 처음 호출됬을 때 ``module``의 타입이 무엇이든 ``None`` 반환\n",
    "        (2) Otherwise,\n",
    "            - LSTM의 경우 tuple of ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "                 and ``(num_layers, num_valid, memory_size)``\n",
    "            - GRU의 경우  single ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "        \"\"\"\n",
    "        # We don't know the state sizes the first time calling forward,\n",
    "        # so we let the module define what it's initial hidden state looks like.\n",
    "        if self._states is None:\n",
    "            return None\n",
    "\n",
    "        # Otherwise, we have some previous states.\n",
    "        if batch_size > self._states[0].size(1):\n",
    "            # This batch is larger than the all previous states.\n",
    "            # If so, resize the states.\n",
    "            num_states_to_concat = batch_size - self._states[0].size(1)\n",
    "            resized_states = []\n",
    "            # state has shape (num_layers, batch_size, hidden_size)\n",
    "            for state in self._states:\n",
    "                # This _must_ be inside the loop because some\n",
    "                # RNNs have states with different last dimension sizes.\n",
    "                zeros = state.data.new(state.size(0),\n",
    "                                       num_states_to_concat,\n",
    "                                       state.size(2)).fill_(0)\n",
    "                zeros = Variable(zeros)\n",
    "                resized_states.append(torch.cat([state, zeros], 1))\n",
    "            self._states = tuple(resized_states)\n",
    "            correctly_shaped_states = self._states\n",
    "        elif batch_size < self._states[0].size(1):\n",
    "            # This batch is smaller than the previous one.\n",
    "            correctly_shaped_states = tuple(staet[:, :batch_size, :] for state in self._states)\n",
    "        else:\n",
    "            correctly_shaped_states = self._states\n",
    "\n",
    "        # At this point, out states are of shape (num_layers, batch_size, hidden_size).\n",
    "        # However, the encoder uses sorted sequences and additionally removes elements\n",
    "        # of the batch which are fully padded. We need the states to match up to these\n",
    "        # sorted and filtered sequences, so we do that in the next two blocks before\n",
    "        # returning the states.\n",
    "        if len(self._states) == 1:\n",
    "            # GRU\n",
    "            correctly_shaped_state = correctly_shaped_states[0]\n",
    "            sorted_state = correctly_shaped_state.index_select(1, sorting_indices)\n",
    "            return sorted_state[:, :num_valid, :]\n",
    "        else:\n",
    "            # LSTM\n",
    "            sorted_states = [state.index_select(1, sorting_indices)\n",
    "                             for state in correctly_shaped_states]\n",
    "            return tuple(state[:, :num_valid, :] for state in sorted_states)\n",
    "\n",
    "    def _update_states(self,\n",
    "                       final_states: RnnStateStorage,\n",
    "                       restoration_indices: torch.LongTensor) -> None:\n",
    "        \"\"\"\n",
    "        RNN forward 동작 후에 state를 update\n",
    "        새로운 state로 update하며 몇 가지 book-keeping을 실시\n",
    "        즉, 상태를 해제하고 완전히 padding된 state가 업데이트되지 않도록 함\n",
    "        마지막으로 graph가 매 batch iteration후에 gc되도록 계산 그래프에서\n",
    "        state variable을 떼어냄.\n",
    "        \"\"\"\n",
    "        # TODO(Mark)L seems weird to sort here, but append zeros in the subclasses.\n",
    "        # which way around is best?\n",
    "        print('_EncoderBase의 `_update_states` 메서드 실행')\n",
    "        print(f'inputs:\\nfinal_states = {final_states}\\nrestoration_indices = {restoration_indices}')\n",
    "        new_unsorted_states = [state.index_select(1, restoration_indices)\n",
    "                               for state in final_states]\n",
    "        print(f\"new_unsorted_states = {new_unsorted_states}\")\n",
    "        print(f\"self._states is None = {self._states is None}\")\n",
    "        if self._states is None:\n",
    "            print(\"이전 상태가 존재하지 않습니다. new_unsorted_states로 새롭게 만들어 줍니다.\")\n",
    "            # We don't already have states, so just set the\n",
    "            # ones we receive to be the current state.\n",
    "            self._states = tuple([Variable(state.data)\n",
    "                                  for state in new_unsorted_states])\n",
    "            print('STATES:', self._states)\n",
    "        else:\n",
    "            print(\"이전 상태가 존재합니다. 현재 상태와 입력받은 final_state로 새로운 상태를 update합니다.\")\n",
    "            # Now we've sorted the states back so that they correspond to the original\n",
    "            # indices, we need to figure out what states we need to update, because if we\n",
    "            # didn't use a state for a particular row, we want to preserve its state.\n",
    "            # Thankfully, the rows which are all zero in the state correspond exactly\n",
    "            # to those which aren't used, so we create masks of shape (new_batch_size,),\n",
    "            # denoting which states were used in the RNN computation.\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            print(f\"current_state_batch_size = {current_state_batch_size} = self._states[0].size(1)\")\n",
    "            print(f\"new_state_batch_size = {new_state_batch_size} = final_states[0].size(1)\")\n",
    "            # Masks for the unused states of shape (1, new_batch_size, 1)\n",
    "            used_new_rows_mask = [(state[0, :, :].sum(-1)\n",
    "                                   != 0.0).float().view(1, new_state_batch_size, 1)\n",
    "                                  for state in new_unsorted_states]\n",
    "            new_states = []\n",
    "            if current_state_batch_size > new_state_batch_size:\n",
    "                # The new state is smaller than the old one,\n",
    "                # so just update the indices which we used.\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows in the previous state\n",
    "                    # which _were_ used in the current state.\n",
    "                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(old_state.data))\n",
    "            else:\n",
    "                # The states are the same size, so we just have to\n",
    "                # deal with the possibility that some rows weren't used.\n",
    "                new_states = []\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows which _were_ used in the current state.\n",
    "                    masked_old_state = old_state * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    new_state += masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(new_state.data))\n",
    "\n",
    "            # It looks like there should be another case handled here - when\n",
    "            # the current_state_batch_size < new_state_batch_size. However,\n",
    "            # this never happens, because the states themeselves are mutated\n",
    "            # by appending zeros when calling _get_inital_states, meaning that\n",
    "            # the new states are either of equal size, or smaller, in the case\n",
    "            # that there are some unused elements (zero-length) for the RNN computation.\n",
    "            self._states = tuple(new_states)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._states = None\n",
    "\n",
    "\n",
    "class ElmobiLm(_EncoderBase):\n",
    "    def __init__(self, config, use_cuda=False):\n",
    "        super(ElmobiLm, self).__init__(stateful=True)\n",
    "        self.config = config\n",
    "        self.use_cuda = use_cuda\n",
    "        input_size = config['encoder']['projection_dim']\n",
    "        hidden_size = config['encoder']['projection_dim']\n",
    "        cell_size = config['encoder']['dim']\n",
    "        num_layers = config['encoder']['n_layers']\n",
    "        memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "        state_projection_clip_value = config['encoder']['proj_clip']\n",
    "        recurrent_dropout_probability = config['dropout']\n",
    "        \n",
    "        print('ELMo biLM layer params')\n",
    "        print(f\"\\tinput_size = {input_size}\")\n",
    "        print(f\"\\thidden_size = {hidden_size}\")\n",
    "        print(f\"\\tcell_size = {cell_size}\")\n",
    "        print(f\"\\tnum_layers = {num_layers}\")\n",
    "        print(f\"\\tmemory_cell_clip_value = {memory_cell_clip_value}\")\n",
    "        print(f\"\\tstate_projection_clip_value = {state_projection_clip_value}\")\n",
    "#         print(f\"\\trecurrent_dropout_probability = {config['dropout']}\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        forward_layers = []\n",
    "        backward_layers = []\n",
    "\n",
    "        lstm_input_size = input_size\n",
    "        go_forward = True\n",
    "        for layer_index in range(num_layers):\n",
    "            forward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                                   hidden_size,\n",
    "                                                   cell_size,\n",
    "                                                   go_forward,\n",
    "                                                   recurrent_dropout_probability,\n",
    "                                                   memory_cell_clip_value,\n",
    "                                                   state_projection_clip_value).cuda()\n",
    "            backward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                                    hidden_size,\n",
    "                                                    cell_size,\n",
    "                                                    not go_forward,\n",
    "                                                    recurrent_dropout_probability,\n",
    "                                                    memory_cell_clip_value,\n",
    "                                                    state_projection_clip_value).cuda()\n",
    "            if use_cuda:\n",
    "                forward_layer = forward_layer.cuda()\n",
    "                backward_layer = backward_layer.cuda()\n",
    "            lstm_input_size = hidden_size\n",
    "\n",
    "            self.add_module('forward_layer_{}'.format(layer_index), forward_layer)\n",
    "            self.add_module('backward_layer_{}'.format(layer_index), backward_layer)\n",
    "            forward_layers.append(forward_layer)\n",
    "            backward_layers.append(backward_layer)\n",
    "        self.forward_layers = forward_layers\n",
    "        self.backward_layers = backward_layers\n",
    "        print(f\"forward_layers = {forward_layers}\")\n",
    "        print(f\"backward_layers = {backward_layers}\")\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        print('FORWARD!!!!**************')\n",
    "        batch_size, total_sequence_length = mask.size()\n",
    "        print(f\"batch_size = {batch_size}\")\n",
    "        print(f\"total_sequence_length = {total_sequence_length}\")\n",
    "        print(\"_EncoderBase.sort_and_run_forward 메서드 실시...\")\n",
    "        stacked_sequence_output, final_states, restoration_indices = \\\n",
    "            self.sort_and_run_forward(self._lstm_forward, inputs, mask)\n",
    "        print(f\"stacked_sequence_output.shape = {stacked_sequence_output.shape}\")\n",
    "        print(f\"final_states = {final_states}\")\n",
    "        print(f\"restoration_indices = {restoration_indices}\")\n",
    "        num_layers, num_valid, returned_timesteps, encoder_dim = stacked_sequence_output.size()\n",
    "        # Add back invalid rows which were removed in the call to sort_and_run_forward.\n",
    "        print(\"stacked\")\n",
    "        print(f\"num_layers = {num_layers}\")\n",
    "        print(f\"num_valid = {num_valid}\")\n",
    "        print(f\"returned_timesteps = {returned_timesteps}\")\n",
    "        print(f\"encoder_dim = {encoder_dim}\")\n",
    "        print(f\"num_valid < batch_size -> {num_valid < batch_size}\")\n",
    "        if num_valid < batch_size:\n",
    "            zeros = stacked_sequence_output.data.new(num_layers,\n",
    "                                                     batch_size - num_valid,\n",
    "                                                     returned_timesteps,\n",
    "                                                     encoder_dim).fill_(0)\n",
    "            zeros = Variable(zeros)\n",
    "            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 1)\n",
    "\n",
    "            # The states also need to have invalid rows added back.\n",
    "            new_states = []\n",
    "            for state in final_states:\n",
    "                state_dim = state.size(-1)\n",
    "                zeros = state.data.new(num_layers, batch_size - num_valid, state_dim).fill_(0)\n",
    "                zeros = Variable(zeros)\n",
    "                new_states.append(torch.cat([state, zeros], 1))\n",
    "            final_states = new_states\n",
    "\n",
    "        # It's possible to need to pass sequences which are padded to longer than the\n",
    "        # max length of the sequence to a Seq2StackEncoder. However, packing and unpacking\n",
    "        # the sequences mean that the returned tensor won't include these dimensions, because\n",
    "        # the RNN did not need to process them. We add them back on in the form of zeros here.\n",
    "        sequence_length_difference = total_sequence_length - returned_timesteps\n",
    "        print(\"sequence_length_difference = total_sequence_length - returned_timesteps\")\n",
    "        print(f\"sequence_length_difference = {sequence_length_difference}\")\n",
    "        print(f\"sequence_length_difference is larger than 0? : {sequence_length_difference > 0}\")\n",
    "        if sequence_length_difference > 0:\n",
    "            zeros = stacked_sequence_output.data.new(num_layers,\n",
    "                                                     batch_size,\n",
    "                                                     sequence_length_difference,\n",
    "                                                     stacked_sequence_output[0].size(-1)).fill_(0)\n",
    "            zeros = Variable(zeros)\n",
    "            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)\n",
    "        print('UPDATE STATES... inputs: final_states, restoration_indices')\n",
    "        self._update_states(final_states, restoration_indices)\n",
    "\n",
    "        # Restore the original indices and return the sequence.\n",
    "        # Has shape (num_layers, batch_size, sequence_length, hidden_size)\n",
    "        return stacked_sequence_output.index_select(1, restoration_indices)\n",
    "\n",
    "\n",
    "    def _lstm_forward(self,\n",
    "                      inputs: PackedSequence,\n",
    "                      initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> \\\n",
    "        Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        print(f\"\\t\\tinitial_state is None? {initial_state is None}\")\n",
    "        if initial_state is None:\n",
    "            print(\"\\t\\tOops, Assign hidden_state = [None] * len(self.forward_layers)\")\n",
    "            hidden_states: List[Optional[Tuple[torch.Tensor,\n",
    "                                         torch.Tensor]]] = [None] * len(self.forward_layers)\n",
    "            print(f\"\\t\\thidden_states = {hidden_states}\")\n",
    "        elif initial_state[0].size()[0] != len(self.forward_layers):\n",
    "            print(f\"\\t\\tinitial_state[0].size()[0] = {initial_state[0].size()[0]}\")\n",
    "            print(f\"\\t\\tlen(self.forward_layers) = {len(self.forward_layers)}\")\n",
    "            raise Exception(\"Initial states were passed to forward() but the number of \"\n",
    "                            \"initial states does not match the number of layers.\")\n",
    "        else:\n",
    "            print(\"\\t\\tinitial is not None and it's size equal to forward_layers' length,\")\n",
    "            print(\"\\t\\tthen hidden_states is\")\n",
    "            print(f\"\\t\\t A = initial_state[0].split(1, 0) = {initial_state[0].split(1, 0)}\")\n",
    "            print(f\"\\t\\t B = initial_state[1].split(1, 0) = {initial_state[1].split(1, 0)}\")\n",
    "            print(\"\\t\\t hidden_states = list(zip(A, B))\")\n",
    "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
    "                                     initial_state[1].split(1, 0)))\n",
    "            print(f\"\\t\\t               = {hidden_states}\")\n",
    "        \n",
    "        print(\"\\t\\tinputs is `PackedSequence`\")\n",
    "        print(f\"\\t\\ttype(inputs) = {type(inputs)}\")\n",
    "        print(f\"\\t\\t\\tinputs.data.shape = {inputs.data.shape}\")\n",
    "        print(f\"\\t\\t\\tinputs.batch_sizes = {inputs.batch_sizes}\")\n",
    "        print(f\"\\t\\t\\tinputs.sorted_indices = {inputs.sorted_indices}\")\n",
    "        print(f\"\\t\\t\\tinputs.unsorted_indices = {inputs.unsorted_indices}\")\n",
    "        \n",
    "        print(\"\\t\\tRestore PAD_char to inputs...\")\n",
    "        inputs, batch_lengths = pad_packed_sequence(inputs, batch_first=True)\n",
    "        print(\"\\t\\t바뀐 inputs의 정보 출력\")\n",
    "        print(f\"\\t\\ttype(inputs) = {type(inputs)}\")\n",
    "        print(f\"\\t\\t\\tinputs.shape = {inputs.shape}\")\n",
    "        print(f\"\\t\\tbatch_lengths = {batch_lengths}\")\n",
    "        print(\"\\t\\tAssign forward_output_sequence = backward_output_sequence = inputs\")\n",
    "        forward_output_sequence = inputs\n",
    "        backward_output_sequence = inputs\n",
    "        \n",
    "        print(\"\\t\\tSet final_states, sequqnce_outputs as empty list, []\")\n",
    "        final_states = []\n",
    "        sequence_outputs = []\n",
    "        for layer_index, state in enumerate(hidden_states):\n",
    "            print(f\"\\t\\tGet a forward layer and backward layer at layer {layer_index+1}\")\n",
    "            forward_layer = getattr(self, 'forward_layer_{}'.format(layer_index))\n",
    "            backward_layer = getattr(self, 'backward_layer_{}'.format(layer_index))\n",
    "            \n",
    "            print(\"\\t\\tCaching...: output_sequence to cache both forward and backward\")\n",
    "            forward_cache = forward_output_sequence\n",
    "            backward_cache = backward_output_sequence\n",
    "            \n",
    "            print(f\"\\t\\tstate is None? {state is None}\")\n",
    "            if state is not None:\n",
    "                print(\"\\t\\t\\tAlright, Set hidden_state/memory_state for both forward and backward\")\n",
    "                print(f\"\\t\\t\\tstate[0](hidden_state) = {state[0]}\")\n",
    "                print(f\"\\t\\t\\tstate[1](memory_state) = {state[1]}\")\n",
    "                forward_hidden_state, backward_hidden_state = state[0].split(self.hidden_size, 2)\n",
    "                forward_memory_state, backward_memory_state = state[1].split(self.cell_size, 2)\n",
    "                forward_state = (forward_hidden_state, forward_memory_state)\n",
    "                backward_state = (backward_hidden_state, backward_memory_state)\n",
    "            else:\n",
    "                print(\"\\t\\t\\tOops, then forward and backward state is also 'None'\")\n",
    "                forward_state = None\n",
    "                backward_state = None\n",
    "                \n",
    "            print(\"\\t\\tRUN forward_layer.forward method...\")\n",
    "            forward_output_sequence, forward_state = forward_layer(forward_output_sequence,\n",
    "                                                                   batch_lengths,\n",
    "                                                                   forward_state)\n",
    "            print(\"\\t\\tRUN backward_layer.forward method...\")\n",
    "            backward_output_sequence, backward_state = backward_layer(backward_output_sequence,\n",
    "                                                                      batch_lengths,\n",
    "                                                                      backward_state)\n",
    "            # Skip connections, just adding the input to the output.\n",
    "            if layer_index != 0:\n",
    "                print('\\t\\tsince layer_index != 0, adding cache to output sequence')\n",
    "                forward_output_sequence += forward_cache\n",
    "                backward_output_sequence += backward_cache\n",
    "            \n",
    "            \n",
    "            sequence_outputs.append(torch.cat([forward_output_sequence,\n",
    "                                               backward_output_sequence], -1))\n",
    "            # Append the state tuples in a list, so that we can return\n",
    "            # the final states for all the layers.\n",
    "            final_states.append((torch.cat([forward_state[0], backward_state[0]], dim=-1),\n",
    "                                 torch.cat([forward_state[1], backward_state[1]], dim=-1)))\n",
    "\n",
    "        stacked_sequence_outputs: torch.FloatTensor = torch.stack(sequence_outputs)\n",
    "        # Stack the hidden state and memory for each layer into 2 tensors of shape\n",
    "        # (num_layers, batch_size, hidden_size) and (num_layers, batch_size, cell_size)\n",
    "        # respectively.\n",
    "        final_hidden_states, final_memory_states = zip(*final_states)\n",
    "        final_state_tuple: Tuple[torch.FloatTensor,\n",
    "                                 torch.FloatTensor] = (torch.cat(final_hidden_states, 0),\n",
    "                                                       torch.cat(final_memory_states, 0))\n",
    "        return stacked_sequence_outputs, final_state_tuple\n",
    "    \n",
    "class LstmCellWithProjection(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 cell_size: int,\n",
    "                 go_forward: bool = True,\n",
    "                 recurrent_dropout_probability: float = 0.0,\n",
    "                 memory_cell_clip_value: Optional[float] = None,\n",
    "                 state_projection_clip_value: Optional[float] = None) -> None:\n",
    "        super(LstmCellWithProjection, self).__init__()\n",
    "        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`.\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        self.go_forward = go_forward\n",
    "        self.state_projection_clip_value = state_projection_clip_value\n",
    "        self.memory_cell_clip_value = memory_cell_clip_value\n",
    "        self.recurrent_dropout_probability = recurrent_dropout_probability\n",
    "\n",
    "        # We do the projections for all the gates all at once.\n",
    "        self.input_linearity = nn.Linear(input_size, 4 * cell_size, bias=False)\n",
    "        self.state_linearity = nn.Linear(hidden_size, 4 * cell_size, bias=True)\n",
    "\n",
    "        # Additional projection matrix for making the hidden state smaller.\n",
    "        self.state_projection = nn.Linear(cell_size, hidden_size, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Use sensible default initializations for parameters.\n",
    "        block_orthogonal(self.input_linearity.weight.data, [self.cell_size, self.input_size])\n",
    "        block_orthogonal(self.state_linearity.weight.data, [self.cell_size, self.hidden_size])\n",
    "\n",
    "        self.state_linearity.bias.data.fill_(0.0)\n",
    "        # Initialize forget gate biases to 1.0 as per An Empirical\n",
    "        # Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n",
    "        self.state_linearity.bias.data[self.cell_size:2 * self.cell_size].fill_(1.0)\n",
    "\n",
    "    def forward(self,  # pylint: disable=arguments-differ\n",
    "                inputs: torch.FloatTensor,\n",
    "                batch_lengths: List[int],\n",
    "                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        print(f\"\\t\\t\\tinputs.size() = {inputs.size()}\")\n",
    "        batch_size = inputs.size()[0]\n",
    "        total_timesteps = inputs.size()[1]\n",
    "        print('\\t\\t\\tUnpacking batch_size, total_timesteps = inputs.size()')\n",
    "        print(f'\\t\\t\\tbatch_size = {batch_size}, total_timesteps = {total_timesteps}')\n",
    "\n",
    "        # We have to use this '.data.new().fill_' pattern to create tensors with the correct\n",
    "        # type - forward has no knowledge of whether these are torch.Tensors or torch.cuda.Tensors.\n",
    "        output_accumulator = Variable(inputs.data.new(batch_size,\n",
    "                                                      total_timesteps,\n",
    "                                                      self.hidden_size).fill_(0))\n",
    "        print(f\"\\t\\t\\tCreate tensor(output_accumulator) which has ({batch_size}, {total_timesteps}, {self.hidden_size}) shape, filling 0.\")\n",
    "        print(f\"\\t\\t\\tis `initial_state` is None? {initial_state is None}\")\n",
    "        if initial_state is None:\n",
    "            print(\"\\t\\t\\t\\tOh, then create full_batch_previous memory and state by \"\n",
    "                  f\"({batch_size}, {self.cell_size}) tensor filling 0.\")\n",
    "            full_batch_previous_memory = Variable(inputs.data.new(batch_size,\n",
    "                                                                  self.cell_size).fill_(0))\n",
    "            full_batch_previous_state = Variable(inputs.data.new(batch_size,\n",
    "                                                                 self.hidden_size).fill_(0))\n",
    "        else:\n",
    "            print(\"\\t\\t\\t\\tOk, Using `initial_state`, create full_batch_previous memory and state.\")\n",
    "            print(f\"\\t\\t\\t\\t(previous_state) = initial_state[0] = {initial_state[0]}\")\n",
    "            print(f\"\\t\\t\\t\\tfull_batch_previous_state = initial_state[0].squeeze(0) = {initial_state[0].squeeze(0)}\")\n",
    "            full_batch_previous_state = initial_state[0].squeeze(0)\n",
    "            print(f\"\\t\\t\\t\\t(previous_memory) = initial_state[1] = {initial_state[1]}\")\n",
    "            print(f\"\\t\\t\\t\\tfull_batch_previous_memory = initial_state[1].squeeze(0) = {initial_state[1].squeeze(0)}\")\n",
    "            full_batch_previous_memory = initial_state[1].squeeze(0)\n",
    "\n",
    "        current_length_index = batch_size - 1 if self.go_forward else 0\n",
    "        print(f\"\\t\\t\\t\\tSet current_length_index... is it forward?? {self.go_forward}\")\n",
    "        if self.go_forward:\n",
    "            print(f\"\\t\\t\\t\\tOk, forward!! current_length_index = batch_size - 1 = {batch_size - 1}\")\n",
    "            current_length_index = batch_size - 1\n",
    "        else:\n",
    "            print(f\"\\t\\t\\t\\tOops, backward!! current_length_index = 0\")\n",
    "            current_length_index = 0\n",
    "            \n",
    "        print('\\t\\t\\t\\tis recurrent_dropout_probability is larger than 0?', self.recurrent_dropout_probability > 0.0)\n",
    "        print('\\t\\t\\t\\tand is training?', self.training)\n",
    "        if self.recurrent_dropout_probability > 0.0 and self.training:\n",
    "            print('\\t\\t\\t\\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!')\n",
    "            dropout_mask = get_dropout_mask(self.recurrent_dropout_probability,\n",
    "                                            full_batch_previous_state)\n",
    "        else:\n",
    "            print('\\t\\t\\t\\toh, is not trainig. then dropout_mask = None.')\n",
    "            dropout_mask = None\n",
    "            \n",
    "        print(f\"\\t\\t\\t\\tStarting Loops with {total_timesteps}...\")\n",
    "        for timestep in range(total_timesteps):\n",
    "            # The index depends on which end we start.\n",
    "            index = timestep if self.go_forward else total_timesteps - timestep - 1\n",
    "            print(f\"\\t\\t\\t\\tindex = {index} since {'forward' if self.go_forward else 'backward'}\")\n",
    "\n",
    "            # What we are doing here is finding the index into the batch dimension\n",
    "            # which we need to use for this timestep, because the sequences have\n",
    "            # variable length, so once the index is greater than the length of this\n",
    "            # particular batch sequence, we no longer need to do the computation for\n",
    "            # this sequence. The key thing to recognise here is that the batch inputs\n",
    "            # must be _ordered_ by length from longest (first in batch) to shortest\n",
    "            # (last) so initially, we are going forwards with every sequence and as we\n",
    "            # pass the index at which the shortest elements of the batch finish,\n",
    "            # we stop picking them up for the computation.\n",
    "            if self.go_forward:\n",
    "                print('\\t\\t\\t\\tIn case forward')\n",
    "                print(f\"\\t\\t\\t\\tbatch_lengths[current_length_index] <= index = {batch_lengths[current_length_index] <= index}\")\n",
    "                while batch_lengths[current_length_index] <= index:\n",
    "                    print(\"\\t\\t\\t\\tcurrent_length_index -= 1\")\n",
    "                    current_length_index -= 1\n",
    "            # If we're going backwards, we are _picking up_ more indices.\n",
    "            else:\n",
    "                # First conditional: Are we already at the maximum number of elements in the batch?\n",
    "                # Second conditional: Does the next shortest sequence beyond the current batch\n",
    "                # index require computation use this timestep?\n",
    "                print('\\t\\t\\t\\tIn case backward,')\n",
    "                print(f\"\\t\\t\\t\\tbatch_lengths[current_length_index] <= index = {batch_lengths[current_length_index] <= index}\")\n",
    "                while current_length_index < (len(batch_lengths) - 1) and \\\n",
    "                                batch_lengths[current_length_index + 1] > index:\n",
    "                    print(\"\\t\\t\\t\\tcurrent_length_index += 1\")\n",
    "                    current_length_index += 1\n",
    "            print(f'\\t\\t\\t\\tbatch_lengths[length_index] is {batch_lengths[current_length_index]}')\n",
    "\n",
    "            # Actually get the slices of the batch which we\n",
    "            # need for the computation at this timestep.\n",
    "            # shape (batch_size, cell_size)\n",
    "            print(\"\\t\\t\\t\\tGet a previous memory...\")\n",
    "            print(full_batch_previous_memory[0: current_length_index + 1])\n",
    "            previous_memory = full_batch_previous_memory[0: current_length_index + 1].clone()\n",
    "            print(previous_memory.shape)\n",
    "            # Shape (batch_size, hidden_size)\n",
    "            print(\"\\t\\t\\t\\tGet a previous state...\")\n",
    "            print(full_batch_previous_memory[0: current_length_index + 1])\n",
    "            previous_state = full_batch_previous_state[0: current_length_index + 1].clone()\n",
    "            print(previous_state.shape)\n",
    "            # Shape (batch_size, input_size)\n",
    "            timestep_input = inputs[0: current_length_index + 1, index]\n",
    "            print(\"\\t\\t\\t\\tGet a timestep input...\")\n",
    "            print(timestep_input)\n",
    "            print(timestep_input.shape)\n",
    "\n",
    "            # Do the projections for all the gates all at once.\n",
    "            # Both have shape (batch_size, 4 * cell_size)\n",
    "            print(\"\\t\\t\\t\\tProjection to 4*cell_size...\")\n",
    "            projected_input = self.input_linearity(timestep_input)\n",
    "            print(\"\\t\\t\\t\\t`input_linearity`: W1 * timestep_input\")\n",
    "            print(f\"\\t\\t\\t\\tprojected_input.shape = {projected_input.shape}\")\n",
    "            projected_state = self.state_linearity(previous_state)\n",
    "            print(\"\\t\\t\\t\\t`state_linearity`: W2 * previous_state + b\")\n",
    "            print(f\"\\t\\t\\t\\tprojected_state.shape = {projected_state.shape}\")\n",
    "\n",
    "            # Main LSTM equations using relevant chunks of the big linear\n",
    "            # projections of the hidden state and inputs.\n",
    "            print(\"\\t\\t\\t\\tCalc LSTM hidden unit...\")\n",
    "            input_gate = torch.sigmoid(projected_input[:, (0 * self.cell_size):(1 * self.cell_size)] +\n",
    "                                       projected_state[:, (0 * self.cell_size):(1 * self.cell_size)])\n",
    "            forget_gate = torch.sigmoid(projected_input[:, (1 * self.cell_size):(2 * self.cell_size)] +\n",
    "                                        projected_state[:, (1 * self.cell_size):(2 * self.cell_size)])\n",
    "            memory_init = torch.tanh(projected_input[:, (2 * self.cell_size):(3 * self.cell_size)] +\n",
    "                                     projected_state[:, (2 * self.cell_size):(3 * self.cell_size)])\n",
    "            output_gate = torch.sigmoid(projected_input[:, (3 * self.cell_size):(4 * self.cell_size)] +\n",
    "                                        projected_state[:, (3 * self.cell_size):(4 * self.cell_size)])\n",
    "            memory = input_gate * memory_init + forget_gate * previous_memory\n",
    "\n",
    "            # Here is the non-standard part of this LSTM cell; first, we clip the\n",
    "            # memory cell, then we project the output of the timestep to a smaller size\n",
    "            # and again clip it.\n",
    "            print(f\"\\t\\t\\t\\tis memory_cell_clip_value is exist? {'Yes' if self.memory_cell_clip_value else 'No'}\")\n",
    "            if self.memory_cell_clip_value:\n",
    "                print(f\"\\t\\t\\t\\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value\", end='')\n",
    "                print(self.memory_cell_clip_value)\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                memory = torch.clamp(memory, -self.memory_cell_clip_value, self.memory_cell_clip_value)\n",
    "            else:\n",
    "                print(\"\\t\\t\\t\\tOh, it's None. passing the way.\")\n",
    "\n",
    "            print(\"\\t\\t\\t\\tCalc next timestep output...\")\n",
    "            # shape (current_length_index, cell_size)\n",
    "            pre_projection_timestep_output = output_gate * torch.tanh(memory)\n",
    "\n",
    "            # shape (current_length_index, hidden_size)\n",
    "            timestep_output = self.state_projection(pre_projection_timestep_output)\n",
    "            print(f\"\\t\\t\\t\\tstate_projection_clip_value is exist? {'Yes' if self.state_projection_clip_value else 'No'}\")\n",
    "            if self.state_projection_clip_value:\n",
    "                print(f\"\\t\\t\\t\\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value\", end='')\n",
    "                print(self.state_projection_clip_value)\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                timestep_output = torch.clamp(timestep_output,\n",
    "                                              -self.state_projection_clip_value,\n",
    "                                              self.state_projection_clip_value)\n",
    "            else:\n",
    "                print(\"\\t\\t\\t\\tOh, it's None. passing the way.\")\n",
    "\n",
    "            # Only do dropout if the dropout prob is > 0.0 and we are in training mode.\n",
    "            print(\"\\t\\t\\t\\tIf dropout_mask exists, Adjust.\")\n",
    "            if dropout_mask is not None:\n",
    "                timestep_output = timestep_output * dropout_mask[0: current_length_index + 1]\n",
    "\n",
    "            # We've been doing computation with less than the full batch, so here we create a new\n",
    "            # variable for the the whole batch at this timestep and insert the result for the\n",
    "            # relevant elements of the batch into it.\n",
    "            print('\\t\\t\\t\\tset full_batch_previous memory/state!!')\n",
    "            full_batch_previous_memory = Variable(full_batch_previous_memory.data.clone())\n",
    "            full_batch_previous_state = Variable(full_batch_previous_state.data.clone())\n",
    "            full_batch_previous_memory[0:current_length_index + 1] = memory\n",
    "            full_batch_previous_state[0:current_length_index + 1] = timestep_output\n",
    "            output_accumulator[0:current_length_index + 1, index] = timestep_output\n",
    "\n",
    "        # Mimic the pytorch API by returning state in the following shape:\n",
    "        # (num_layers * num_directions, batch_size, ...). As this\n",
    "        # LSTM cell cannot be stacked, the first dimension here is just 1.\n",
    "        final_state = (full_batch_previous_state.unsqueeze(0),\n",
    "                       full_batch_previous_memory.unsqueeze(0))\n",
    "        print(f\"\\t\\t\\t\\tfinal_state = {final_state}\")\n",
    "\n",
    "        return output_accumulator, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo biLM layer params\n",
      "\tinput_size = 512\n",
      "\thidden_size = 512\n",
      "\tcell_size = 4096\n",
      "\tnum_layers = 2\n",
      "\tmemory_cell_clip_value = 3\n",
      "\tstate_projection_clip_value = 3\n",
      "forward_layers = [LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "), LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      ")]\n",
      "backward_layers = [LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "), LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "encoder = ElmobiLm(config, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD!!!!**************\n",
      "batch_size = 3\n",
      "total_sequence_length = 10\n",
      "_EncoderBase.sort_and_run_forward 메서드 실시...\n",
      "\tbatch_size = 3, num_valid = 3\n",
      "\tsequence_lengths = tensor([ 7,  6, 10], device='cuda:0')\n",
      "\t1. sorted_inputs.shape = torch.Size([3, 10, 512])\n",
      "\t2. sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "\t3. restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "\t4. sorting_indices = tensor([2, 0, 1], device='cuda:0')\n",
      "\t             sorted_inputs.shape  = torch.Size([3, 10, 512])\n",
      "\tpacked_sequence_input.data.shape  = torch.Size([23, 512])\n",
      "\tpacked_sequence_input.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n",
      "\tself.stateful is True\n",
      "\tstateful is True,\n",
      "\t\tConduct `_get_initial_states`\n",
      "\tRUN `_lstm_forward`... by initial_states\n",
      "\t\tinitial_state is None? True\n",
      "\t\tOops, Assign hidden_state = [None] * len(self.forward_layers)\n",
      "\t\thidden_states = [None, None]\n",
      "\t\tinputs is `PackedSequence`\n",
      "\t\ttype(inputs) = <class 'torch.nn.utils.rnn.PackedSequence'>\n",
      "\t\t\tinputs.data.shape = torch.Size([23, 512])\n",
      "\t\t\tinputs.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n",
      "\t\t\tinputs.sorted_indices = None\n",
      "\t\t\tinputs.unsorted_indices = None\n",
      "\t\tRestore PAD_char to inputs...\n",
      "\t\t바뀐 inputs의 정보 출력\n",
      "\t\ttype(inputs) = <class 'torch.Tensor'>\n",
      "\t\t\tinputs.shape = torch.Size([3, 10, 512])\n",
      "\t\tbatch_lengths = tensor([10,  7,  6])\n",
      "\t\tAssign forward_output_sequence = backward_output_sequence = inputs\n",
      "\t\tSet final_states, sequqnce_outputs as empty list, []\n",
      "\t\tGet a forward layer and backward layer at layer 1\n",
      "\t\tCaching...: output_sequence to cache both forward and backward\n",
      "\t\tstate is None? True\n",
      "\t\t\tOops, then forward and backward state is also 'None'\n",
      "\t\tRUN forward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? True\n",
      "\t\t\t\tOh, then create full_batch_previous memory and state by (3, 4096) tensor filling 0.\n",
      "\t\t\t\tSet current_length_index... is it forward?? True\n",
      "\t\t\t\tOk, forward!! current_length_index = batch_size - 1 = 2\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "binary_mask = tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [False,  True,  True,  ...,  True,  True,  True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[0.0000, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 0 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0039, -0.0040,  0.0037,  ...,  0.0005, -0.0002, -0.0097],\n",
      "        [-0.0039, -0.0040,  0.0037,  ...,  0.0005, -0.0002, -0.0097],\n",
      "        [-0.0039, -0.0040,  0.0037,  ...,  0.0005, -0.0002, -0.0097]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0039, -0.0040,  0.0037,  ...,  0.0005, -0.0002, -0.0097],\n",
      "        [-0.0039, -0.0040,  0.0037,  ...,  0.0005, -0.0002, -0.0097],\n",
      "        [-0.0039, -0.0040,  0.0037,  ...,  0.0005, -0.0002, -0.0097]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0078, -0.0260, -0.0205,  ...,  0.0145,  0.0510, -0.0102],\n",
      "        [ 0.0290,  0.0047, -0.0227,  ...,  0.0162,  0.0292, -0.0161],\n",
      "        [ 0.0056, -0.0269, -0.0216,  ..., -0.0044,  0.0573, -0.0453]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0074, -0.0038,  0.0095,  ...,  0.0020, -0.0020, -0.0162],\n",
      "        [-0.0041, -0.0024,  0.0056,  ...,  0.0042, -0.0019, -0.0126],\n",
      "        [-0.0085, -0.0078,  0.0061,  ...,  0.0016, -0.0025, -0.0147]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0074, -0.0038,  0.0095,  ...,  0.0020, -0.0020, -0.0162],\n",
      "        [-0.0041, -0.0024,  0.0056,  ...,  0.0042, -0.0019, -0.0126],\n",
      "        [-0.0085, -0.0078,  0.0061,  ...,  0.0016, -0.0025, -0.0147]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0439, -0.0200, -0.0223,  ...,  0.0114,  0.0445, -0.0566],\n",
      "        [-0.0055, -0.0275, -0.0182,  ...,  0.0095,  0.0294, -0.0096],\n",
      "        [ 0.0261, -0.0046, -0.0160,  ...,  0.0313,  0.0107, -0.0573]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-7.3963e-03, -7.1740e-04,  1.3260e-02,  ...,  6.4142e-03,\n",
      "          1.1457e-05, -1.6692e-02],\n",
      "        [-3.2916e-03, -2.0885e-03,  7.0508e-03,  ...,  6.7454e-03,\n",
      "         -2.3522e-03, -1.4807e-02],\n",
      "        [-7.1011e-03, -1.2703e-03,  1.3867e-02,  ...,  5.7810e-03,\n",
      "          3.6582e-03, -1.8108e-02]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-7.3963e-03, -7.1740e-04,  1.3260e-02,  ...,  6.4142e-03,\n",
      "          1.1457e-05, -1.6692e-02],\n",
      "        [-3.2916e-03, -2.0885e-03,  7.0508e-03,  ...,  6.7454e-03,\n",
      "         -2.3522e-03, -1.4807e-02],\n",
      "        [-7.1011e-03, -1.2703e-03,  1.3867e-02,  ...,  5.7810e-03,\n",
      "          3.6582e-03, -1.8108e-02]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0195,  0.0024, -0.0233,  ...,  0.0120,  0.0189, -0.0226],\n",
      "        [ 0.0259, -0.0076, -0.0144,  ...,  0.0053,  0.0274, -0.0328],\n",
      "        [ 0.0088, -0.0010, -0.0169,  ...,  0.0276,  0.0261, -0.0068]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0127,  0.0013,  0.0197,  ...,  0.0096,  0.0027, -0.0182],\n",
      "        [-0.0059, -0.0074,  0.0071,  ...,  0.0074, -0.0051, -0.0148],\n",
      "        [-0.0065, -0.0021,  0.0095,  ...,  0.0033, -0.0022, -0.0186]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0127,  0.0013,  0.0197,  ...,  0.0096,  0.0027, -0.0182],\n",
      "        [-0.0059, -0.0074,  0.0071,  ...,  0.0074, -0.0051, -0.0148],\n",
      "        [-0.0065, -0.0021,  0.0095,  ...,  0.0033, -0.0022, -0.0186]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0267,  0.0010, -0.0196,  ...,  0.0005,  0.0283, -0.0203],\n",
      "        [ 0.0260, -0.0238,  0.0250,  ...,  0.0017,  0.0297, -0.0235],\n",
      "        [-0.0157,  0.0032, -0.0359,  ...,  0.0232,  0.0049, -0.0217]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0125,  0.0002,  0.0185,  ...,  0.0049,  0.0060, -0.0169],\n",
      "        [-0.0054, -0.0098,  0.0103,  ...,  0.0071, -0.0063, -0.0148],\n",
      "        [-0.0100, -0.0047,  0.0079,  ...,  0.0065, -0.0025, -0.0214]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0125,  0.0002,  0.0185,  ...,  0.0049,  0.0060, -0.0169],\n",
      "        [-0.0054, -0.0098,  0.0103,  ...,  0.0071, -0.0063, -0.0148],\n",
      "        [-0.0100, -0.0047,  0.0079,  ...,  0.0065, -0.0025, -0.0214]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-6.8748e-03, -1.3605e-02,  7.1700e-03,  ...,  1.2522e-02,\n",
      "          2.0355e-02, -2.4865e-02],\n",
      "        [-2.9532e-03, -1.3722e-02, -2.7846e-02,  ..., -3.4922e-03,\n",
      "          1.9039e-03, -3.1346e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0146, -0.0030,  0.0176,  ..., -0.0002,  0.0026, -0.0199],\n",
      "        [-0.0086, -0.0078,  0.0109,  ...,  0.0064, -0.0026, -0.0179]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0146, -0.0030,  0.0176,  ..., -0.0002,  0.0026, -0.0199],\n",
      "        [-0.0086, -0.0078,  0.0109,  ...,  0.0064, -0.0026, -0.0179]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 7.8088e-03, -2.3229e-02, -1.9114e-02,  ...,  2.5953e-02,\n",
      "          5.1098e-02, -5.5057e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0176, -0.0037,  0.0164,  ..., -0.0060, -0.0005, -0.0230]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0176, -0.0037,  0.0164,  ..., -0.0060, -0.0005, -0.0230]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 3.9943e-02, -6.3473e-04, -2.4333e-02, -2.6465e-02,  3.6282e-02,\n",
      "         -8.1245e-02,  2.0234e-02, -4.7533e-02, -1.4706e-02,  1.8934e-02,\n",
      "          4.9761e-04, -6.6569e-02,  3.0510e-02,  8.3242e-03,  3.4185e-03,\n",
      "          4.2514e-02,  3.6875e-02,  1.1155e-02, -6.4569e-02, -2.6975e-02,\n",
      "          1.4409e-02, -8.3901e-03,  1.4958e-02,  1.4861e-02,  1.6609e-02,\n",
      "          4.6618e-02,  8.1440e-02,  6.3781e-02,  4.3447e-02,  5.9358e-03,\n",
      "         -5.9265e-04,  3.1957e-02, -3.8824e-03,  6.6324e-02,  1.1476e-03,\n",
      "          3.0135e-02,  2.8017e-02, -2.6136e-03, -8.7879e-02,  8.0971e-03,\n",
      "          4.2865e-02,  2.4699e-02, -1.6333e-03,  4.5484e-02,  4.8548e-02,\n",
      "         -2.7261e-02,  4.9371e-02, -1.1094e-02, -6.2017e-02,  6.3433e-02,\n",
      "          1.1362e-02, -3.7070e-02, -2.8188e-02, -3.8195e-02, -5.8539e-02,\n",
      "          1.2449e-02, -2.7576e-03,  3.2095e-02,  7.0868e-03,  7.0801e-03,\n",
      "         -2.9167e-02, -6.1884e-02,  3.3293e-02,  7.4093e-03,  8.6460e-03,\n",
      "          2.1394e-02,  2.8104e-02, -2.3261e-02, -4.2063e-02,  2.1559e-02,\n",
      "         -7.4343e-03, -4.5396e-02, -1.7797e-02, -8.0800e-03, -5.4489e-02,\n",
      "         -7.8084e-03,  5.2327e-03, -1.9146e-02, -4.8832e-02,  2.1798e-02,\n",
      "          9.2141e-03, -3.6777e-02,  6.6118e-03,  3.2340e-02,  1.9615e-02,\n",
      "          2.1029e-02, -2.2645e-02, -2.0520e-02, -8.7323e-02, -4.5272e-02,\n",
      "         -2.7091e-02, -2.3780e-02, -2.8575e-02,  7.5322e-02,  2.6836e-02,\n",
      "         -1.4969e-02,  1.4021e-02,  3.6706e-03, -6.0353e-02, -9.9927e-03,\n",
      "         -7.4608e-02, -6.0827e-03,  4.9744e-02, -1.1646e-02, -1.5283e-02,\n",
      "          3.9851e-02,  1.9263e-02, -2.9827e-02,  3.6977e-02,  4.5320e-02,\n",
      "         -4.4115e-02, -5.5769e-02,  1.1291e-02,  3.4049e-02, -4.0742e-02,\n",
      "         -7.8004e-02, -3.9623e-02, -6.5474e-02, -5.7309e-02,  3.2225e-03,\n",
      "          4.3883e-03,  2.9382e-02,  7.0717e-02,  6.7704e-02, -2.4883e-02,\n",
      "          2.6200e-02, -1.4513e-02,  2.5271e-02,  3.6185e-02, -6.2445e-03,\n",
      "         -1.5884e-02, -5.3081e-03, -5.1873e-03,  4.8760e-02,  4.0506e-02,\n",
      "          4.3785e-02,  1.0459e-02,  5.6056e-02, -4.2227e-03,  4.8874e-02,\n",
      "         -2.1245e-02, -3.1359e-03,  1.8048e-02, -2.3473e-02,  5.5935e-02,\n",
      "         -3.8959e-02,  1.0918e-02, -8.7178e-02,  3.0119e-03, -2.1534e-02,\n",
      "          3.2873e-03,  7.5276e-02, -3.9145e-02, -2.3037e-02,  1.6807e-02,\n",
      "          5.1875e-02, -6.6744e-05, -2.1719e-02,  3.4947e-02, -1.9906e-02,\n",
      "         -4.2533e-02, -1.4287e-02, -1.7697e-02, -3.5453e-02, -2.6380e-02,\n",
      "          3.0872e-03, -3.2254e-02, -5.6165e-02, -1.4399e-02,  6.4918e-04,\n",
      "         -4.0651e-02,  5.3044e-02,  5.6464e-02, -7.1317e-02,  2.5132e-02,\n",
      "         -2.7497e-02, -1.5007e-02, -4.0442e-02,  3.2662e-02, -3.9329e-02,\n",
      "          1.1346e-02,  5.2525e-02,  2.5804e-02,  2.6124e-02, -8.0331e-02,\n",
      "          1.4954e-02,  2.4630e-02,  1.5274e-02, -4.8547e-02,  3.7759e-02,\n",
      "         -2.4851e-02, -6.2086e-02,  1.5069e-02,  5.3204e-02,  1.5821e-02,\n",
      "         -2.3344e-02, -1.3930e-03,  1.4708e-02,  1.1591e-02, -7.3345e-02,\n",
      "         -3.6105e-02, -2.8722e-02,  7.6852e-04, -2.2699e-02, -8.3008e-02,\n",
      "          7.0183e-02, -3.0014e-02, -9.3609e-03, -2.8232e-02,  6.1581e-02,\n",
      "         -2.4002e-02,  1.5176e-02,  5.0153e-02, -9.7399e-04, -5.9783e-02,\n",
      "          3.4617e-02, -2.9192e-03,  2.7485e-02,  6.9439e-02, -1.1364e-02,\n",
      "         -1.3452e-01,  2.2476e-02, -1.4151e-02, -7.1442e-03, -1.2852e-02,\n",
      "         -2.2974e-02,  2.5305e-02,  9.9605e-03,  2.9039e-03, -3.2136e-02,\n",
      "          1.0917e-02,  3.9640e-02, -1.0201e-02, -1.9363e-02, -1.1269e-02,\n",
      "          9.5938e-03,  5.2384e-02, -5.6701e-02, -8.5196e-03,  1.2629e-02,\n",
      "         -4.5141e-02, -2.6792e-02, -1.0696e-02,  2.9739e-02, -5.2543e-02,\n",
      "          4.4625e-03,  3.8448e-02, -1.4611e-03,  7.8076e-02, -1.0361e-02,\n",
      "         -1.6192e-02, -7.5027e-03,  5.8933e-02, -8.2104e-03, -1.0303e-02,\n",
      "         -1.0491e-02, -6.1503e-02,  3.2754e-02, -1.5981e-02, -2.7549e-02,\n",
      "         -5.1394e-02, -3.0974e-02,  4.4842e-02,  1.3567e-02, -4.8374e-03,\n",
      "          4.6250e-02,  4.9839e-02, -1.7988e-02,  3.7572e-02, -4.8327e-02,\n",
      "         -1.2389e-01,  9.6038e-03,  2.2187e-02,  1.4403e-02,  4.4363e-03,\n",
      "         -7.8948e-03, -3.4436e-02, -5.8862e-02, -3.2297e-02,  2.4687e-03,\n",
      "          1.3516e-02, -1.9046e-02,  6.2428e-02, -2.4486e-02,  2.5915e-02,\n",
      "         -4.7463e-02,  4.1579e-02, -1.3121e-02, -2.5279e-02, -1.6883e-02,\n",
      "         -1.2437e-02, -6.8095e-02,  3.2792e-02, -3.9081e-02,  2.3718e-02,\n",
      "         -7.6230e-03,  2.5899e-02, -9.0227e-03, -4.4538e-02,  1.8885e-02,\n",
      "          6.7445e-03,  2.3967e-02,  3.6491e-02,  6.1667e-02,  2.9088e-02,\n",
      "         -4.6740e-02,  4.2268e-02,  5.3146e-03,  3.4716e-03,  1.3579e-02,\n",
      "         -3.4293e-02, -7.6553e-03, -3.0861e-02,  2.1517e-02, -1.5814e-02,\n",
      "          1.6505e-02,  2.4900e-02,  3.6272e-02, -1.1502e-02,  2.1404e-02,\n",
      "         -1.9012e-02,  4.1999e-02, -2.3130e-02, -1.2259e-03, -3.5337e-02,\n",
      "         -6.3823e-02, -1.7515e-02, -4.7187e-02, -2.1176e-02,  2.9516e-02,\n",
      "         -5.6702e-02, -3.0155e-02, -4.3931e-02,  5.9581e-02, -4.8076e-02,\n",
      "          3.2992e-02,  5.8642e-02,  4.0638e-02, -4.9440e-03, -2.6222e-02,\n",
      "         -1.7771e-02, -4.0846e-02,  2.8135e-02, -4.8641e-02, -3.6589e-02,\n",
      "         -4.3364e-03, -4.0931e-02,  1.2174e-02, -2.9099e-02, -3.0528e-02,\n",
      "          1.0307e-02,  6.1238e-02, -5.8316e-02, -1.3105e-02,  9.0719e-03,\n",
      "         -1.7113e-02, -1.6299e-02, -4.1770e-02,  5.2599e-02, -2.1581e-02,\n",
      "          1.3731e-02,  3.1022e-02, -9.9482e-03,  2.3787e-02, -7.2789e-02,\n",
      "          3.9918e-03, -1.1121e-01,  6.5336e-02, -3.4317e-02,  6.3555e-02,\n",
      "          4.7264e-02, -2.1690e-02,  3.2510e-02, -2.9224e-03,  8.1941e-03,\n",
      "          2.2115e-03,  4.4054e-02, -2.8647e-02, -1.7062e-02,  3.1700e-02,\n",
      "          1.3082e-02, -7.8945e-03,  5.9706e-02, -3.6144e-03,  1.2505e-02,\n",
      "          3.5541e-02,  5.5393e-02,  7.4438e-03,  3.9664e-02, -2.1253e-02,\n",
      "         -6.8311e-03, -4.3957e-02,  3.7919e-02, -2.8258e-02, -8.0768e-03,\n",
      "         -4.2647e-02,  3.2717e-02, -2.9899e-02, -9.3153e-03,  1.5475e-02,\n",
      "          3.0740e-02,  1.8109e-02,  2.8607e-02, -3.8829e-02, -4.7982e-02,\n",
      "         -7.1243e-03, -2.4547e-02,  8.3619e-04, -1.8965e-02,  1.3804e-02,\n",
      "          1.0712e-02, -3.4088e-02, -7.9899e-02, -7.0846e-03,  6.9593e-03,\n",
      "         -2.1685e-02,  5.5655e-02, -1.2060e-02,  2.3431e-03,  7.2531e-02,\n",
      "         -6.0682e-02, -7.4575e-02, -1.7317e-02,  5.8419e-03, -2.3470e-02,\n",
      "          9.9523e-03, -6.2580e-02,  7.6685e-02,  2.9135e-02, -3.8954e-02,\n",
      "          1.4604e-02,  3.1968e-03,  3.2509e-02, -2.8317e-03,  2.2350e-02,\n",
      "         -4.7101e-02, -5.0960e-02,  3.1412e-03, -1.5378e-02, -2.6794e-02,\n",
      "         -1.9578e-02,  1.0552e-02, -6.9772e-02,  2.0636e-02, -2.9148e-03,\n",
      "         -4.0716e-02,  2.5176e-03,  7.4695e-03, -3.4024e-04,  3.2252e-02,\n",
      "         -2.8218e-02, -1.9674e-02, -2.1514e-02, -9.1328e-03,  2.0247e-02,\n",
      "          3.0113e-02, -7.1527e-02,  4.6293e-02,  2.1005e-02, -2.0541e-02,\n",
      "          4.0748e-02,  4.6423e-03, -2.4288e-02, -1.5478e-02, -3.5585e-02,\n",
      "          2.1604e-02,  3.8710e-02,  1.4737e-02, -3.1023e-02, -5.9019e-02,\n",
      "          3.0318e-02,  3.5039e-02, -1.1337e-03,  3.8222e-02,  1.5961e-02,\n",
      "          2.4988e-02,  1.0752e-02,  7.9247e-02,  3.4722e-02, -7.4136e-02,\n",
      "          3.9050e-02, -1.8820e-02,  6.0536e-02,  6.0405e-03, -1.8136e-02,\n",
      "          4.7733e-02,  4.5106e-03, -5.9822e-03, -1.5836e-02,  1.4583e-03,\n",
      "         -2.1234e-02, -2.3092e-02, -6.0511e-03,  7.6471e-02, -1.0707e-02,\n",
      "         -3.3524e-02,  3.2486e-02, -7.2459e-03, -6.4545e-02, -4.4839e-02,\n",
      "          3.7363e-02, -2.0615e-02, -1.1821e-01, -2.0830e-02,  6.9055e-02,\n",
      "         -1.0918e-02,  5.4751e-02,  2.7437e-02,  3.1381e-02,  1.9435e-02,\n",
      "          5.1162e-02, -3.3509e-02]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0158, -0.0033,  0.0181,  ..., -0.0054, -0.0009, -0.0210]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0158, -0.0033,  0.0181,  ..., -0.0054, -0.0009, -0.0210]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0225,  0.0021, -0.0073, -0.0453,  0.0067, -0.1035,  0.0007, -0.0384,\n",
      "         -0.0241, -0.0236, -0.0056, -0.0469,  0.0058, -0.0028,  0.0082,  0.0394,\n",
      "          0.0411,  0.0036, -0.0933, -0.0109,  0.0350, -0.0014,  0.0195,  0.0090,\n",
      "          0.0102,  0.0144,  0.0910,  0.0898,  0.0680, -0.0267,  0.0296,  0.0123,\n",
      "         -0.0376,  0.0462,  0.0312, -0.0027,  0.0219, -0.0156, -0.0906, -0.0108,\n",
      "          0.0491, -0.0022, -0.0330,  0.0454,  0.0668, -0.0107,  0.0695, -0.0285,\n",
      "         -0.1164,  0.0746,  0.0207,  0.0046, -0.0736, -0.0499, -0.0580, -0.0040,\n",
      "          0.0032,  0.0278,  0.0149, -0.0144,  0.0139, -0.0536,  0.0244,  0.0268,\n",
      "         -0.0166, -0.0146, -0.0104, -0.0225, -0.0341,  0.0614, -0.0234, -0.0311,\n",
      "         -0.0392,  0.0299, -0.0474, -0.0211, -0.0228, -0.0282, -0.0288, -0.0021,\n",
      "         -0.0283, -0.0399,  0.0053,  0.0269, -0.0246,  0.0394, -0.0482, -0.0131,\n",
      "         -0.0672, -0.0286, -0.0382, -0.0050, -0.0136,  0.0675,  0.0515,  0.0383,\n",
      "          0.0347, -0.0290, -0.0956,  0.0055, -0.0542,  0.0374,  0.0811, -0.0625,\n",
      "          0.0048,  0.0439,  0.0274, -0.0130,  0.0037,  0.0702, -0.0189, -0.0653,\n",
      "          0.0338,  0.0484, -0.0758, -0.1118, -0.0982, -0.0586, -0.1017,  0.0323,\n",
      "         -0.0187,  0.0306,  0.0579,  0.0252, -0.0435,  0.0251, -0.0279,  0.0555,\n",
      "          0.0822,  0.0015,  0.0172,  0.0274, -0.0081,  0.0368,  0.0218,  0.0329,\n",
      "          0.0185,  0.0088, -0.0144,  0.0301,  0.0175, -0.0075,  0.0198,  0.0053,\n",
      "          0.0708,  0.0166, -0.0065, -0.0641, -0.0199, -0.0439,  0.0324,  0.0846,\n",
      "         -0.0156, -0.0623,  0.0368,  0.0376, -0.0219, -0.0321,  0.0116, -0.0214,\n",
      "         -0.0275, -0.0293, -0.0180, -0.0346, -0.0491,  0.0416, -0.0381, -0.0820,\n",
      "         -0.0298,  0.0151, -0.0111,  0.0488,  0.0482, -0.0697,  0.0230, -0.0346,\n",
      "         -0.0385, -0.0320,  0.0022, -0.0195,  0.0382,  0.0669,  0.0117, -0.0143,\n",
      "         -0.0986,  0.0230, -0.0114,  0.0136, -0.0391,  0.0525, -0.0245, -0.0297,\n",
      "         -0.0252,  0.0744,  0.0637,  0.0055,  0.0261,  0.0363, -0.0025, -0.0683,\n",
      "         -0.0255, -0.0456,  0.0402, -0.0391, -0.0926,  0.0581, -0.0183, -0.0362,\n",
      "         -0.0535,  0.0409, -0.0179,  0.0289,  0.0655,  0.0294, -0.0775,  0.0396,\n",
      "          0.0239,  0.0403,  0.0801, -0.0544, -0.1100, -0.0068, -0.0387, -0.0217,\n",
      "          0.0214, -0.0616, -0.0089, -0.0123,  0.0534, -0.0370,  0.0211,  0.0167,\n",
      "         -0.0111, -0.0011,  0.0025,  0.0404,  0.0642, -0.0452, -0.0113,  0.0538,\n",
      "         -0.0520, -0.0056,  0.0366,  0.0759, -0.0529, -0.0374,  0.0588,  0.0106,\n",
      "          0.1057, -0.0457, -0.0078,  0.0133,  0.0720, -0.0095, -0.0401,  0.0218,\n",
      "         -0.0456,  0.0502,  0.0087, -0.0067, -0.0664, -0.0569,  0.0319,  0.0456,\n",
      "          0.0117,  0.0440,  0.0148, -0.0104,  0.0672, -0.0583, -0.1598,  0.0023,\n",
      "          0.0538, -0.0143,  0.0224,  0.0161, -0.0428, -0.0923, -0.0291, -0.0308,\n",
      "         -0.0071, -0.0245,  0.0439, -0.0270,  0.0261, -0.0623,  0.0007, -0.0537,\n",
      "         -0.0007, -0.0388,  0.0227, -0.0308,  0.0428, -0.0017,  0.0260, -0.0640,\n",
      "         -0.0064, -0.0349, -0.0219,  0.0544,  0.0239,  0.0471,  0.0232,  0.0261,\n",
      "          0.0282, -0.0696,  0.0527,  0.0118,  0.0263, -0.0006, -0.0661,  0.0209,\n",
      "          0.0058,  0.0030, -0.0387,  0.0064,  0.0166,  0.0501, -0.0120,  0.0489,\n",
      "          0.0016,  0.0325, -0.0278,  0.0218,  0.0103, -0.0845, -0.0082, -0.0481,\n",
      "         -0.0449,  0.0369, -0.0878, -0.0075, -0.0770,  0.0889, -0.0595,  0.0261,\n",
      "          0.0722,  0.0245, -0.0125, -0.0249, -0.0267, -0.0498, -0.0126, -0.0515,\n",
      "         -0.0232, -0.0248, -0.0696,  0.0107, -0.0039,  0.0008,  0.0189,  0.0703,\n",
      "         -0.0633,  0.0129,  0.0091, -0.0232,  0.0251, -0.0279,  0.1155, -0.0159,\n",
      "         -0.0343,  0.0638, -0.0056,  0.0369, -0.0636, -0.0179, -0.1186,  0.0721,\n",
      "         -0.0307,  0.0792,  0.0270,  0.0300,  0.0747,  0.0376,  0.0251,  0.0128,\n",
      "          0.0466, -0.0187, -0.0412,  0.0287,  0.0269, -0.0115,  0.0786,  0.0066,\n",
      "         -0.0067,  0.0407,  0.0497,  0.0495,  0.0253, -0.0395, -0.0038, -0.0613,\n",
      "          0.0059, -0.0225,  0.0075,  0.0116,  0.0301, -0.0510,  0.0483, -0.0205,\n",
      "          0.0458,  0.0350,  0.0831, -0.0208, -0.0651,  0.0033, -0.0349,  0.0098,\n",
      "         -0.0437,  0.0113, -0.0011, -0.0237, -0.0102,  0.0026,  0.0296, -0.0294,\n",
      "          0.0504,  0.0353, -0.0013,  0.0868, -0.0338, -0.1060, -0.0469, -0.0076,\n",
      "         -0.0594, -0.0139, -0.1025,  0.0495,  0.0576, -0.0711,  0.0156, -0.0270,\n",
      "          0.0126,  0.0289,  0.0422, -0.0147, -0.0343,  0.0242, -0.0238,  0.0042,\n",
      "         -0.0163, -0.0254, -0.0318, -0.0033, -0.0148, -0.0554, -0.0152,  0.0177,\n",
      "         -0.0006,  0.0078,  0.0026, -0.0279, -0.0466, -0.0133, -0.0026,  0.0167,\n",
      "         -0.0890,  0.0875, -0.0195, -0.0232,  0.0426, -0.0253, -0.0016, -0.0328,\n",
      "          0.0101,  0.0372,  0.0486, -0.0026, -0.0226, -0.0536,  0.0217,  0.0102,\n",
      "         -0.0430,  0.0425,  0.0180, -0.0253,  0.0030,  0.0580,  0.0355, -0.0872,\n",
      "          0.0397, -0.0145,  0.0678,  0.0079, -0.0342,  0.0471,  0.0262,  0.0146,\n",
      "         -0.0287, -0.0009, -0.0416,  0.0084, -0.0227,  0.0526, -0.0619, -0.0201,\n",
      "          0.0613,  0.0342, -0.0878, -0.0522,  0.0243, -0.0385, -0.0821, -0.0479,\n",
      "          0.0745, -0.0005,  0.0558,  0.0333, -0.0184,  0.0317,  0.0303, -0.0447]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 9 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0154, -0.0035,  0.0223,  ...,  0.0023,  0.0046, -0.0197]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0154, -0.0035,  0.0223,  ...,  0.0023,  0.0046, -0.0197]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 4.0999e-03, -6.1420e-03, -4.0220e-03, -4.2812e-02,  2.2545e-03,\n",
      "         -5.5843e-02, -3.3867e-03, -2.4588e-02, -5.0965e-02,  5.2278e-03,\n",
      "          3.2208e-03, -6.6673e-02, -4.4035e-03, -3.9993e-02, -1.1946e-02,\n",
      "          5.1611e-02,  9.1874e-03, -3.4062e-03, -6.2161e-02, -1.2382e-02,\n",
      "          1.7805e-02,  3.6729e-03,  4.2529e-03,  1.7206e-02, -1.2278e-02,\n",
      "          1.4176e-02,  6.9620e-02,  5.3863e-02,  2.9146e-02,  1.3687e-04,\n",
      "          4.2660e-03,  2.6004e-02, -2.9728e-04,  2.5874e-02,  5.3049e-04,\n",
      "         -9.1188e-03, -2.0923e-03, -3.1675e-02, -6.1669e-02,  2.7857e-02,\n",
      "          3.7919e-02,  4.7040e-02, -2.9004e-02,  2.8061e-02,  2.2440e-02,\n",
      "         -1.7007e-02,  4.6762e-02, -9.9050e-03, -6.0391e-02,  4.4008e-02,\n",
      "          2.7127e-02, -1.0071e-02, -4.4653e-02, -3.4113e-02, -5.4936e-02,\n",
      "         -2.7576e-02, -1.1772e-02,  1.6111e-02, -2.4791e-02, -1.2392e-02,\n",
      "         -4.5851e-02, -7.0682e-02,  6.8990e-02,  1.5804e-02, -1.8523e-02,\n",
      "          1.7020e-02,  3.7107e-03, -3.7832e-02, -9.7512e-03,  2.5285e-02,\n",
      "         -1.8639e-02, -1.1800e-02, -3.4856e-02,  2.3652e-03, -4.5387e-02,\n",
      "         -2.9169e-02, -7.3532e-03, -5.6582e-02, -4.0548e-02,  2.6095e-02,\n",
      "         -4.1448e-03, -7.7175e-03, -4.9508e-03,  5.8557e-02,  7.4085e-03,\n",
      "          2.0493e-02, -2.6433e-02, -5.9150e-02, -8.5825e-02, -3.6332e-02,\n",
      "         -6.7253e-02, -1.8667e-02,  1.9783e-02,  4.1228e-02,  3.5272e-02,\n",
      "         -1.5310e-02,  2.6563e-02,  9.0099e-03, -5.8591e-02, -1.2875e-02,\n",
      "         -4.5589e-02,  3.7654e-02,  5.1700e-02, -9.7946e-03, -2.8365e-02,\n",
      "          3.8642e-02,  3.2772e-02, -7.0115e-03,  1.7229e-02,  4.8634e-02,\n",
      "         -4.3900e-02, -5.7408e-02,  4.0625e-02,  4.4493e-02, -5.2082e-02,\n",
      "         -7.4520e-02, -5.9377e-02, -6.7939e-02, -7.3847e-02,  1.3183e-02,\n",
      "         -4.9373e-03,  1.8830e-02,  9.7497e-02,  1.3256e-02,  1.6883e-03,\n",
      "          1.4859e-02, -2.3568e-03,  5.6624e-02,  5.3152e-02, -1.2295e-02,\n",
      "          1.2090e-02, -1.3728e-05,  1.0096e-02,  3.3864e-02,  4.2704e-02,\n",
      "          5.4648e-02,  4.1818e-03,  3.7166e-03, -1.5428e-02,  4.5995e-02,\n",
      "          2.1937e-02, -2.8221e-02,  1.6410e-02, -4.7979e-04,  4.1428e-02,\n",
      "         -2.7716e-02,  1.6255e-03, -6.3316e-02, -1.8258e-02, -4.5491e-02,\n",
      "          1.1397e-02,  7.0264e-02,  1.8543e-02, -4.3941e-02,  1.5809e-02,\n",
      "          1.0576e-02, -1.1876e-02,  1.1684e-02, -1.4857e-03, -1.3847e-02,\n",
      "         -3.5052e-03, -2.9399e-02, -3.4140e-02, -3.0366e-02, -4.2807e-02,\n",
      "          2.2627e-02,  9.7901e-06, -3.6235e-02, -5.8573e-03,  2.0378e-02,\n",
      "         -6.4172e-02,  3.1886e-02,  1.5496e-02, -2.8181e-02,  3.5096e-02,\n",
      "         -6.8952e-03, -2.5324e-02, -3.1501e-02,  7.9062e-03, -2.8681e-02,\n",
      "         -7.9037e-03,  3.5255e-02,  2.4252e-02,  2.9042e-02, -6.2465e-02,\n",
      "          1.4517e-02, -1.9319e-02, -2.7564e-02, -2.5307e-02,  3.7366e-02,\n",
      "         -3.2373e-02, -6.7099e-02,  1.8414e-02,  2.4672e-02,  1.2600e-02,\n",
      "         -5.6666e-03,  2.5549e-03,  4.0709e-03,  1.9000e-02, -6.1543e-02,\n",
      "          1.2990e-02, -2.5736e-02,  1.1150e-02, -2.5157e-02, -8.1259e-02,\n",
      "          2.5197e-02,  1.6732e-03,  2.1115e-02, -4.1658e-02,  6.6255e-02,\n",
      "         -2.5587e-02, -3.2157e-04,  1.2388e-02,  4.0023e-02, -6.1048e-02,\n",
      "          6.0717e-02,  2.1476e-02,  5.8257e-03,  4.3190e-02, -5.3267e-03,\n",
      "         -1.1518e-01,  5.6670e-03, -2.2334e-02, -2.0324e-02,  1.9542e-03,\n",
      "         -3.7446e-02,  1.5559e-02, -7.3694e-03,  2.4515e-02, -4.5967e-02,\n",
      "         -3.7894e-03,  3.2286e-02,  1.0811e-02,  4.4440e-03,  4.8463e-03,\n",
      "          2.5042e-02,  4.7359e-02, -6.8103e-02, -6.5093e-03,  7.0785e-02,\n",
      "         -3.5528e-02,  4.7189e-03, -1.0189e-02,  4.2352e-02, -2.0553e-02,\n",
      "         -1.7234e-02,  1.3394e-02,  1.8244e-02,  6.2473e-02, -4.8055e-02,\n",
      "          1.2645e-02, -7.0589e-03,  5.6752e-02, -3.4373e-02, -3.1496e-02,\n",
      "         -5.9777e-03, -2.4881e-02,  7.9201e-02,  1.3747e-02, -2.5393e-03,\n",
      "         -3.2503e-02, -3.1110e-02,  5.7813e-02, -4.5621e-03, -1.0531e-02,\n",
      "          4.2530e-02,  3.0264e-02, -6.1265e-03,  4.5427e-02, -5.1061e-02,\n",
      "         -1.5275e-01,  1.0616e-02,  2.6280e-02,  1.8947e-02,  3.2359e-02,\n",
      "         -1.4984e-02,  3.7450e-03, -6.5668e-02,  2.4765e-03, -3.2025e-02,\n",
      "          2.9947e-02,  4.5328e-03,  3.0595e-02, -5.2844e-03,  7.7454e-02,\n",
      "         -1.9671e-02,  2.9783e-02, -3.6750e-02, -1.6449e-02,  6.7979e-03,\n",
      "          6.7633e-03, -3.4163e-02,  2.9027e-02, -1.8871e-02,  3.0973e-02,\n",
      "         -3.5600e-02, -2.3323e-02, -2.5930e-02, -4.0542e-02,  2.5499e-02,\n",
      "          3.3001e-02,  1.2359e-02,  2.5706e-02,  3.0320e-02,  4.6401e-02,\n",
      "         -2.3879e-02,  2.0202e-02, -2.2375e-03,  8.2037e-03, -2.0483e-02,\n",
      "         -7.8328e-02, -1.1248e-02, -9.1449e-03,  7.5573e-03, -2.2487e-02,\n",
      "         -1.6329e-02,  1.8263e-02,  3.7522e-02,  1.9167e-02,  1.4377e-02,\n",
      "         -1.4204e-02,  3.2154e-02, -4.2945e-02,  5.7657e-03, -8.0327e-04,\n",
      "         -6.6083e-02, -2.8194e-04, -1.8378e-02, -4.8475e-02,  2.3617e-02,\n",
      "         -3.3750e-02,  1.1834e-02, -1.0890e-02,  2.5747e-02, -3.7289e-02,\n",
      "          5.2489e-02,  3.3673e-02,  2.8601e-02,  4.2754e-03, -4.7038e-02,\n",
      "         -3.0422e-02, -1.7425e-02,  1.3590e-02, -5.1289e-02, -2.6229e-02,\n",
      "         -1.0611e-02, -7.7437e-03,  2.7641e-02, -4.1801e-03,  2.6006e-02,\n",
      "         -3.3880e-03,  3.3067e-03, -5.1195e-02,  1.0351e-02, -5.3868e-03,\n",
      "         -3.0990e-02,  2.9896e-02, -2.5370e-02,  7.7419e-02, -1.1452e-02,\n",
      "         -2.1355e-02,  2.0652e-02,  5.2692e-02,  3.8496e-02, -5.2882e-02,\n",
      "          2.0792e-03, -8.6128e-02,  3.2290e-02, -1.0719e-02,  1.2896e-02,\n",
      "          3.0134e-02,  1.5601e-02,  3.8173e-02,  3.0548e-02,  4.3263e-02,\n",
      "          3.2726e-02,  1.9825e-02, -1.0034e-02, -2.1873e-02,  1.3308e-02,\n",
      "          3.9475e-02, -1.2140e-02,  8.5712e-02,  4.2187e-02, -2.3477e-02,\n",
      "         -1.9578e-02,  3.7402e-02,  1.4542e-03,  3.4245e-02, -1.5425e-02,\n",
      "         -2.8997e-03, -3.4361e-02,  3.4428e-02, -3.5852e-02,  1.1834e-02,\n",
      "         -3.3657e-03, -3.2083e-04, -5.2636e-02,  3.2873e-02, -6.1226e-04,\n",
      "          2.0334e-02,  3.3753e-02,  5.1543e-02,  4.3593e-03, -4.9230e-02,\n",
      "         -1.4788e-02, -2.5702e-02, -9.4918e-04, -9.5153e-03,  9.0001e-03,\n",
      "          2.0273e-02, -2.8655e-02, -2.4118e-02,  2.3451e-02,  2.7414e-02,\n",
      "         -3.3920e-02,  5.2001e-02,  3.6443e-02, -6.0504e-03,  7.4200e-02,\n",
      "         -5.9112e-02, -5.3881e-02, -1.7870e-02,  7.4283e-03, -2.7031e-02,\n",
      "         -1.5785e-02, -6.3814e-02,  4.7019e-02,  1.4489e-02, -3.2238e-02,\n",
      "          5.6374e-03, -6.0349e-03,  6.9934e-03,  2.6128e-02,  1.4643e-02,\n",
      "          1.8811e-03,  1.8494e-02, -9.8517e-04, -2.6079e-02, -2.9486e-02,\n",
      "         -7.7439e-02, -1.8391e-02, -4.0349e-02,  8.9021e-03,  2.3187e-02,\n",
      "         -5.5234e-02, -2.4531e-03,  7.9741e-03,  8.4657e-03,  4.5370e-03,\n",
      "         -2.5644e-02,  2.0494e-03, -3.9706e-02, -5.1241e-02,  1.2272e-02,\n",
      "         -8.8885e-03, -6.7159e-02,  5.1499e-02,  1.8548e-02, -5.7593e-02,\n",
      "          4.5964e-02,  2.3069e-02, -1.5816e-02, -1.7899e-02, -2.6860e-02,\n",
      "          3.9363e-02,  4.0149e-02,  3.6070e-03,  9.7411e-03, -8.0640e-02,\n",
      "          5.5356e-02,  1.6593e-02, -3.2203e-03,  7.8833e-02,  4.0068e-02,\n",
      "          4.2472e-02, -1.1705e-02,  7.8580e-02,  3.7629e-02, -6.7491e-02,\n",
      "          1.6742e-02, -1.4671e-02,  4.7917e-02, -2.8045e-02, -2.8992e-02,\n",
      "          1.3161e-03, -6.2237e-03, -1.4460e-02, -1.6555e-03,  1.5189e-02,\n",
      "          1.8578e-02, -3.5313e-02, -9.5090e-05,  7.6177e-02,  2.5247e-03,\n",
      "         -3.2401e-02,  2.0006e-02,  1.6701e-02, -4.7967e-02, -4.5037e-02,\n",
      "          2.0186e-02, -3.6078e-03, -8.5892e-02, -3.5775e-02,  4.8689e-02,\n",
      "          4.3603e-03,  6.5434e-02,  1.7247e-02, -1.3526e-02,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[ 0.0000,  0.0011, -0.0009,  ..., -0.0045, -0.0087, -0.0035],\n",
      "         [ 0.0049,  0.0010, -0.0017,  ..., -0.0043, -0.0081, -0.0045],\n",
      "         [ 0.0045, -0.0001, -0.0016,  ..., -0.0028, -0.0049, -0.0042]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>), tensor([[[-0.0153, -0.0023,  0.0193,  ...,  0.0028,  0.0051, -0.0221],\n",
      "         [-0.0099, -0.0065,  0.0105,  ...,  0.0071,  0.0007, -0.0204],\n",
      "         [-0.0105, -0.0043,  0.0096,  ...,  0.0066,  0.0005, -0.0227]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tRUN backward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? True\n",
      "\t\t\t\tOh, then create full_batch_previous memory and state by (3, 4096) tensor filling 0.\n",
      "\t\t\t\tSet current_length_index... is it forward?? False\n",
      "\t\t\t\tOops, backward!! current_length_index = 0\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "binary_mask = tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [0.0000, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 0.0000, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 9 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 4.0999e-03, -6.1420e-03, -4.0220e-03, -4.2812e-02,  2.2545e-03,\n",
      "         -5.5843e-02, -3.3867e-03, -2.4588e-02, -5.0965e-02,  5.2278e-03,\n",
      "          3.2208e-03, -6.6673e-02, -4.4035e-03, -3.9993e-02, -1.1946e-02,\n",
      "          5.1611e-02,  9.1874e-03, -3.4062e-03, -6.2161e-02, -1.2382e-02,\n",
      "          1.7805e-02,  3.6729e-03,  4.2529e-03,  1.7206e-02, -1.2278e-02,\n",
      "          1.4176e-02,  6.9620e-02,  5.3863e-02,  2.9146e-02,  1.3687e-04,\n",
      "          4.2660e-03,  2.6004e-02, -2.9728e-04,  2.5874e-02,  5.3049e-04,\n",
      "         -9.1188e-03, -2.0923e-03, -3.1675e-02, -6.1669e-02,  2.7857e-02,\n",
      "          3.7919e-02,  4.7040e-02, -2.9004e-02,  2.8061e-02,  2.2440e-02,\n",
      "         -1.7007e-02,  4.6762e-02, -9.9050e-03, -6.0391e-02,  4.4008e-02,\n",
      "          2.7127e-02, -1.0071e-02, -4.4653e-02, -3.4113e-02, -5.4936e-02,\n",
      "         -2.7576e-02, -1.1772e-02,  1.6111e-02, -2.4791e-02, -1.2392e-02,\n",
      "         -4.5851e-02, -7.0682e-02,  6.8990e-02,  1.5804e-02, -1.8523e-02,\n",
      "          1.7020e-02,  3.7107e-03, -3.7832e-02, -9.7512e-03,  2.5285e-02,\n",
      "         -1.8639e-02, -1.1800e-02, -3.4856e-02,  2.3652e-03, -4.5387e-02,\n",
      "         -2.9169e-02, -7.3532e-03, -5.6582e-02, -4.0548e-02,  2.6095e-02,\n",
      "         -4.1448e-03, -7.7175e-03, -4.9508e-03,  5.8557e-02,  7.4085e-03,\n",
      "          2.0493e-02, -2.6433e-02, -5.9150e-02, -8.5825e-02, -3.6332e-02,\n",
      "         -6.7253e-02, -1.8667e-02,  1.9783e-02,  4.1228e-02,  3.5272e-02,\n",
      "         -1.5310e-02,  2.6563e-02,  9.0099e-03, -5.8591e-02, -1.2875e-02,\n",
      "         -4.5589e-02,  3.7654e-02,  5.1700e-02, -9.7946e-03, -2.8365e-02,\n",
      "          3.8642e-02,  3.2772e-02, -7.0115e-03,  1.7229e-02,  4.8634e-02,\n",
      "         -4.3900e-02, -5.7408e-02,  4.0625e-02,  4.4493e-02, -5.2082e-02,\n",
      "         -7.4520e-02, -5.9377e-02, -6.7939e-02, -7.3847e-02,  1.3183e-02,\n",
      "         -4.9373e-03,  1.8830e-02,  9.7497e-02,  1.3256e-02,  1.6883e-03,\n",
      "          1.4859e-02, -2.3568e-03,  5.6624e-02,  5.3152e-02, -1.2295e-02,\n",
      "          1.2090e-02, -1.3728e-05,  1.0096e-02,  3.3864e-02,  4.2704e-02,\n",
      "          5.4648e-02,  4.1818e-03,  3.7166e-03, -1.5428e-02,  4.5995e-02,\n",
      "          2.1937e-02, -2.8221e-02,  1.6410e-02, -4.7979e-04,  4.1428e-02,\n",
      "         -2.7716e-02,  1.6255e-03, -6.3316e-02, -1.8258e-02, -4.5491e-02,\n",
      "          1.1397e-02,  7.0264e-02,  1.8543e-02, -4.3941e-02,  1.5809e-02,\n",
      "          1.0576e-02, -1.1876e-02,  1.1684e-02, -1.4857e-03, -1.3847e-02,\n",
      "         -3.5052e-03, -2.9399e-02, -3.4140e-02, -3.0366e-02, -4.2807e-02,\n",
      "          2.2627e-02,  9.7901e-06, -3.6235e-02, -5.8573e-03,  2.0378e-02,\n",
      "         -6.4172e-02,  3.1886e-02,  1.5496e-02, -2.8181e-02,  3.5096e-02,\n",
      "         -6.8952e-03, -2.5324e-02, -3.1501e-02,  7.9062e-03, -2.8681e-02,\n",
      "         -7.9037e-03,  3.5255e-02,  2.4252e-02,  2.9042e-02, -6.2465e-02,\n",
      "          1.4517e-02, -1.9319e-02, -2.7564e-02, -2.5307e-02,  3.7366e-02,\n",
      "         -3.2373e-02, -6.7099e-02,  1.8414e-02,  2.4672e-02,  1.2600e-02,\n",
      "         -5.6666e-03,  2.5549e-03,  4.0709e-03,  1.9000e-02, -6.1543e-02,\n",
      "          1.2990e-02, -2.5736e-02,  1.1150e-02, -2.5157e-02, -8.1259e-02,\n",
      "          2.5197e-02,  1.6732e-03,  2.1115e-02, -4.1658e-02,  6.6255e-02,\n",
      "         -2.5587e-02, -3.2157e-04,  1.2388e-02,  4.0023e-02, -6.1048e-02,\n",
      "          6.0717e-02,  2.1476e-02,  5.8257e-03,  4.3190e-02, -5.3267e-03,\n",
      "         -1.1518e-01,  5.6670e-03, -2.2334e-02, -2.0324e-02,  1.9542e-03,\n",
      "         -3.7446e-02,  1.5559e-02, -7.3694e-03,  2.4515e-02, -4.5967e-02,\n",
      "         -3.7894e-03,  3.2286e-02,  1.0811e-02,  4.4440e-03,  4.8463e-03,\n",
      "          2.5042e-02,  4.7359e-02, -6.8103e-02, -6.5093e-03,  7.0785e-02,\n",
      "         -3.5528e-02,  4.7189e-03, -1.0189e-02,  4.2352e-02, -2.0553e-02,\n",
      "         -1.7234e-02,  1.3394e-02,  1.8244e-02,  6.2473e-02, -4.8055e-02,\n",
      "          1.2645e-02, -7.0589e-03,  5.6752e-02, -3.4373e-02, -3.1496e-02,\n",
      "         -5.9777e-03, -2.4881e-02,  7.9201e-02,  1.3747e-02, -2.5393e-03,\n",
      "         -3.2503e-02, -3.1110e-02,  5.7813e-02, -4.5621e-03, -1.0531e-02,\n",
      "          4.2530e-02,  3.0264e-02, -6.1265e-03,  4.5427e-02, -5.1061e-02,\n",
      "         -1.5275e-01,  1.0616e-02,  2.6280e-02,  1.8947e-02,  3.2359e-02,\n",
      "         -1.4984e-02,  3.7450e-03, -6.5668e-02,  2.4765e-03, -3.2025e-02,\n",
      "          2.9947e-02,  4.5328e-03,  3.0595e-02, -5.2844e-03,  7.7454e-02,\n",
      "         -1.9671e-02,  2.9783e-02, -3.6750e-02, -1.6449e-02,  6.7979e-03,\n",
      "          6.7633e-03, -3.4163e-02,  2.9027e-02, -1.8871e-02,  3.0973e-02,\n",
      "         -3.5600e-02, -2.3323e-02, -2.5930e-02, -4.0542e-02,  2.5499e-02,\n",
      "          3.3001e-02,  1.2359e-02,  2.5706e-02,  3.0320e-02,  4.6401e-02,\n",
      "         -2.3879e-02,  2.0202e-02, -2.2375e-03,  8.2037e-03, -2.0483e-02,\n",
      "         -7.8328e-02, -1.1248e-02, -9.1449e-03,  7.5573e-03, -2.2487e-02,\n",
      "         -1.6329e-02,  1.8263e-02,  3.7522e-02,  1.9167e-02,  1.4377e-02,\n",
      "         -1.4204e-02,  3.2154e-02, -4.2945e-02,  5.7657e-03, -8.0327e-04,\n",
      "         -6.6083e-02, -2.8194e-04, -1.8378e-02, -4.8475e-02,  2.3617e-02,\n",
      "         -3.3750e-02,  1.1834e-02, -1.0890e-02,  2.5747e-02, -3.7289e-02,\n",
      "          5.2489e-02,  3.3673e-02,  2.8601e-02,  4.2754e-03, -4.7038e-02,\n",
      "         -3.0422e-02, -1.7425e-02,  1.3590e-02, -5.1289e-02, -2.6229e-02,\n",
      "         -1.0611e-02, -7.7437e-03,  2.7641e-02, -4.1801e-03,  2.6006e-02,\n",
      "         -3.3880e-03,  3.3067e-03, -5.1195e-02,  1.0351e-02, -5.3868e-03,\n",
      "         -3.0990e-02,  2.9896e-02, -2.5370e-02,  7.7419e-02, -1.1452e-02,\n",
      "         -2.1355e-02,  2.0652e-02,  5.2692e-02,  3.8496e-02, -5.2882e-02,\n",
      "          2.0792e-03, -8.6128e-02,  3.2290e-02, -1.0719e-02,  1.2896e-02,\n",
      "          3.0134e-02,  1.5601e-02,  3.8173e-02,  3.0548e-02,  4.3263e-02,\n",
      "          3.2726e-02,  1.9825e-02, -1.0034e-02, -2.1873e-02,  1.3308e-02,\n",
      "          3.9475e-02, -1.2140e-02,  8.5712e-02,  4.2187e-02, -2.3477e-02,\n",
      "         -1.9578e-02,  3.7402e-02,  1.4542e-03,  3.4245e-02, -1.5425e-02,\n",
      "         -2.8997e-03, -3.4361e-02,  3.4428e-02, -3.5852e-02,  1.1834e-02,\n",
      "         -3.3657e-03, -3.2083e-04, -5.2636e-02,  3.2873e-02, -6.1226e-04,\n",
      "          2.0334e-02,  3.3753e-02,  5.1543e-02,  4.3593e-03, -4.9230e-02,\n",
      "         -1.4788e-02, -2.5702e-02, -9.4918e-04, -9.5153e-03,  9.0001e-03,\n",
      "          2.0273e-02, -2.8655e-02, -2.4118e-02,  2.3451e-02,  2.7414e-02,\n",
      "         -3.3920e-02,  5.2001e-02,  3.6443e-02, -6.0504e-03,  7.4200e-02,\n",
      "         -5.9112e-02, -5.3881e-02, -1.7870e-02,  7.4283e-03, -2.7031e-02,\n",
      "         -1.5785e-02, -6.3814e-02,  4.7019e-02,  1.4489e-02, -3.2238e-02,\n",
      "          5.6374e-03, -6.0349e-03,  6.9934e-03,  2.6128e-02,  1.4643e-02,\n",
      "          1.8811e-03,  1.8494e-02, -9.8517e-04, -2.6079e-02, -2.9486e-02,\n",
      "         -7.7439e-02, -1.8391e-02, -4.0349e-02,  8.9021e-03,  2.3187e-02,\n",
      "         -5.5234e-02, -2.4531e-03,  7.9741e-03,  8.4657e-03,  4.5370e-03,\n",
      "         -2.5644e-02,  2.0494e-03, -3.9706e-02, -5.1241e-02,  1.2272e-02,\n",
      "         -8.8885e-03, -6.7159e-02,  5.1499e-02,  1.8548e-02, -5.7593e-02,\n",
      "          4.5964e-02,  2.3069e-02, -1.5816e-02, -1.7899e-02, -2.6860e-02,\n",
      "          3.9363e-02,  4.0149e-02,  3.6070e-03,  9.7411e-03, -8.0640e-02,\n",
      "          5.5356e-02,  1.6593e-02, -3.2203e-03,  7.8833e-02,  4.0068e-02,\n",
      "          4.2472e-02, -1.1705e-02,  7.8580e-02,  3.7629e-02, -6.7491e-02,\n",
      "          1.6742e-02, -1.4671e-02,  4.7917e-02, -2.8045e-02, -2.8992e-02,\n",
      "          1.3161e-03, -6.2237e-03, -1.4460e-02, -1.6555e-03,  1.5189e-02,\n",
      "          1.8578e-02, -3.5313e-02, -9.5090e-05,  7.6177e-02,  2.5247e-03,\n",
      "         -3.2401e-02,  2.0006e-02,  1.6701e-02, -4.7967e-02, -4.5037e-02,\n",
      "          2.0186e-02, -3.6078e-03, -8.5892e-02, -3.5775e-02,  4.8689e-02,\n",
      "          4.3603e-03,  6.5434e-02,  1.7247e-02, -1.3526e-02,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0046,  0.0048,  0.0051,  ...,  0.0045, -0.0217, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0046,  0.0048,  0.0051,  ...,  0.0045, -0.0217, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0225,  0.0021, -0.0073, -0.0453,  0.0067, -0.1035,  0.0007, -0.0384,\n",
      "         -0.0241, -0.0236, -0.0056, -0.0469,  0.0058, -0.0028,  0.0082,  0.0394,\n",
      "          0.0411,  0.0036, -0.0933, -0.0109,  0.0350, -0.0014,  0.0195,  0.0090,\n",
      "          0.0102,  0.0144,  0.0910,  0.0898,  0.0680, -0.0267,  0.0296,  0.0123,\n",
      "         -0.0376,  0.0462,  0.0312, -0.0027,  0.0219, -0.0156, -0.0906, -0.0108,\n",
      "          0.0491, -0.0022, -0.0330,  0.0454,  0.0668, -0.0107,  0.0695, -0.0285,\n",
      "         -0.1164,  0.0746,  0.0207,  0.0046, -0.0736, -0.0499, -0.0580, -0.0040,\n",
      "          0.0032,  0.0278,  0.0149, -0.0144,  0.0139, -0.0536,  0.0244,  0.0268,\n",
      "         -0.0166, -0.0146, -0.0104, -0.0225, -0.0341,  0.0614, -0.0234, -0.0311,\n",
      "         -0.0392,  0.0299, -0.0474, -0.0211, -0.0228, -0.0282, -0.0288, -0.0021,\n",
      "         -0.0283, -0.0399,  0.0053,  0.0269, -0.0246,  0.0394, -0.0482, -0.0131,\n",
      "         -0.0672, -0.0286, -0.0382, -0.0050, -0.0136,  0.0675,  0.0515,  0.0383,\n",
      "          0.0347, -0.0290, -0.0956,  0.0055, -0.0542,  0.0374,  0.0811, -0.0625,\n",
      "          0.0048,  0.0439,  0.0274, -0.0130,  0.0037,  0.0702, -0.0189, -0.0653,\n",
      "          0.0338,  0.0484, -0.0758, -0.1118, -0.0982, -0.0586, -0.1017,  0.0323,\n",
      "         -0.0187,  0.0306,  0.0579,  0.0252, -0.0435,  0.0251, -0.0279,  0.0555,\n",
      "          0.0822,  0.0015,  0.0172,  0.0274, -0.0081,  0.0368,  0.0218,  0.0329,\n",
      "          0.0185,  0.0088, -0.0144,  0.0301,  0.0175, -0.0075,  0.0198,  0.0053,\n",
      "          0.0708,  0.0166, -0.0065, -0.0641, -0.0199, -0.0439,  0.0324,  0.0846,\n",
      "         -0.0156, -0.0623,  0.0368,  0.0376, -0.0219, -0.0321,  0.0116, -0.0214,\n",
      "         -0.0275, -0.0293, -0.0180, -0.0346, -0.0491,  0.0416, -0.0381, -0.0820,\n",
      "         -0.0298,  0.0151, -0.0111,  0.0488,  0.0482, -0.0697,  0.0230, -0.0346,\n",
      "         -0.0385, -0.0320,  0.0022, -0.0195,  0.0382,  0.0669,  0.0117, -0.0143,\n",
      "         -0.0986,  0.0230, -0.0114,  0.0136, -0.0391,  0.0525, -0.0245, -0.0297,\n",
      "         -0.0252,  0.0744,  0.0637,  0.0055,  0.0261,  0.0363, -0.0025, -0.0683,\n",
      "         -0.0255, -0.0456,  0.0402, -0.0391, -0.0926,  0.0581, -0.0183, -0.0362,\n",
      "         -0.0535,  0.0409, -0.0179,  0.0289,  0.0655,  0.0294, -0.0775,  0.0396,\n",
      "          0.0239,  0.0403,  0.0801, -0.0544, -0.1100, -0.0068, -0.0387, -0.0217,\n",
      "          0.0214, -0.0616, -0.0089, -0.0123,  0.0534, -0.0370,  0.0211,  0.0167,\n",
      "         -0.0111, -0.0011,  0.0025,  0.0404,  0.0642, -0.0452, -0.0113,  0.0538,\n",
      "         -0.0520, -0.0056,  0.0366,  0.0759, -0.0529, -0.0374,  0.0588,  0.0106,\n",
      "          0.1057, -0.0457, -0.0078,  0.0133,  0.0720, -0.0095, -0.0401,  0.0218,\n",
      "         -0.0456,  0.0502,  0.0087, -0.0067, -0.0664, -0.0569,  0.0319,  0.0456,\n",
      "          0.0117,  0.0440,  0.0148, -0.0104,  0.0672, -0.0583, -0.1598,  0.0023,\n",
      "          0.0538, -0.0143,  0.0224,  0.0161, -0.0428, -0.0923, -0.0291, -0.0308,\n",
      "         -0.0071, -0.0245,  0.0439, -0.0270,  0.0261, -0.0623,  0.0007, -0.0537,\n",
      "         -0.0007, -0.0388,  0.0227, -0.0308,  0.0428, -0.0017,  0.0260, -0.0640,\n",
      "         -0.0064, -0.0349, -0.0219,  0.0544,  0.0239,  0.0471,  0.0232,  0.0261,\n",
      "          0.0282, -0.0696,  0.0527,  0.0118,  0.0263, -0.0006, -0.0661,  0.0209,\n",
      "          0.0058,  0.0030, -0.0387,  0.0064,  0.0166,  0.0501, -0.0120,  0.0489,\n",
      "          0.0016,  0.0325, -0.0278,  0.0218,  0.0103, -0.0845, -0.0082, -0.0481,\n",
      "         -0.0449,  0.0369, -0.0878, -0.0075, -0.0770,  0.0889, -0.0595,  0.0261,\n",
      "          0.0722,  0.0245, -0.0125, -0.0249, -0.0267, -0.0498, -0.0126, -0.0515,\n",
      "         -0.0232, -0.0248, -0.0696,  0.0107, -0.0039,  0.0008,  0.0189,  0.0703,\n",
      "         -0.0633,  0.0129,  0.0091, -0.0232,  0.0251, -0.0279,  0.1155, -0.0159,\n",
      "         -0.0343,  0.0638, -0.0056,  0.0369, -0.0636, -0.0179, -0.1186,  0.0721,\n",
      "         -0.0307,  0.0792,  0.0270,  0.0300,  0.0747,  0.0376,  0.0251,  0.0128,\n",
      "          0.0466, -0.0187, -0.0412,  0.0287,  0.0269, -0.0115,  0.0786,  0.0066,\n",
      "         -0.0067,  0.0407,  0.0497,  0.0495,  0.0253, -0.0395, -0.0038, -0.0613,\n",
      "          0.0059, -0.0225,  0.0075,  0.0116,  0.0301, -0.0510,  0.0483, -0.0205,\n",
      "          0.0458,  0.0350,  0.0831, -0.0208, -0.0651,  0.0033, -0.0349,  0.0098,\n",
      "         -0.0437,  0.0113, -0.0011, -0.0237, -0.0102,  0.0026,  0.0296, -0.0294,\n",
      "          0.0504,  0.0353, -0.0013,  0.0868, -0.0338, -0.1060, -0.0469, -0.0076,\n",
      "         -0.0594, -0.0139, -0.1025,  0.0495,  0.0576, -0.0711,  0.0156, -0.0270,\n",
      "          0.0126,  0.0289,  0.0422, -0.0147, -0.0343,  0.0242, -0.0238,  0.0042,\n",
      "         -0.0163, -0.0254, -0.0318, -0.0033, -0.0148, -0.0554, -0.0152,  0.0177,\n",
      "         -0.0006,  0.0078,  0.0026, -0.0279, -0.0466, -0.0133, -0.0026,  0.0167,\n",
      "         -0.0890,  0.0875, -0.0195, -0.0232,  0.0426, -0.0253, -0.0016, -0.0328,\n",
      "          0.0101,  0.0372,  0.0486, -0.0026, -0.0226, -0.0536,  0.0217,  0.0102,\n",
      "         -0.0430,  0.0425,  0.0180, -0.0253,  0.0030,  0.0580,  0.0355, -0.0872,\n",
      "          0.0397, -0.0145,  0.0678,  0.0079, -0.0342,  0.0471,  0.0262,  0.0146,\n",
      "         -0.0287, -0.0009, -0.0416,  0.0084, -0.0227,  0.0526, -0.0619, -0.0201,\n",
      "          0.0613,  0.0342, -0.0878, -0.0522,  0.0243, -0.0385, -0.0821, -0.0479,\n",
      "          0.0745, -0.0005,  0.0558,  0.0333, -0.0184,  0.0317,  0.0303, -0.0447]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0033,  0.0014,  0.0092,  ...,  0.0101, -0.0346, -0.0023]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0033,  0.0014,  0.0092,  ...,  0.0101, -0.0346, -0.0023]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.9943e-02, -6.3473e-04, -2.4333e-02, -2.6465e-02,  3.6282e-02,\n",
      "         -8.1245e-02,  2.0234e-02, -4.7533e-02, -1.4706e-02,  1.8934e-02,\n",
      "          4.9761e-04, -6.6569e-02,  3.0510e-02,  8.3242e-03,  3.4185e-03,\n",
      "          4.2514e-02,  3.6875e-02,  1.1155e-02, -6.4569e-02, -2.6975e-02,\n",
      "          1.4409e-02, -8.3901e-03,  1.4958e-02,  1.4861e-02,  1.6609e-02,\n",
      "          4.6618e-02,  8.1440e-02,  6.3781e-02,  4.3447e-02,  5.9358e-03,\n",
      "         -5.9265e-04,  3.1957e-02, -3.8824e-03,  6.6324e-02,  1.1476e-03,\n",
      "          3.0135e-02,  2.8017e-02, -2.6136e-03, -8.7879e-02,  8.0971e-03,\n",
      "          4.2865e-02,  2.4699e-02, -1.6333e-03,  4.5484e-02,  4.8548e-02,\n",
      "         -2.7261e-02,  4.9371e-02, -1.1094e-02, -6.2017e-02,  6.3433e-02,\n",
      "          1.1362e-02, -3.7070e-02, -2.8188e-02, -3.8195e-02, -5.8539e-02,\n",
      "          1.2449e-02, -2.7576e-03,  3.2095e-02,  7.0868e-03,  7.0801e-03,\n",
      "         -2.9167e-02, -6.1884e-02,  3.3293e-02,  7.4093e-03,  8.6460e-03,\n",
      "          2.1394e-02,  2.8104e-02, -2.3261e-02, -4.2063e-02,  2.1559e-02,\n",
      "         -7.4343e-03, -4.5396e-02, -1.7797e-02, -8.0800e-03, -5.4489e-02,\n",
      "         -7.8084e-03,  5.2327e-03, -1.9146e-02, -4.8832e-02,  2.1798e-02,\n",
      "          9.2141e-03, -3.6777e-02,  6.6118e-03,  3.2340e-02,  1.9615e-02,\n",
      "          2.1029e-02, -2.2645e-02, -2.0520e-02, -8.7323e-02, -4.5272e-02,\n",
      "         -2.7091e-02, -2.3780e-02, -2.8575e-02,  7.5322e-02,  2.6836e-02,\n",
      "         -1.4969e-02,  1.4021e-02,  3.6706e-03, -6.0353e-02, -9.9927e-03,\n",
      "         -7.4608e-02, -6.0827e-03,  4.9744e-02, -1.1646e-02, -1.5283e-02,\n",
      "          3.9851e-02,  1.9263e-02, -2.9827e-02,  3.6977e-02,  4.5320e-02,\n",
      "         -4.4115e-02, -5.5769e-02,  1.1291e-02,  3.4049e-02, -4.0742e-02,\n",
      "         -7.8004e-02, -3.9623e-02, -6.5474e-02, -5.7309e-02,  3.2225e-03,\n",
      "          4.3883e-03,  2.9382e-02,  7.0717e-02,  6.7704e-02, -2.4883e-02,\n",
      "          2.6200e-02, -1.4513e-02,  2.5271e-02,  3.6185e-02, -6.2445e-03,\n",
      "         -1.5884e-02, -5.3081e-03, -5.1873e-03,  4.8760e-02,  4.0506e-02,\n",
      "          4.3785e-02,  1.0459e-02,  5.6056e-02, -4.2227e-03,  4.8874e-02,\n",
      "         -2.1245e-02, -3.1359e-03,  1.8048e-02, -2.3473e-02,  5.5935e-02,\n",
      "         -3.8959e-02,  1.0918e-02, -8.7178e-02,  3.0119e-03, -2.1534e-02,\n",
      "          3.2873e-03,  7.5276e-02, -3.9145e-02, -2.3037e-02,  1.6807e-02,\n",
      "          5.1875e-02, -6.6744e-05, -2.1719e-02,  3.4947e-02, -1.9906e-02,\n",
      "         -4.2533e-02, -1.4287e-02, -1.7697e-02, -3.5453e-02, -2.6380e-02,\n",
      "          3.0872e-03, -3.2254e-02, -5.6165e-02, -1.4399e-02,  6.4918e-04,\n",
      "         -4.0651e-02,  5.3044e-02,  5.6464e-02, -7.1317e-02,  2.5132e-02,\n",
      "         -2.7497e-02, -1.5007e-02, -4.0442e-02,  3.2662e-02, -3.9329e-02,\n",
      "          1.1346e-02,  5.2525e-02,  2.5804e-02,  2.6124e-02, -8.0331e-02,\n",
      "          1.4954e-02,  2.4630e-02,  1.5274e-02, -4.8547e-02,  3.7759e-02,\n",
      "         -2.4851e-02, -6.2086e-02,  1.5069e-02,  5.3204e-02,  1.5821e-02,\n",
      "         -2.3344e-02, -1.3930e-03,  1.4708e-02,  1.1591e-02, -7.3345e-02,\n",
      "         -3.6105e-02, -2.8722e-02,  7.6852e-04, -2.2699e-02, -8.3008e-02,\n",
      "          7.0183e-02, -3.0014e-02, -9.3609e-03, -2.8232e-02,  6.1581e-02,\n",
      "         -2.4002e-02,  1.5176e-02,  5.0153e-02, -9.7399e-04, -5.9783e-02,\n",
      "          3.4617e-02, -2.9192e-03,  2.7485e-02,  6.9439e-02, -1.1364e-02,\n",
      "         -1.3452e-01,  2.2476e-02, -1.4151e-02, -7.1442e-03, -1.2852e-02,\n",
      "         -2.2974e-02,  2.5305e-02,  9.9605e-03,  2.9039e-03, -3.2136e-02,\n",
      "          1.0917e-02,  3.9640e-02, -1.0201e-02, -1.9363e-02, -1.1269e-02,\n",
      "          9.5938e-03,  5.2384e-02, -5.6701e-02, -8.5196e-03,  1.2629e-02,\n",
      "         -4.5141e-02, -2.6792e-02, -1.0696e-02,  2.9739e-02, -5.2543e-02,\n",
      "          4.4625e-03,  3.8448e-02, -1.4611e-03,  7.8076e-02, -1.0361e-02,\n",
      "         -1.6192e-02, -7.5027e-03,  5.8933e-02, -8.2104e-03, -1.0303e-02,\n",
      "         -1.0491e-02, -6.1503e-02,  3.2754e-02, -1.5981e-02, -2.7549e-02,\n",
      "         -5.1394e-02, -3.0974e-02,  4.4842e-02,  1.3567e-02, -4.8374e-03,\n",
      "          4.6250e-02,  4.9839e-02, -1.7988e-02,  3.7572e-02, -4.8327e-02,\n",
      "         -1.2389e-01,  9.6038e-03,  2.2187e-02,  1.4403e-02,  4.4363e-03,\n",
      "         -7.8948e-03, -3.4436e-02, -5.8862e-02, -3.2297e-02,  2.4687e-03,\n",
      "          1.3516e-02, -1.9046e-02,  6.2428e-02, -2.4486e-02,  2.5915e-02,\n",
      "         -4.7463e-02,  4.1579e-02, -1.3121e-02, -2.5279e-02, -1.6883e-02,\n",
      "         -1.2437e-02, -6.8095e-02,  3.2792e-02, -3.9081e-02,  2.3718e-02,\n",
      "         -7.6230e-03,  2.5899e-02, -9.0227e-03, -4.4538e-02,  1.8885e-02,\n",
      "          6.7445e-03,  2.3967e-02,  3.6491e-02,  6.1667e-02,  2.9088e-02,\n",
      "         -4.6740e-02,  4.2268e-02,  5.3146e-03,  3.4716e-03,  1.3579e-02,\n",
      "         -3.4293e-02, -7.6553e-03, -3.0861e-02,  2.1517e-02, -1.5814e-02,\n",
      "          1.6505e-02,  2.4900e-02,  3.6272e-02, -1.1502e-02,  2.1404e-02,\n",
      "         -1.9012e-02,  4.1999e-02, -2.3130e-02, -1.2259e-03, -3.5337e-02,\n",
      "         -6.3823e-02, -1.7515e-02, -4.7187e-02, -2.1176e-02,  2.9516e-02,\n",
      "         -5.6702e-02, -3.0155e-02, -4.3931e-02,  5.9581e-02, -4.8076e-02,\n",
      "          3.2992e-02,  5.8642e-02,  4.0638e-02, -4.9440e-03, -2.6222e-02,\n",
      "         -1.7771e-02, -4.0846e-02,  2.8135e-02, -4.8641e-02, -3.6589e-02,\n",
      "         -4.3364e-03, -4.0931e-02,  1.2174e-02, -2.9099e-02, -3.0528e-02,\n",
      "          1.0307e-02,  6.1238e-02, -5.8316e-02, -1.3105e-02,  9.0719e-03,\n",
      "         -1.7113e-02, -1.6299e-02, -4.1770e-02,  5.2599e-02, -2.1581e-02,\n",
      "          1.3731e-02,  3.1022e-02, -9.9482e-03,  2.3787e-02, -7.2789e-02,\n",
      "          3.9918e-03, -1.1121e-01,  6.5336e-02, -3.4317e-02,  6.3555e-02,\n",
      "          4.7264e-02, -2.1690e-02,  3.2510e-02, -2.9224e-03,  8.1941e-03,\n",
      "          2.2115e-03,  4.4054e-02, -2.8647e-02, -1.7062e-02,  3.1700e-02,\n",
      "          1.3082e-02, -7.8945e-03,  5.9706e-02, -3.6144e-03,  1.2505e-02,\n",
      "          3.5541e-02,  5.5393e-02,  7.4438e-03,  3.9664e-02, -2.1253e-02,\n",
      "         -6.8311e-03, -4.3957e-02,  3.7919e-02, -2.8258e-02, -8.0768e-03,\n",
      "         -4.2647e-02,  3.2717e-02, -2.9899e-02, -9.3153e-03,  1.5475e-02,\n",
      "          3.0740e-02,  1.8109e-02,  2.8607e-02, -3.8829e-02, -4.7982e-02,\n",
      "         -7.1243e-03, -2.4547e-02,  8.3619e-04, -1.8965e-02,  1.3804e-02,\n",
      "          1.0712e-02, -3.4088e-02, -7.9899e-02, -7.0846e-03,  6.9593e-03,\n",
      "         -2.1685e-02,  5.5655e-02, -1.2060e-02,  2.3431e-03,  7.2531e-02,\n",
      "         -6.0682e-02, -7.4575e-02, -1.7317e-02,  5.8419e-03, -2.3470e-02,\n",
      "          9.9523e-03, -6.2580e-02,  7.6685e-02,  2.9135e-02, -3.8954e-02,\n",
      "          1.4604e-02,  3.1968e-03,  3.2509e-02, -2.8317e-03,  2.2350e-02,\n",
      "         -4.7101e-02, -5.0960e-02,  3.1412e-03, -1.5378e-02, -2.6794e-02,\n",
      "         -1.9578e-02,  1.0552e-02, -6.9772e-02,  2.0636e-02, -2.9148e-03,\n",
      "         -4.0716e-02,  2.5176e-03,  7.4695e-03, -3.4024e-04,  3.2252e-02,\n",
      "         -2.8218e-02, -1.9674e-02, -2.1514e-02, -9.1328e-03,  2.0247e-02,\n",
      "          3.0113e-02, -7.1527e-02,  4.6293e-02,  2.1005e-02, -2.0541e-02,\n",
      "          4.0748e-02,  4.6423e-03, -2.4288e-02, -1.5478e-02, -3.5585e-02,\n",
      "          2.1604e-02,  3.8710e-02,  1.4737e-02, -3.1023e-02, -5.9019e-02,\n",
      "          3.0318e-02,  3.5039e-02, -1.1337e-03,  3.8222e-02,  1.5961e-02,\n",
      "          2.4988e-02,  1.0752e-02,  7.9247e-02,  3.4722e-02, -7.4136e-02,\n",
      "          3.9050e-02, -1.8820e-02,  6.0536e-02,  6.0405e-03, -1.8136e-02,\n",
      "          4.7733e-02,  4.5106e-03, -5.9822e-03, -1.5836e-02,  1.4583e-03,\n",
      "         -2.1234e-02, -2.3092e-02, -6.0511e-03,  7.6471e-02, -1.0707e-02,\n",
      "         -3.3524e-02,  3.2486e-02, -7.2459e-03, -6.4545e-02, -4.4839e-02,\n",
      "          3.7363e-02, -2.0615e-02, -1.1821e-01, -2.0830e-02,  6.9055e-02,\n",
      "         -1.0918e-02,  5.4751e-02,  2.7437e-02,  3.1381e-02,  1.9435e-02,\n",
      "          5.1162e-02, -3.3509e-02]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0037,  0.0009,  0.0113,  ...,  0.0136, -0.0423, -0.0008],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0037,  0.0009,  0.0113,  ...,  0.0136, -0.0423, -0.0008],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 7.8088e-03, -2.3229e-02, -1.9114e-02,  ...,  2.5953e-02,\n",
      "          5.1098e-02, -5.5057e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0066,  0.0049,  0.0136,  ...,  0.0148, -0.0475, -0.0043],\n",
      "        [ 0.0046,  0.0048,  0.0051,  ...,  0.0045, -0.0217, -0.0039],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0066,  0.0049,  0.0136,  ...,  0.0148, -0.0475, -0.0043],\n",
      "        [ 0.0046,  0.0048,  0.0051,  ...,  0.0045, -0.0217, -0.0039],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-6.8748e-03, -1.3605e-02,  7.1700e-03,  ...,  1.2522e-02,\n",
      "          2.0355e-02, -2.4865e-02],\n",
      "        [-2.9532e-03, -1.3722e-02, -2.7846e-02,  ..., -3.4922e-03,\n",
      "          1.9039e-03, -3.1346e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0078,  0.0051,  0.0144,  ...,  0.0149, -0.0535, -0.0058],\n",
      "        [ 0.0028,  0.0020,  0.0093,  ...,  0.0060, -0.0352, -0.0048],\n",
      "        [ 0.0046,  0.0048,  0.0051,  ...,  0.0045, -0.0217, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0078,  0.0051,  0.0144,  ...,  0.0149, -0.0535, -0.0058],\n",
      "        [ 0.0028,  0.0020,  0.0093,  ...,  0.0060, -0.0352, -0.0048],\n",
      "        [ 0.0046,  0.0048,  0.0051,  ...,  0.0045, -0.0217, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0267,  0.0010, -0.0196,  ...,  0.0005,  0.0283, -0.0203],\n",
      "        [ 0.0260, -0.0238,  0.0250,  ...,  0.0017,  0.0297, -0.0235],\n",
      "        [-0.0157,  0.0032, -0.0359,  ...,  0.0232,  0.0049, -0.0217]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0092,  0.0055,  0.0167,  ...,  0.0168, -0.0539, -0.0092],\n",
      "        [-0.0005,  0.0014,  0.0076,  ...,  0.0101, -0.0452, -0.0049],\n",
      "        [ 0.0009,  0.0027,  0.0066,  ...,  0.0134, -0.0366, -0.0054]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0092,  0.0055,  0.0167,  ...,  0.0168, -0.0539, -0.0092],\n",
      "        [-0.0005,  0.0014,  0.0076,  ...,  0.0101, -0.0452, -0.0049],\n",
      "        [ 0.0009,  0.0027,  0.0066,  ...,  0.0134, -0.0366, -0.0054]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0195,  0.0024, -0.0233,  ...,  0.0120,  0.0189, -0.0226],\n",
      "        [ 0.0259, -0.0076, -0.0144,  ...,  0.0053,  0.0274, -0.0328],\n",
      "        [ 0.0088, -0.0010, -0.0169,  ...,  0.0276,  0.0261, -0.0068]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 8.9464e-03,  5.7961e-03,  1.6823e-02,  ...,  1.5124e-02,\n",
      "         -5.9164e-02, -9.2504e-03],\n",
      "        [-1.5914e-05,  8.1000e-03,  9.0710e-03,  ...,  1.1221e-02,\n",
      "         -5.4090e-02, -7.9934e-03],\n",
      "        [ 2.9502e-03,  6.5684e-03,  2.5448e-03,  ...,  1.1615e-02,\n",
      "         -4.1457e-02, -2.4653e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 8.9464e-03,  5.7961e-03,  1.6823e-02,  ...,  1.5124e-02,\n",
      "         -5.9164e-02, -9.2504e-03],\n",
      "        [-1.5914e-05,  8.1000e-03,  9.0710e-03,  ...,  1.1221e-02,\n",
      "         -5.4090e-02, -7.9934e-03],\n",
      "        [ 2.9502e-03,  6.5684e-03,  2.5448e-03,  ...,  1.1615e-02,\n",
      "         -4.1457e-02, -2.4653e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0439, -0.0200, -0.0223,  ...,  0.0114,  0.0445, -0.0566],\n",
      "        [-0.0055, -0.0275, -0.0182,  ...,  0.0095,  0.0294, -0.0096],\n",
      "        [ 0.0261, -0.0046, -0.0160,  ...,  0.0313,  0.0107, -0.0573]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0070,  0.0095,  0.0201,  ...,  0.0165, -0.0588, -0.0062],\n",
      "        [-0.0040,  0.0099,  0.0054,  ...,  0.0171, -0.0591, -0.0056],\n",
      "        [ 0.0027,  0.0010,  0.0008,  ...,  0.0125, -0.0562, -0.0032]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0070,  0.0095,  0.0201,  ...,  0.0165, -0.0588, -0.0062],\n",
      "        [-0.0040,  0.0099,  0.0054,  ...,  0.0171, -0.0591, -0.0056],\n",
      "        [ 0.0027,  0.0010,  0.0008,  ...,  0.0125, -0.0562, -0.0032]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0078, -0.0260, -0.0205,  ...,  0.0145,  0.0510, -0.0102],\n",
      "        [ 0.0290,  0.0047, -0.0227,  ...,  0.0162,  0.0292, -0.0161],\n",
      "        [ 0.0056, -0.0269, -0.0216,  ..., -0.0044,  0.0573, -0.0453]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 0 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0063,  0.0123,  0.0193,  ...,  0.0146, -0.0598, -0.0026],\n",
      "        [ 0.0031,  0.0101,  0.0101,  ...,  0.0135, -0.0573, -0.0067],\n",
      "        [ 0.0052,  0.0052,  0.0046,  ...,  0.0167, -0.0604, -0.0081]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0063,  0.0123,  0.0193,  ...,  0.0146, -0.0598, -0.0026],\n",
      "        [ 0.0031,  0.0101,  0.0101,  ...,  0.0135, -0.0573, -0.0067],\n",
      "        [ 0.0052,  0.0052,  0.0046,  ...,  0.0167, -0.0604, -0.0081]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[ 0.0005, -0.0015, -0.0009,  ...,  0.0021,  0.0017, -0.0105],\n",
      "         [ 0.0000,  0.0007, -0.0009,  ...,  0.0026,  0.0013, -0.0073],\n",
      "         [-0.0008,  0.0000, -0.0002,  ...,  0.0018,  0.0030, -0.0066]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>), tensor([[[ 0.0095,  0.0134,  0.0190,  ...,  0.0185, -0.0602, -0.0058],\n",
      "         [ 0.0069,  0.0114,  0.0114,  ...,  0.0188, -0.0591, -0.0084],\n",
      "         [ 0.0090,  0.0069,  0.0082,  ...,  0.0215, -0.0620, -0.0097]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tGet a forward layer and backward layer at layer 2\n",
      "\t\tCaching...: output_sequence to cache both forward and backward\n",
      "\t\tstate is None? True\n",
      "\t\t\tOops, then forward and backward state is also 'None'\n",
      "\t\tRUN forward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? True\n",
      "\t\t\t\tOh, then create full_batch_previous memory and state by (3, 4096) tensor filling 0.\n",
      "\t\t\t\tSet current_length_index... is it forward?? True\n",
      "\t\t\t\tOk, forward!! current_length_index = batch_size - 1 = 2\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "binary_mask = tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True, False,  True],\n",
      "        [ True,  True, False,  ...,  True,  True, False]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 0.0000,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 0 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000,  0.0018, -0.0013,  ..., -0.0011, -0.0022, -0.0007],\n",
      "        [ 0.0020,  0.0018, -0.0013,  ..., -0.0011, -0.0022, -0.0007],\n",
      "        [ 0.0020,  0.0018, -0.0013,  ..., -0.0011, -0.0022, -0.0007]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-2.8905e-04,  1.8467e-04,  6.2516e-05,  ...,  4.6534e-04,\n",
      "         -1.7624e-04, -2.2456e-04],\n",
      "        [-1.2510e-04,  1.5904e-04,  2.1280e-04,  ...,  3.8620e-04,\n",
      "         -1.5549e-04, -2.8255e-04],\n",
      "        [-8.6121e-05,  1.7063e-04, -7.7332e-05,  ...,  4.4087e-04,\n",
      "         -1.9687e-04, -3.2502e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-2.8905e-04,  1.8467e-04,  6.2516e-05,  ...,  4.6534e-04,\n",
      "         -1.7624e-04, -2.2456e-04],\n",
      "        [-1.2510e-04,  1.5904e-04,  2.1280e-04,  ...,  3.8620e-04,\n",
      "         -1.5549e-04, -2.8255e-04],\n",
      "        [-8.6121e-05,  1.7063e-04, -7.7332e-05,  ...,  4.4087e-04,\n",
      "         -1.9687e-04, -3.2502e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000,  0.0013, -0.0003,  ..., -0.0021, -0.0036, -0.0014],\n",
      "        [ 0.0030,  0.0009, -0.0011,  ..., -0.0010, -0.0031, -0.0021],\n",
      "        [ 0.0036,  0.0029, -0.0015,  ..., -0.0013, -0.0014, -0.0017]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-4.4272e-04,  3.0841e-04,  1.9257e-04,  ...,  8.5428e-04,\n",
      "          9.1085e-05, -5.4883e-04],\n",
      "        [ 1.2918e-04,  1.4960e-04,  3.6549e-04,  ...,  5.1102e-04,\n",
      "         -2.7080e-04, -6.0694e-04],\n",
      "        [ 2.3399e-04,  4.7917e-05, -9.3713e-05,  ...,  6.6387e-04,\n",
      "         -3.9998e-04, -2.8389e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-4.4272e-04,  3.0841e-04,  1.9257e-04,  ...,  8.5428e-04,\n",
      "          9.1085e-05, -5.4883e-04],\n",
      "        [ 1.2918e-04,  1.4960e-04,  3.6549e-04,  ...,  5.1102e-04,\n",
      "         -2.7080e-04, -6.0694e-04],\n",
      "        [ 2.3399e-04,  4.7917e-05, -9.3713e-05,  ...,  6.6387e-04,\n",
      "         -3.9998e-04, -2.8389e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000,  0.0012, -0.0025,  ..., -0.0014, -0.0040, -0.0039],\n",
      "        [ 0.0048,  0.0013, -0.0023,  ..., -0.0026, -0.0048, -0.0032],\n",
      "        [ 0.0037,  0.0011, -0.0013,  ...,  0.0003, -0.0021, -0.0029]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0004,  0.0007,  0.0004,  ...,  0.0013,  0.0002, -0.0009],\n",
      "        [ 0.0003,  0.0004,  0.0006,  ...,  0.0005, -0.0004, -0.0009],\n",
      "        [ 0.0009,  0.0004, -0.0005,  ...,  0.0009, -0.0004, -0.0004]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0004,  0.0007,  0.0004,  ...,  0.0013,  0.0002, -0.0009],\n",
      "        [ 0.0003,  0.0004,  0.0006,  ...,  0.0005, -0.0004, -0.0009],\n",
      "        [ 0.0009,  0.0004, -0.0005,  ...,  0.0009, -0.0004, -0.0004]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000e+00, -6.5847e-05, -2.1445e-03,  ..., -2.4584e-03,\n",
      "         -5.9713e-03, -3.0772e-03],\n",
      "        [ 5.7188e-03,  2.1276e-03, -1.6579e-03,  ..., -3.1450e-03,\n",
      "         -4.9592e-03, -3.6135e-03],\n",
      "        [ 4.9852e-03,  9.8143e-04, -1.9807e-03,  ...,  8.1292e-05,\n",
      "         -2.7208e-03, -2.3974e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0002,  0.0011,  0.0002,  ...,  0.0013,  0.0001, -0.0009],\n",
      "        [ 0.0007,  0.0008,  0.0008,  ...,  0.0006, -0.0004, -0.0010],\n",
      "        [ 0.0014,  0.0004, -0.0009,  ...,  0.0012, -0.0005, -0.0004]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0002,  0.0011,  0.0002,  ...,  0.0013,  0.0001, -0.0009],\n",
      "        [ 0.0007,  0.0008,  0.0008,  ...,  0.0006, -0.0004, -0.0010],\n",
      "        [ 0.0014,  0.0004, -0.0009,  ...,  0.0012, -0.0005, -0.0004]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000e+00,  5.2593e-04, -2.5219e-03,  ..., -2.8662e-03,\n",
      "         -5.7259e-03, -3.0806e-03],\n",
      "        [ 5.9694e-03,  2.5037e-03, -1.9194e-03,  ..., -1.9858e-03,\n",
      "         -5.1635e-03, -3.5509e-03],\n",
      "        [ 4.7147e-03,  5.1391e-05, -2.6301e-03,  ..., -6.4629e-04,\n",
      "         -3.1915e-03, -3.3651e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-2.3543e-04,  1.6494e-03, -8.9001e-05,  ...,  1.4458e-03,\n",
      "          5.2153e-05, -1.1236e-03],\n",
      "        [ 1.3564e-03,  1.1446e-03,  9.0791e-04,  ...,  5.4099e-04,\n",
      "         -4.4836e-04, -8.9195e-04],\n",
      "        [ 2.0109e-03,  4.3985e-04, -1.1258e-03,  ...,  1.2718e-03,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         -3.4580e-04, -3.3531e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-2.3543e-04,  1.6494e-03, -8.9001e-05,  ...,  1.4458e-03,\n",
      "          5.2153e-05, -1.1236e-03],\n",
      "        [ 1.3564e-03,  1.1446e-03,  9.0791e-04,  ...,  5.4099e-04,\n",
      "         -4.4836e-04, -8.9195e-04],\n",
      "        [ 2.0109e-03,  4.3985e-04, -1.1258e-03,  ...,  1.2718e-03,\n",
      "         -3.4580e-04, -3.3531e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000,  0.0024, -0.0022,  ..., -0.0039, -0.0060, -0.0030],\n",
      "        [ 0.0058,  0.0014, -0.0022,  ..., -0.0024, -0.0074, -0.0039],\n",
      "        [ 0.0045, -0.0001, -0.0016,  ..., -0.0028, -0.0049, -0.0042]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-3.4774e-04,  1.7009e-03, -2.5247e-04,  ...,  1.6251e-03,\n",
      "         -3.3965e-05, -1.3819e-03],\n",
      "        [ 2.2060e-03,  1.6063e-03,  9.3168e-04,  ...,  4.7573e-04,\n",
      "         -6.1792e-04, -8.3811e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-3.4774e-04,  1.7009e-03, -2.5247e-04,  ...,  1.6251e-03,\n",
      "         -3.3965e-05, -1.3819e-03],\n",
      "        [ 2.2060e-03,  1.6063e-03,  9.3168e-04,  ...,  4.7573e-04,\n",
      "         -6.1792e-04, -8.3811e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000,  0.0034, -0.0009,  ..., -0.0039, -0.0072, -0.0042],\n",
      "        [ 0.0049,  0.0010, -0.0017,  ..., -0.0043, -0.0081, -0.0045]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-5.0072e-04,  1.7628e-03, -1.3841e-04,  ...,  1.7206e-03,\n",
      "         -7.6717e-05, -1.6692e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-5.0072e-04,  1.7628e-03, -1.3841e-04,  ...,  1.7206e-03,\n",
      "         -7.6717e-05, -1.6692e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000e+00,  3.1890e-03, -1.5671e-03,  8.2852e-03,  8.7844e-03,\n",
      "         -1.4946e-03, -9.1593e-03,  2.4863e-03, -2.6216e-03,  0.0000e+00,\n",
      "          7.3114e-03,  6.6789e-03,  0.0000e+00, -8.7950e-03, -2.2846e-02,\n",
      "          1.0954e-03,  5.7483e-03, -8.3244e-03, -0.0000e+00, -4.5144e-03,\n",
      "          4.5821e-03,  1.2379e-02, -5.7224e-03, -3.1032e-03,  1.0134e-02,\n",
      "         -1.9348e-03,  5.6147e-03,  1.7171e-05,  4.1781e-03,  0.0000e+00,\n",
      "         -1.1556e-03,  2.4060e-03,  1.6245e-02, -6.4198e-03,  2.2997e-03,\n",
      "          2.5580e-03,  5.3056e-03,  4.8405e-04,  1.8842e-04, -1.8077e-03,\n",
      "          2.4509e-03, -1.0582e-02, -1.9142e-03,  5.8007e-03,  4.2965e-03,\n",
      "         -9.9483e-03,  4.0617e-03,  2.2026e-03, -0.0000e+00, -4.8705e-03,\n",
      "         -4.2584e-03, -8.8581e-05, -1.2736e-02, -0.0000e+00,  6.6046e-03,\n",
      "         -8.6546e-03,  1.8408e-03,  8.6053e-04,  1.0451e-02,  2.4237e-03,\n",
      "          7.1794e-03, -5.0669e-04,  9.9852e-03, -3.1600e-03, -0.0000e+00,\n",
      "          1.3255e-02,  9.3807e-04,  3.3163e-03, -1.5242e-03, -5.4544e-04,\n",
      "          8.5544e-04, -1.9746e-03,  0.0000e+00,  2.5554e-03,  0.0000e+00,\n",
      "          8.1600e-03, -4.6345e-03,  1.0397e-02,  1.2631e-04, -6.0099e-03,\n",
      "          1.3123e-02,  9.5178e-03,  0.0000e+00,  7.1289e-03,  4.8173e-03,\n",
      "          1.5299e-02,  5.2193e-03,  7.9570e-03, -9.5711e-05, -1.2852e-04,\n",
      "          7.4168e-03, -7.8148e-03, -0.0000e+00, -2.3282e-03, -2.2425e-03,\n",
      "         -1.0565e-03,  2.0953e-03,  1.8489e-03,  8.1117e-03, -1.5547e-02,\n",
      "         -6.1399e-03, -1.3185e-03, -2.0795e-03, -4.9125e-04,  2.4272e-03,\n",
      "         -0.0000e+00, -2.0390e-03, -1.9564e-03, -2.1080e-03, -1.8745e-03,\n",
      "          1.6429e-02, -9.5319e-03,  2.6199e-03,  5.1127e-03, -9.5986e-03,\n",
      "         -5.8602e-04,  4.3859e-04, -6.2269e-03, -4.4222e-04, -0.0000e+00,\n",
      "         -9.2045e-03,  2.9577e-04,  1.5113e-03,  9.8870e-04,  0.0000e+00,\n",
      "         -2.1416e-03,  4.7595e-03,  6.2696e-03, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -4.1856e-03, -0.0000e+00,  3.1414e-03,\n",
      "          1.0814e-03,  8.1129e-03,  5.5039e-04,  0.0000e+00, -6.9042e-03,\n",
      "         -6.1738e-05, -1.2535e-02, -0.0000e+00, -1.7925e-03,  1.0571e-02,\n",
      "          0.0000e+00,  1.7968e-03,  1.2740e-02,  1.0248e-02,  5.3410e-03,\n",
      "         -4.2409e-03,  5.9359e-03, -0.0000e+00, -0.0000e+00, -8.5024e-03,\n",
      "          7.7907e-04,  1.0511e-03, -1.5623e-03, -4.7821e-03,  7.3043e-03,\n",
      "         -3.8469e-03,  2.4640e-03,  8.5436e-03,  3.9324e-03, -1.0217e-02,\n",
      "         -4.2045e-03, -0.0000e+00, -3.5579e-03,  2.2034e-03,  0.0000e+00,\n",
      "          5.4310e-03, -1.0136e-02, -4.9752e-03,  4.1360e-03,  9.6616e-04,\n",
      "          4.7476e-03, -1.7485e-03,  1.9525e-03, -1.2004e-02, -8.3478e-03,\n",
      "         -6.5133e-03, -2.0854e-03,  1.4385e-03,  2.9608e-03, -8.1937e-04,\n",
      "          7.0619e-03,  0.0000e+00, -1.6060e-02, -7.4054e-03, -4.5813e-03,\n",
      "         -1.0986e-02,  3.5165e-03,  5.0843e-03,  1.3656e-02,  1.4074e-02,\n",
      "         -3.2277e-03, -7.0039e-04, -7.0884e-04,  6.0073e-03,  0.0000e+00,\n",
      "         -2.9962e-03, -2.8283e-03, -4.1847e-03,  7.9540e-03,  0.0000e+00,\n",
      "         -8.2529e-03,  1.6021e-02,  3.3293e-04,  4.0644e-03, -1.9983e-02,\n",
      "         -6.0935e-03,  1.7341e-02,  3.8138e-03,  0.0000e+00, -0.0000e+00,\n",
      "          1.7642e-03, -4.3009e-03, -3.6452e-03,  0.0000e+00, -1.5246e-02,\n",
      "         -8.3266e-03,  2.0786e-03, -7.9667e-03,  0.0000e+00,  2.8551e-03,\n",
      "          4.8422e-03,  5.0343e-03, -3.1986e-03, -1.0448e-03, -9.0841e-04,\n",
      "          4.9156e-04, -1.1426e-02,  0.0000e+00, -5.4714e-03, -4.5270e-03,\n",
      "          0.0000e+00, -1.5740e-02, -5.9789e-03, -7.3973e-03,  2.7339e-03,\n",
      "         -8.5002e-04, -7.7987e-04, -8.1956e-03,  1.6452e-02,  1.3633e-03,\n",
      "         -4.4267e-03, -7.5154e-03, -5.9634e-03,  1.3146e-03,  2.4291e-02,\n",
      "         -5.7331e-03,  0.0000e+00, -3.0272e-03,  6.0137e-03, -4.1165e-04,\n",
      "         -2.2783e-04,  2.1392e-02,  2.9705e-03,  0.0000e+00,  1.0588e-03,\n",
      "          8.3437e-05,  6.6398e-03,  1.0050e-02, -4.4790e-03,  1.2622e-02,\n",
      "         -6.5045e-03,  6.8353e-03,  7.3400e-03,  3.7572e-03, -9.2121e-04,\n",
      "          6.9958e-04, -3.2220e-03, -1.0927e-02,  7.8708e-03,  3.1806e-03,\n",
      "         -2.2910e-03, -1.3173e-03, -1.9211e-03, -5.4989e-03, -1.3095e-02,\n",
      "         -5.9786e-03, -6.0402e-03, -1.3406e-02,  2.9485e-03, -0.0000e+00,\n",
      "          6.1739e-03,  9.1832e-03, -1.4157e-02,  1.2364e-02,  2.1980e-03,\n",
      "         -2.3617e-03,  8.4385e-03, -1.1648e-02,  9.0942e-03,  4.9913e-03,\n",
      "          2.6903e-03, -6.7427e-03,  1.4774e-02, -0.0000e+00, -6.4361e-03,\n",
      "          6.6118e-03,  6.6122e-03,  9.1504e-03, -6.0016e-03,  1.3553e-02,\n",
      "          7.9908e-03, -1.6246e-04,  4.8532e-03,  8.8572e-03,  8.9424e-03,\n",
      "         -3.8192e-04, -7.1792e-04, -1.5181e-02, -3.2721e-04, -3.7487e-03,\n",
      "         -2.2552e-03, -8.3655e-03, -1.7202e-02, -5.8522e-03,  1.5962e-02,\n",
      "         -8.0151e-03, -4.5137e-03, -0.0000e+00, -1.0675e-03,  8.5555e-03,\n",
      "         -1.1132e-02,  1.5208e-05,  3.3467e-03,  0.0000e+00,  7.0574e-03,\n",
      "          7.2528e-03, -1.3904e-03,  5.2704e-04,  8.2692e-03,  2.9516e-03,\n",
      "          0.0000e+00, -1.6818e-03,  0.0000e+00,  4.3441e-03,  7.8426e-03,\n",
      "          6.4013e-05,  1.1921e-02, -1.2241e-02, -2.5846e-03,  3.3811e-03,\n",
      "         -0.0000e+00, -8.3511e-03, -7.0922e-04,  4.5245e-03, -3.6476e-03,\n",
      "          3.3271e-03,  0.0000e+00,  4.4916e-03,  8.6541e-03,  7.5621e-03,\n",
      "         -1.1020e-02,  1.3519e-02, -2.8163e-03,  3.1235e-03,  8.1854e-04,\n",
      "         -1.4191e-02,  3.7798e-04, -3.0281e-03, -8.7624e-03, -7.7391e-03,\n",
      "          1.0376e-02, -9.9515e-03,  9.3955e-03, -7.5716e-03, -1.3346e-02,\n",
      "         -1.3109e-02,  1.9555e-04,  1.8538e-04, -1.0811e-02,  8.2025e-04,\n",
      "          2.5005e-04,  1.7688e-03, -5.4141e-03,  1.0165e-02,  1.5971e-03,\n",
      "          4.6905e-03, -1.7762e-03, -2.0350e-03, -4.4599e-04, -0.0000e+00,\n",
      "          4.5451e-03, -0.0000e+00,  3.2562e-03,  6.1497e-03,  4.3530e-03,\n",
      "          0.0000e+00, -1.1167e-02,  6.5779e-03,  1.1431e-04, -6.2906e-03,\n",
      "          1.7090e-03, -4.3009e-03,  4.1480e-03,  4.1792e-03,  4.3963e-03,\n",
      "          2.6541e-03,  8.0356e-03, -5.8391e-03,  9.8171e-03,  1.9921e-03,\n",
      "         -7.1087e-03, -4.3451e-03, -3.3537e-04,  1.7768e-02, -9.7923e-03,\n",
      "          2.1212e-03, -5.3488e-04,  0.0000e+00, -2.2210e-03,  1.8937e-02,\n",
      "         -1.4699e-02,  2.3231e-03, -0.0000e+00,  7.5184e-04,  6.6701e-03,\n",
      "         -6.5535e-04, -8.2929e-03,  1.6502e-03,  2.5406e-04, -4.1728e-03,\n",
      "         -7.3800e-03,  2.6099e-03, -1.1065e-02,  1.2625e-03,  7.8231e-03,\n",
      "         -0.0000e+00,  4.2556e-03,  3.9605e-03, -7.1195e-03, -9.3573e-03,\n",
      "         -0.0000e+00,  0.0000e+00, -6.8007e-03,  1.3573e-03, -3.7150e-03,\n",
      "         -1.0943e-03, -3.3968e-03, -2.8744e-03, -4.1447e-03,  3.1672e-03,\n",
      "         -2.2208e-03, -1.7766e-03, -7.4479e-03,  6.2334e-03,  0.0000e+00,\n",
      "         -5.5126e-04, -1.1902e-02, -0.0000e+00,  6.4308e-03,  3.2373e-03,\n",
      "          1.0283e-02, -2.0806e-03,  6.6735e-03, -3.8646e-04,  1.9212e-03,\n",
      "         -3.3743e-03,  3.1585e-03,  6.5213e-03,  7.4830e-03, -2.1114e-03,\n",
      "          2.7954e-03, -1.0628e-02, -6.3631e-03, -7.3373e-03,  4.2089e-04,\n",
      "          2.8929e-03,  8.9558e-05,  5.7493e-03,  9.6589e-04, -6.6548e-03,\n",
      "         -7.8501e-03,  4.3137e-03, -1.1083e-02,  4.8952e-03,  2.1433e-04,\n",
      "          4.1607e-03,  4.3513e-04,  7.7691e-03, -1.0246e-02, -3.5887e-03,\n",
      "          2.0078e-03,  5.2215e-03,  0.0000e+00,  5.9533e-03,  6.0081e-04,\n",
      "         -2.2063e-03,  8.0496e-03, -9.5407e-03, -3.3206e-03, -1.6267e-02,\n",
      "          3.5194e-04, -4.0435e-03,  2.3797e-03,  1.5588e-02, -6.3006e-03,\n",
      "          5.3833e-03, -0.0000e+00,  3.4804e-03, -6.7421e-03, -3.4019e-03,\n",
      "          5.8066e-03, -2.0532e-03, -4.5563e-03,  4.2886e-03, -3.1147e-03,\n",
      "         -7.8134e-03, -4.2526e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-3.3452e-04,  1.5643e-03, -4.2950e-05,  ...,  1.6443e-03,\n",
      "         -3.8740e-04, -1.8117e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-3.3452e-04,  1.5643e-03, -4.2950e-05,  ...,  1.6443e-03,\n",
      "         -3.8740e-04, -1.8117e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000e+00,  1.8143e-03, -1.5673e-03,  1.0043e-02,  1.1163e-02,\n",
      "         -8.3524e-05, -1.1078e-02,  2.8044e-03, -3.7135e-03,  0.0000e+00,\n",
      "          8.2538e-03,  7.7665e-03, -0.0000e+00, -8.8365e-03, -2.4787e-02,\n",
      "          1.8870e-03,  3.6826e-03, -1.0505e-02, -0.0000e+00, -4.9054e-03,\n",
      "          5.4184e-03,  1.3974e-02, -4.2120e-03, -4.3417e-03,  9.4597e-03,\n",
      "         -3.5648e-04,  6.7118e-03, -4.5433e-05,  4.3550e-03,  0.0000e+00,\n",
      "         -2.6806e-04, -1.1772e-04,  1.7609e-02, -6.4042e-03,  3.0045e-04,\n",
      "          3.6450e-03,  7.9296e-03,  1.6028e-03, -4.6865e-04, -1.1300e-03,\n",
      "          3.3437e-03, -1.2519e-02, -2.2354e-03,  5.3427e-03,  6.0054e-03,\n",
      "         -1.0960e-02,  4.6854e-03,  2.7335e-03, -0.0000e+00, -4.8193e-03,\n",
      "         -4.1486e-03,  2.4509e-04, -1.3281e-02, -0.0000e+00,  7.7373e-03,\n",
      "         -9.1239e-03,  3.0294e-03,  1.7752e-03,  1.0497e-02,  1.0196e-03,\n",
      "          7.3812e-03, -4.4160e-05,  1.0988e-02, -4.8540e-03, -0.0000e+00,\n",
      "          1.3356e-02,  2.2454e-04,  2.8262e-03, -2.3742e-03, -1.0389e-03,\n",
      "          1.6355e-03, -2.9886e-03,  0.0000e+00,  3.2545e-03,  0.0000e+00,\n",
      "          7.9696e-03, -6.2468e-03,  1.1701e-02, -1.3568e-03, -5.9164e-03,\n",
      "          1.4683e-02,  1.2572e-02,  0.0000e+00,  7.4866e-03,  5.4465e-03,\n",
      "          1.8403e-02,  5.3707e-03,  1.0737e-02, -1.3215e-04,  2.2709e-05,\n",
      "          8.8726e-03, -7.7613e-03, -0.0000e+00, -1.5044e-03, -2.1542e-03,\n",
      "         -1.7917e-03,  4.4910e-04,  3.7032e-03,  8.6264e-03, -1.6684e-02,\n",
      "         -4.1129e-03,  2.7337e-04, -3.8937e-03,  3.1662e-04,  2.5193e-03,\n",
      "         -0.0000e+00, -2.8753e-03, -1.3997e-03, -2.7600e-03, -4.5792e-03,\n",
      "          1.9092e-02, -9.7933e-03,  1.5204e-03,  4.4198e-03, -1.1368e-02,\n",
      "         -1.0019e-03,  9.7743e-04, -5.7363e-03, -6.4443e-04, -0.0000e+00,\n",
      "         -1.0597e-02,  8.3470e-04,  1.0382e-03,  2.1123e-03,  0.0000e+00,\n",
      "         -3.2830e-03,  4.8426e-03,  4.6186e-03, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -3.3726e-03, -0.0000e+00,  3.7800e-04,\n",
      "          2.0589e-03,  8.2358e-03, -1.7349e-03,  0.0000e+00, -9.6638e-03,\n",
      "         -5.5604e-04, -1.3380e-02, -0.0000e+00, -2.9370e-03,  1.1162e-02,\n",
      "          0.0000e+00,  1.9767e-03,  1.3260e-02,  1.0510e-02,  6.0630e-03,\n",
      "         -4.8968e-03,  5.3509e-03, -0.0000e+00, -0.0000e+00, -1.0508e-02,\n",
      "          2.7836e-05,  9.0052e-04, -8.6519e-04, -4.0220e-03,  6.6360e-03,\n",
      "         -2.8437e-03,  1.9138e-03,  1.0913e-02,  3.9682e-03, -9.8116e-03,\n",
      "         -4.4300e-03, -0.0000e+00, -4.5102e-03,  2.2350e-04,  0.0000e+00,\n",
      "          5.9407e-03, -1.0850e-02, -4.0901e-03,  3.2599e-03,  1.0703e-03,\n",
      "          4.7434e-03, -1.9350e-03,  2.1418e-03, -1.1940e-02, -9.4979e-03,\n",
      "         -7.5140e-03, -2.1138e-03,  7.2967e-04,  2.9305e-03, -1.1571e-03,\n",
      "          8.5385e-03,  0.0000e+00, -1.7872e-02, -8.5267e-03, -4.4632e-03,\n",
      "         -1.2000e-02,  2.7328e-03,  5.0865e-03,  1.4645e-02,  1.5424e-02,\n",
      "         -2.5130e-03, -1.6282e-03, -1.3945e-03,  6.3282e-03,  0.0000e+00,\n",
      "         -1.4418e-03, -7.0537e-04, -3.7813e-03,  9.0105e-03,  0.0000e+00,\n",
      "         -9.6517e-03,  1.4889e-02, -1.0119e-03,  3.7299e-03, -2.1058e-02,\n",
      "         -6.5510e-03,  1.6590e-02,  5.0079e-03,  0.0000e+00, -0.0000e+00,\n",
      "          1.4167e-03, -4.0408e-03, -5.1007e-03,  0.0000e+00, -1.6111e-02,\n",
      "         -7.8809e-03, -1.1593e-04, -8.1266e-03,  0.0000e+00,  2.0557e-03,\n",
      "          5.3852e-03,  4.6637e-03, -1.7935e-03, -3.5055e-04, -1.9625e-04,\n",
      "          3.8605e-04, -1.1311e-02,  0.0000e+00, -6.3278e-03, -3.4777e-03,\n",
      "          0.0000e+00, -1.6557e-02, -6.9952e-03, -8.7739e-03,  3.6491e-03,\n",
      "         -1.5214e-03, -1.4290e-03, -9.6119e-03,  1.8278e-02,  1.9928e-03,\n",
      "         -5.1439e-03, -8.0394e-03, -7.3652e-03,  3.2871e-03,  2.7257e-02,\n",
      "         -6.3302e-03,  0.0000e+00, -2.4758e-03,  5.1425e-03, -1.1700e-03,\n",
      "          5.9578e-04,  2.4032e-02,  2.0688e-03,  0.0000e+00, -5.7461e-04,\n",
      "         -8.0249e-04,  5.7936e-03,  9.8785e-03, -4.1535e-03,  1.3225e-02,\n",
      "         -5.7543e-03,  6.9124e-03,  6.5839e-03,  5.1052e-03, -2.5715e-03,\n",
      "          9.4114e-04, -4.3082e-03, -1.1277e-02,  7.0901e-03,  3.4517e-03,\n",
      "         -2.2222e-03, -4.5921e-04, -3.3955e-03, -5.9319e-03, -1.4502e-02,\n",
      "         -4.8008e-03, -5.6593e-03, -1.5872e-02,  1.6952e-03, -0.0000e+00,\n",
      "          6.5701e-03,  1.0912e-02, -1.6258e-02,  1.1769e-02,  1.8804e-03,\n",
      "         -3.3364e-03,  1.0465e-02, -1.1940e-02,  8.4566e-03,  5.1093e-03,\n",
      "          3.8924e-04, -7.2112e-03,  1.5691e-02, -0.0000e+00, -5.9584e-03,\n",
      "          7.8373e-03,  5.9499e-03,  9.0078e-03, -7.2538e-03,  1.4839e-02,\n",
      "          9.3342e-03,  1.3044e-03,  5.4317e-03,  8.7290e-03,  1.0833e-02,\n",
      "         -6.5043e-04, -1.1791e-03, -1.6834e-02, -7.6525e-04, -2.3884e-03,\n",
      "         -2.5703e-03, -7.4902e-03, -1.7779e-02, -6.7716e-03,  1.6925e-02,\n",
      "         -9.2094e-03, -4.8145e-03, -0.0000e+00, -5.9907e-04,  8.7884e-03,\n",
      "         -1.0404e-02, -2.9000e-03,  4.7292e-03,  0.0000e+00,  7.6551e-03,\n",
      "          6.3414e-03, -1.4987e-03, -5.9614e-04,  1.0313e-02,  3.2503e-03,\n",
      "          0.0000e+00, -1.5497e-03,  0.0000e+00,  5.5504e-03,  8.7567e-03,\n",
      "         -6.2828e-04,  1.0846e-02, -1.3016e-02, -2.4955e-03,  4.3339e-03,\n",
      "         -0.0000e+00, -8.7421e-03, -2.7785e-04,  4.6879e-03, -4.3787e-03,\n",
      "          3.6245e-03,  0.0000e+00,  5.1780e-03,  7.7364e-03,  8.4812e-03,\n",
      "         -1.2073e-02,  1.3673e-02, -1.4306e-03,  1.9623e-03,  1.0027e-05,\n",
      "         -1.4735e-02, -4.8431e-04, -4.4522e-03, -9.0657e-03, -8.5800e-03,\n",
      "          1.2148e-02, -1.0433e-02,  1.1766e-02, -7.4055e-03, -1.5328e-02,\n",
      "         -1.7314e-02,  7.6429e-05, -1.6348e-04, -1.2382e-02, -6.2986e-04,\n",
      "          1.0889e-04,  6.8866e-04, -6.4785e-03,  1.0702e-02,  2.6722e-03,\n",
      "          6.4220e-03, -2.1733e-03, -1.9456e-03, -3.1384e-03, -0.0000e+00,\n",
      "          4.4127e-03, -0.0000e+00,  4.8252e-03,  5.5809e-03,  4.7391e-03,\n",
      "          0.0000e+00, -1.1441e-02,  7.5271e-03,  1.0405e-03, -6.2136e-03,\n",
      "          1.1941e-03, -2.7223e-03,  5.3405e-03,  5.0539e-03,  6.1329e-03,\n",
      "          3.7542e-03,  7.8906e-03, -6.6612e-03,  1.0489e-02,  5.1048e-04,\n",
      "         -7.4758e-03, -4.1937e-03, -8.9231e-04,  1.8249e-02, -9.6215e-03,\n",
      "          3.1758e-03, -1.4611e-03,  0.0000e+00, -3.2984e-03,  2.1819e-02,\n",
      "         -1.4449e-02,  2.6087e-03, -0.0000e+00,  1.4785e-03,  9.4995e-03,\n",
      "         -1.7727e-04, -8.2774e-03,  8.0144e-04, -4.2808e-05, -5.0094e-03,\n",
      "         -6.9055e-03,  5.5742e-03, -9.5584e-03,  3.7444e-03,  7.4039e-03,\n",
      "         -0.0000e+00,  3.8126e-03,  5.5604e-03, -6.2778e-03, -9.6972e-03,\n",
      "         -0.0000e+00,  0.0000e+00, -7.7337e-03,  1.4768e-03, -3.7631e-03,\n",
      "         -2.9962e-03, -3.9469e-03, -2.4243e-03, -3.3677e-03,  2.9161e-03,\n",
      "         -1.3997e-03, -3.2888e-03, -7.1770e-03,  5.6158e-03,  0.0000e+00,\n",
      "         -7.3628e-04, -1.2068e-02, -0.0000e+00,  5.7485e-03,  2.8856e-03,\n",
      "          1.1188e-02, -1.9205e-03,  8.3846e-03,  1.2038e-03,  1.9736e-03,\n",
      "         -4.0765e-03,  4.1482e-03,  8.3166e-03,  7.8556e-03, -2.9816e-03,\n",
      "          3.1739e-03, -9.5708e-03, -6.7285e-03, -7.3471e-03, -3.7257e-04,\n",
      "          3.1192e-03,  6.5244e-04,  5.2625e-03,  2.8479e-04, -6.5341e-03,\n",
      "         -8.8142e-03,  4.9581e-03, -1.3823e-02,  5.1606e-03, -4.4780e-04,\n",
      "          5.4435e-03,  3.5442e-04,  9.2341e-03, -1.0829e-02, -3.5630e-03,\n",
      "          1.2680e-03,  4.4705e-03,  0.0000e+00,  5.1040e-03,  3.9879e-04,\n",
      "         -1.2080e-03,  6.3522e-03, -9.5116e-03, -3.8820e-03, -1.7964e-02,\n",
      "         -4.4010e-04, -5.4646e-03,  1.1838e-03,  1.6336e-02, -8.9800e-03,\n",
      "          4.5491e-03, -0.0000e+00,  1.1713e-03, -6.6320e-03, -5.4232e-03,\n",
      "          5.7959e-03, -2.1621e-03, -4.8441e-03,  4.6281e-03, -2.7071e-03,\n",
      "         -8.2393e-03, -2.7342e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 9 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0001,  0.0014, -0.0001,  ...,  0.0012, -0.0004, -0.0019]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0001,  0.0014, -0.0001,  ...,  0.0012, -0.0004, -0.0019]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0000e+00,  1.1465e-03, -8.7353e-04,  9.4325e-03,  1.1390e-02,\n",
      "          9.7895e-04, -1.1663e-02,  3.0905e-03, -3.1397e-03,  0.0000e+00,\n",
      "          8.0866e-03,  6.0290e-03, -0.0000e+00, -9.5793e-03, -2.3943e-02,\n",
      "          1.8181e-03,  4.2131e-03, -1.0818e-02, -0.0000e+00, -4.4826e-03,\n",
      "          3.6229e-03,  1.4120e-02, -5.4786e-03, -3.2847e-03,  8.3364e-03,\n",
      "         -3.1170e-04,  6.7566e-03,  1.3385e-03,  4.9440e-03,  0.0000e+00,\n",
      "         -7.2259e-04,  1.3887e-03,  1.7171e-02, -5.9098e-03,  5.0926e-04,\n",
      "          4.3518e-03,  7.5935e-03,  2.2030e-03,  4.4880e-04, -2.7000e-03,\n",
      "          4.0233e-03, -1.3058e-02, -1.2916e-03,  6.6424e-03,  4.5699e-03,\n",
      "         -1.1446e-02,  3.3469e-03,  1.6866e-03, -0.0000e+00, -5.6709e-03,\n",
      "         -4.9221e-03,  1.3915e-03, -1.3350e-02, -0.0000e+00,  8.3370e-03,\n",
      "         -8.7470e-03,  4.4376e-03,  6.1199e-04,  1.0086e-02,  2.2475e-03,\n",
      "          7.4731e-03,  2.8239e-04,  1.1021e-02, -5.3756e-03, -0.0000e+00,\n",
      "          1.4418e-02, -1.7682e-03,  1.7699e-03, -2.6398e-03, -8.5880e-04,\n",
      "          1.4380e-03, -3.6809e-03, -0.0000e+00,  4.6702e-03,  0.0000e+00,\n",
      "          8.4422e-03, -5.8409e-03,  1.1044e-02, -2.5904e-03, -5.9279e-03,\n",
      "          1.3673e-02,  1.2620e-02,  0.0000e+00,  8.1612e-03,  4.0372e-03,\n",
      "          1.6750e-02,  6.9391e-03,  9.6521e-03, -5.8251e-04,  1.8203e-03,\n",
      "          6.5816e-03, -8.0146e-03, -0.0000e+00,  2.6280e-04, -1.3600e-03,\n",
      "         -1.3903e-03,  4.6405e-04,  3.3869e-03,  8.8427e-03, -1.6069e-02,\n",
      "         -7.6828e-03, -1.1992e-03, -2.4202e-03,  5.8677e-04,  2.9451e-03,\n",
      "         -0.0000e+00, -2.9780e-03, -1.1521e-03, -3.9616e-03, -4.1277e-03,\n",
      "          2.0131e-02, -9.9827e-03,  2.9228e-03,  3.3802e-03, -1.1440e-02,\n",
      "         -1.9780e-03,  7.4639e-04, -4.8996e-03, -6.6173e-04, -0.0000e+00,\n",
      "         -1.0965e-02, -9.2124e-05,  3.8816e-04,  2.3355e-03,  0.0000e+00,\n",
      "         -3.5046e-03,  4.8071e-03,  4.3677e-03, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -4.3086e-03, -0.0000e+00,  1.9815e-03,\n",
      "          2.9962e-03,  7.0923e-03, -1.3172e-03,  0.0000e+00, -7.7414e-03,\n",
      "         -6.8934e-04, -1.2348e-02, -0.0000e+00, -1.9750e-03,  1.0918e-02,\n",
      "          0.0000e+00, -3.2303e-04,  1.2945e-02,  1.1397e-02,  5.4788e-03,\n",
      "         -4.6034e-03,  5.9548e-03, -0.0000e+00, -0.0000e+00, -1.0733e-02,\n",
      "         -4.4686e-04,  2.2711e-03, -3.8730e-04, -4.1738e-03,  6.1811e-03,\n",
      "         -3.0943e-03,  1.3333e-03,  9.5936e-03,  3.0485e-03, -1.0653e-02,\n",
      "         -4.3669e-03, -0.0000e+00, -4.5775e-03,  1.0661e-03,  0.0000e+00,\n",
      "          5.4134e-03, -1.1151e-02, -6.5776e-03,  4.0197e-03,  8.5955e-04,\n",
      "          5.2554e-03, -1.5020e-03,  4.0336e-04, -1.1648e-02, -9.4937e-03,\n",
      "         -6.4052e-03, -1.2075e-03,  6.6109e-05,  4.2781e-03, -2.4601e-04,\n",
      "          7.7740e-03,  0.0000e+00, -1.7369e-02, -7.7818e-03, -5.4538e-03,\n",
      "         -1.1792e-02,  1.1390e-03,  3.8926e-03,  1.4469e-02,  1.4455e-02,\n",
      "         -3.1840e-03, -3.3654e-03,  5.4555e-04,  6.2634e-03,  0.0000e+00,\n",
      "         -1.0427e-03, -2.9952e-04, -5.6194e-03,  8.3184e-03,  0.0000e+00,\n",
      "         -9.8387e-03,  1.5685e-02, -9.8933e-04,  5.9595e-03, -2.1037e-02,\n",
      "         -5.5122e-03,  1.6640e-02,  4.7482e-03,  0.0000e+00, -0.0000e+00,\n",
      "          3.0166e-05, -2.7675e-03, -5.8514e-03,  0.0000e+00, -1.8201e-02,\n",
      "         -6.3461e-03, -1.6714e-03, -8.0412e-03,  0.0000e+00,  1.3476e-03,\n",
      "          4.5911e-03,  4.9884e-03, -1.7709e-03,  4.9054e-04,  8.7488e-04,\n",
      "          7.2782e-05, -1.1205e-02,  0.0000e+00, -6.2737e-03, -3.6070e-03,\n",
      "          0.0000e+00, -1.8614e-02, -7.2136e-03, -8.2668e-03,  4.0754e-03,\n",
      "         -7.2416e-04, -1.8425e-03, -9.0974e-03,  1.7348e-02,  2.1085e-03,\n",
      "         -4.8671e-03, -8.3468e-03, -7.4132e-03,  4.1339e-03,  2.6949e-02,\n",
      "         -7.2232e-03,  0.0000e+00, -5.5225e-04,  4.5570e-03, -1.0758e-03,\n",
      "         -5.4647e-05,  2.3687e-02,  2.5842e-03,  0.0000e+00, -1.7320e-03,\n",
      "         -1.2914e-03,  7.4005e-03,  9.6722e-03, -3.3434e-03,  1.1263e-02,\n",
      "         -7.1003e-03,  7.6703e-03,  7.8460e-03,  5.1180e-03, -2.8160e-03,\n",
      "          1.1936e-03, -4.8605e-03, -1.1978e-02,  7.6663e-03,  4.5884e-03,\n",
      "         -2.2920e-03, -8.7782e-05, -2.7914e-03, -6.4293e-03, -1.4080e-02,\n",
      "         -3.6025e-03, -5.7761e-03, -1.5818e-02,  8.1408e-04, -0.0000e+00,\n",
      "          6.2443e-03,  8.0959e-03, -1.5361e-02,  1.3110e-02,  5.8151e-04,\n",
      "         -3.1931e-03,  1.0013e-02, -1.1682e-02,  7.7155e-03,  6.0310e-03,\n",
      "          5.1723e-04, -7.0109e-03,  1.5527e-02, -0.0000e+00, -7.0732e-03,\n",
      "          8.6510e-03,  4.9933e-03,  8.9132e-03, -7.1812e-03,  1.5929e-02,\n",
      "          8.6781e-03,  1.7060e-03,  4.6501e-03,  7.7651e-03,  9.3442e-03,\n",
      "         -1.0458e-03, -8.6631e-04, -1.7836e-02,  8.5715e-04, -3.5501e-03,\n",
      "         -2.8066e-03, -6.7847e-03, -1.8641e-02, -5.3338e-03,  1.7007e-02,\n",
      "         -9.2497e-03, -4.1663e-03, -0.0000e+00, -9.9738e-04,  7.4830e-03,\n",
      "         -9.5307e-03, -3.1283e-03,  4.8532e-03,  0.0000e+00,  9.0693e-03,\n",
      "          7.0478e-03, -4.0868e-04, -3.8934e-04,  1.0437e-02,  4.1046e-03,\n",
      "          0.0000e+00, -1.6124e-03,  0.0000e+00,  4.6055e-03,  1.1009e-02,\n",
      "          3.5362e-05,  1.1021e-02, -1.3448e-02, -3.9515e-03,  5.2113e-03,\n",
      "         -0.0000e+00, -7.8866e-03, -1.0777e-03,  4.5929e-03, -2.9180e-03,\n",
      "          3.2298e-03,  0.0000e+00,  3.1207e-03,  8.8055e-03,  9.3442e-03,\n",
      "         -1.4035e-02,  1.3773e-02, -1.0714e-03,  2.6997e-03,  3.8946e-04,\n",
      "         -1.4739e-02,  1.1962e-04, -4.4681e-03, -7.5124e-03, -8.8483e-03,\n",
      "          1.2322e-02, -1.0986e-02,  1.2575e-02, -7.9818e-03, -1.6729e-02,\n",
      "         -1.6697e-02, -5.8038e-04,  1.0977e-03, -1.2194e-02, -1.2220e-03,\n",
      "          1.8262e-03,  7.2402e-04, -6.9819e-03,  1.0315e-02,  2.5208e-03,\n",
      "          4.0525e-03, -2.7633e-03, -2.2753e-03, -1.8408e-03, -0.0000e+00,\n",
      "          4.3149e-03, -0.0000e+00,  4.3036e-03,  5.8314e-03,  6.0175e-03,\n",
      "          0.0000e+00, -1.1337e-02,  8.7654e-03, -1.6716e-03, -5.4542e-03,\n",
      "          1.4307e-03, -2.0048e-03,  3.6622e-03,  4.8562e-03,  5.9860e-03,\n",
      "          2.7644e-03,  7.9755e-03, -7.7261e-03,  9.7433e-03,  7.9196e-04,\n",
      "         -8.5152e-03, -5.3368e-03,  5.9182e-04,  1.7230e-02, -7.9498e-03,\n",
      "          1.8079e-03,  3.9828e-04,  0.0000e+00, -3.8016e-03,  2.2315e-02,\n",
      "         -1.6123e-02,  2.5571e-03, -0.0000e+00,  3.1025e-03,  8.3635e-03,\n",
      "         -6.7865e-04, -7.5777e-03,  1.5795e-03, -3.0965e-04, -4.1287e-03,\n",
      "         -6.8487e-03,  5.0424e-03, -1.1266e-02,  4.6789e-03,  6.3095e-03,\n",
      "         -0.0000e+00,  4.3900e-03,  6.8923e-03, -6.7781e-03, -7.6781e-03,\n",
      "         -0.0000e+00,  0.0000e+00, -8.5824e-03,  1.3199e-03, -4.4747e-03,\n",
      "         -2.8047e-03, -3.8055e-03, -2.6849e-03, -4.0626e-03,  3.7037e-03,\n",
      "         -1.4495e-03, -1.4726e-03, -7.5714e-03,  4.4607e-03,  0.0000e+00,\n",
      "         -2.1434e-03, -1.2041e-02, -0.0000e+00,  5.7396e-03,  3.4706e-03,\n",
      "          1.0172e-02, -2.6258e-03,  8.0662e-03, -4.5135e-04,  8.6063e-04,\n",
      "         -5.0954e-03,  5.0815e-03,  7.8759e-03,  7.3350e-03, -1.5474e-03,\n",
      "          3.7863e-03, -9.5220e-03, -4.1406e-03, -5.9809e-03, -1.4259e-04,\n",
      "          2.9683e-03, -1.9364e-04,  5.8495e-03, -1.8560e-04, -5.2840e-03,\n",
      "         -8.5401e-03,  4.9576e-03, -1.2015e-02,  5.8099e-03, -1.1373e-03,\n",
      "          4.4734e-03, -6.5968e-04,  9.8865e-03, -1.0642e-02, -2.3024e-03,\n",
      "          1.6891e-03,  5.0436e-03,  0.0000e+00,  4.4808e-03,  2.5763e-04,\n",
      "         -3.3755e-03,  6.6980e-03, -9.3704e-03, -3.3656e-03, -1.7821e-02,\n",
      "          2.6464e-04, -3.4106e-03,  8.9945e-04,  1.5826e-02, -7.2495e-03,\n",
      "          3.1164e-03, -0.0000e+00,  2.4151e-03, -5.5209e-03, -3.8216e-03,\n",
      "          6.1599e-03, -2.2951e-03, -5.1682e-03,  4.3035e-03, -4.4591e-03,\n",
      "         -8.7055e-03, -3.5307e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[-0.0002,  0.0011,  0.0000,  ..., -0.0017, -0.0005,  0.0018],\n",
      "         [-0.0007, -0.0005, -0.0005,  ..., -0.0015, -0.0006,  0.0011],\n",
      "         [-0.0009, -0.0004, -0.0002,  ..., -0.0011, -0.0007,  0.0013]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>), tensor([[[-2.6428e-05,  1.2669e-03, -1.7266e-04,  ...,  8.4538e-04,\n",
      "          -2.9101e-04, -2.2209e-03],\n",
      "         [ 2.7449e-03,  1.8384e-03,  9.2600e-04,  ...,  2.7611e-04,\n",
      "          -6.2586e-04, -1.2930e-03],\n",
      "         [ 2.2733e-03,  4.9089e-04, -1.4160e-03,  ...,  1.1644e-03,\n",
      "          -1.6973e-04, -6.8455e-04]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tRUN backward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? True\n",
      "\t\t\t\tOh, then create full_batch_previous memory and state by (3, 4096) tensor filling 0.\n",
      "\t\t\t\tSet current_length_index... is it forward?? False\n",
      "\t\t\t\tOops, backward!! current_length_index = 0\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "binary_mask = tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [0.0000, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 9 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.5776e-04,  8.5008e-04, -9.0526e-04,  4.5458e-03, -7.3213e-04,\n",
      "          2.0647e-03, -1.1265e-03, -3.2782e-03,  1.6024e-03,  3.6667e-03,\n",
      "         -4.1003e-04, -1.7901e-03,  2.6226e-04, -3.7832e-04,  0.0000e+00,\n",
      "          0.0000e+00,  1.1847e-03,  3.1437e-03, -2.5433e-03,  1.4463e-03,\n",
      "         -5.9337e-04,  1.8439e-03, -1.7969e-04, -2.8421e-03,  0.0000e+00,\n",
      "          7.0549e-04, -1.5668e-03, -7.0174e-04, -1.2155e-04, -2.2479e-03,\n",
      "         -6.7636e-04,  1.9502e-03,  1.0213e-03, -5.0221e-04, -2.5682e-03,\n",
      "         -1.5692e-03,  0.0000e+00, -6.0628e-04,  1.0332e-04, -7.1813e-04,\n",
      "          1.1502e-05,  4.9686e-03, -0.0000e+00,  9.6054e-04, -3.1009e-03,\n",
      "         -1.9814e-03, -2.2883e-03,  0.0000e+00,  1.3948e-03, -2.6666e-03,\n",
      "          5.6644e-04, -2.0831e-04, -0.0000e+00,  1.8394e-03,  0.0000e+00,\n",
      "         -7.8157e-05,  0.0000e+00, -2.1115e-03,  5.4939e-03, -0.0000e+00,\n",
      "          1.2039e-03, -4.7978e-03,  1.9393e-03, -3.2964e-03, -6.0457e-05,\n",
      "         -6.8667e-04,  2.8970e-03, -5.1602e-04,  0.0000e+00,  3.1340e-03,\n",
      "         -1.3468e-03,  2.5437e-03,  1.1668e-03,  1.4381e-03, -1.5171e-03,\n",
      "          1.5834e-03,  2.2811e-03, -3.6379e-03,  2.2602e-03, -0.0000e+00,\n",
      "          1.2462e-03, -9.8982e-04,  5.4467e-05, -3.7576e-03,  2.1107e-03,\n",
      "         -3.2958e-03,  3.2325e-05,  2.6564e-04, -4.1071e-03,  1.8964e-03,\n",
      "          9.4753e-04,  1.0758e-03, -5.2422e-04,  1.9644e-03,  3.8801e-05,\n",
      "          2.7151e-03,  4.0497e-03,  3.0533e-03,  1.8757e-03, -7.0309e-04,\n",
      "         -4.6426e-04, -4.7183e-04, -0.0000e+00, -2.8152e-03, -1.6463e-03,\n",
      "         -1.8307e-03, -1.9628e-04,  1.8505e-03, -2.5913e-03,  4.0462e-03,\n",
      "         -8.7301e-04,  0.0000e+00,  4.3543e-03, -1.6730e-03, -2.9455e-04,\n",
      "         -1.6676e-03, -1.4985e-04,  8.0494e-04,  0.0000e+00, -7.5891e-04,\n",
      "          1.3438e-04,  1.2193e-03,  7.7371e-04, -1.3923e-03,  2.7770e-04,\n",
      "         -2.0941e-03,  1.8606e-03, -3.8722e-03, -2.9263e-04, -3.0951e-03,\n",
      "         -1.1681e-03, -3.2595e-04, -1.6243e-03, -2.9600e-04, -1.1006e-03,\n",
      "         -2.7569e-03,  1.5049e-03, -5.7323e-04, -5.1604e-04,  3.1358e-04,\n",
      "          5.7718e-04, -4.0222e-03,  1.8154e-03, -6.6027e-04,  2.1883e-03,\n",
      "          2.5920e-03,  1.8552e-03,  3.2691e-03, -0.0000e+00,  0.0000e+00,\n",
      "         -1.4578e-03, -9.0704e-05, -1.9690e-04, -1.5020e-03, -1.4508e-03,\n",
      "         -0.0000e+00, -2.6958e-03,  0.0000e+00, -1.3681e-03, -0.0000e+00,\n",
      "          1.3930e-03, -1.2015e-03, -1.3739e-03,  1.1979e-03,  2.4087e-04,\n",
      "         -8.3153e-04, -2.2274e-03,  2.4166e-03,  3.9723e-03, -3.8011e-05,\n",
      "         -4.4907e-03, -9.4617e-04,  9.9445e-04, -2.5119e-03,  2.9593e-03,\n",
      "         -2.3047e-03,  2.1662e-03,  6.5534e-04, -4.4474e-04, -5.0866e-03,\n",
      "          0.0000e+00, -1.3418e-03, -1.4046e-03,  2.1866e-03,  6.8542e-04,\n",
      "         -2.4408e-03, -2.1722e-03, -1.0315e-03, -3.0589e-03,  4.4086e-04,\n",
      "         -3.9366e-03,  4.5859e-03, -4.0436e-03,  8.4137e-04,  1.2340e-03,\n",
      "         -2.7861e-03, -2.3563e-03, -0.0000e+00,  4.5371e-03,  1.1907e-03,\n",
      "          7.2815e-04, -1.3254e-03,  0.0000e+00,  2.6687e-03, -4.4157e-03,\n",
      "          6.1103e-04,  5.1397e-04,  2.9182e-03, -1.0163e-03, -1.6749e-03,\n",
      "          7.6175e-04, -3.1587e-03, -1.7784e-04,  1.0649e-03, -8.6413e-05,\n",
      "          3.3093e-03, -1.7608e-03,  0.0000e+00,  1.4806e-03, -3.7148e-05,\n",
      "          5.9813e-04,  4.4877e-03,  3.2061e-04,  3.2594e-03,  0.0000e+00,\n",
      "         -1.9271e-03,  7.6146e-04,  2.4513e-03,  0.0000e+00,  2.5148e-04,\n",
      "          5.3401e-04,  3.7072e-03, -2.6413e-03,  0.0000e+00, -1.6432e-03,\n",
      "         -2.9644e-03,  7.6713e-04, -3.7407e-04, -1.5080e-03,  2.3245e-03,\n",
      "          3.1246e-03, -2.2134e-03,  8.4664e-04,  2.1299e-03, -0.0000e+00,\n",
      "          2.6250e-03,  2.1775e-03, -2.6895e-03,  4.5068e-03,  2.2694e-03,\n",
      "          0.0000e+00,  9.8644e-04,  1.4281e-04, -1.8776e-04, -9.2471e-04,\n",
      "         -9.3079e-04,  1.2204e-03,  2.0276e-03,  3.5346e-03,  1.7991e-03,\n",
      "          9.0160e-04, -1.2031e-03, -1.2078e-03,  7.3553e-04,  1.2841e-03,\n",
      "          2.2425e-04, -1.2653e-03, -0.0000e+00, -1.3532e-03,  2.6842e-04,\n",
      "         -4.8379e-04, -1.4528e-03, -1.6838e-03,  2.3192e-03, -4.7701e-03,\n",
      "          0.0000e+00, -7.7770e-04,  7.1780e-04, -1.8742e-04,  3.7439e-03,\n",
      "          2.0534e-03, -7.9394e-04,  0.0000e+00,  2.9282e-03,  3.1403e-03,\n",
      "          1.2361e-03, -1.0660e-03, -2.6567e-03, -4.5729e-04, -1.8658e-03,\n",
      "         -2.3165e-03,  1.8631e-03,  9.6929e-04, -3.4680e-04,  6.5924e-04,\n",
      "         -2.7847e-03, -1.2619e-03,  1.1629e-03, -1.1375e-03, -9.5212e-04,\n",
      "          2.9844e-04, -1.1809e-03,  3.6088e-04, -3.8618e-04,  2.7912e-03,\n",
      "          3.4400e-03, -2.1794e-03,  3.2280e-04, -1.8081e-03,  6.6177e-04,\n",
      "         -1.3861e-03, -1.2636e-04,  0.0000e+00,  0.0000e+00,  2.1020e-03,\n",
      "         -2.8594e-03, -0.0000e+00, -2.2586e-03,  2.1696e-04, -1.8174e-03,\n",
      "          2.5619e-04, -2.1245e-03,  1.8484e-03,  2.9691e-03, -1.4110e-03,\n",
      "          2.5493e-03, -4.8771e-05,  4.4284e-03, -1.1277e-03, -1.0903e-03,\n",
      "          0.0000e+00, -3.2742e-03, -1.7298e-03,  4.1257e-03, -4.1981e-03,\n",
      "          1.7568e-03, -1.7850e-03, -3.6778e-03,  7.0125e-04, -2.8513e-03,\n",
      "         -2.8609e-03, -2.1449e-03, -5.1244e-04,  2.7172e-04, -0.0000e+00,\n",
      "         -2.8307e-03,  1.8495e-04,  2.4235e-03, -4.5150e-05, -2.5259e-03,\n",
      "         -0.0000e+00, -1.4034e-05, -4.7616e-04, -5.4932e-04,  3.7538e-04,\n",
      "          3.7309e-03,  6.0026e-05, -1.4559e-03,  0.0000e+00, -1.0559e-03,\n",
      "          2.6397e-03, -3.9193e-03, -1.0147e-03, -1.9974e-03, -2.3256e-03,\n",
      "         -1.5395e-03,  0.0000e+00,  3.4087e-03, -7.1430e-04,  6.1650e-04,\n",
      "         -1.9668e-05,  9.8486e-04,  0.0000e+00,  2.9670e-03, -1.7776e-04,\n",
      "          2.1513e-03,  1.2376e-03,  0.0000e+00, -5.0424e-04,  2.5174e-03,\n",
      "         -2.0928e-04,  4.7244e-04, -9.4751e-04, -3.6303e-03,  9.5582e-04,\n",
      "         -5.9725e-04,  1.0897e-05,  3.6421e-03,  1.3040e-03, -2.5920e-03,\n",
      "          3.5252e-03,  0.0000e+00,  4.4798e-03, -4.3955e-04,  4.1672e-04,\n",
      "          2.4198e-03, -2.0269e-06, -1.3764e-03, -1.1421e-03,  2.0169e-03,\n",
      "         -2.7343e-03, -3.2750e-04,  2.7712e-05,  6.7746e-03,  1.5790e-03,\n",
      "         -5.7889e-04,  1.4934e-03, -1.6641e-04, -1.2618e-03,  1.7394e-03,\n",
      "          0.0000e+00,  5.2879e-04, -9.1587e-04, -0.0000e+00, -3.1599e-04,\n",
      "         -5.0545e-03,  3.2770e-03,  3.3067e-03,  3.4961e-03,  6.6525e-04,\n",
      "         -2.9276e-03,  7.7938e-04, -8.1667e-04, -0.0000e+00, -2.0907e-03,\n",
      "         -9.1199e-04, -1.9919e-03, -0.0000e+00,  2.2768e-03, -7.5587e-04,\n",
      "          6.3448e-04,  7.3420e-05,  1.3729e-03, -0.0000e+00, -4.1918e-03,\n",
      "          3.9968e-03, -0.0000e+00, -1.3637e-03, -8.5154e-04,  1.4552e-03,\n",
      "         -3.3131e-03, -2.2355e-03,  3.3940e-03,  1.0503e-03,  2.6826e-04,\n",
      "         -3.6932e-04, -2.5686e-03,  0.0000e+00, -1.3816e-03,  7.4101e-04,\n",
      "          2.2728e-03,  2.6813e-04, -3.5677e-03, -1.2991e-03, -3.6850e-03,\n",
      "          1.0319e-03, -7.7866e-04,  2.3821e-03,  1.1232e-03,  6.9687e-04,\n",
      "          2.5895e-03,  2.3088e-04, -1.0969e-03,  1.2427e-03,  2.7537e-04,\n",
      "          1.4771e-03, -1.4737e-03, -3.6301e-03,  0.0000e+00, -2.7182e-04,\n",
      "         -7.7534e-04, -0.0000e+00,  4.4116e-04,  1.7309e-03,  2.6848e-03,\n",
      "         -1.2101e-03, -3.2082e-03,  2.2001e-03, -1.1705e-03, -2.2919e-03,\n",
      "          1.7366e-03, -2.3831e-03,  2.1806e-03, -2.1033e-04, -2.2359e-04,\n",
      "          2.6386e-03,  3.9350e-04, -8.0484e-04,  6.1558e-04, -6.7208e-04,\n",
      "         -3.4304e-03,  3.1402e-03,  2.0162e-03,  2.2227e-03,  9.0153e-04,\n",
      "          3.1009e-03,  9.4589e-04,  2.2610e-04,  3.3628e-03, -5.1629e-04,\n",
      "         -3.4155e-03,  2.9789e-03, -1.5606e-03,  1.8730e-04, -2.4018e-03,\n",
      "          6.6563e-05, -2.9532e-03, -1.6460e-04, -2.5208e-03,  2.5378e-03,\n",
      "          1.9179e-04, -1.6690e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-5.7404e-05,  1.8034e-04, -2.8576e-04,  ...,  1.8815e-04,\n",
      "         -2.6365e-04, -2.3233e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-5.7404e-05,  1.8034e-04, -2.8576e-04,  ...,  1.8815e-04,\n",
      "         -2.6365e-04, -2.3233e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-2.6991e-04, -1.2708e-03, -3.4322e-04,  8.3629e-03, -5.9254e-06,\n",
      "          3.9211e-03, -3.5721e-03, -6.4397e-03,  6.8392e-03,  9.7165e-03,\n",
      "          1.1127e-03, -2.5308e-03,  1.1998e-03,  1.1714e-03,  0.0000e+00,\n",
      "          0.0000e+00,  3.2063e-03,  4.2407e-03, -5.1976e-03,  1.4582e-03,\n",
      "          4.1991e-04,  2.7759e-03,  6.5178e-04, -6.0670e-03,  0.0000e+00,\n",
      "          1.0906e-03, -1.3909e-03, -1.6933e-03, -4.9897e-04, -4.4038e-03,\n",
      "          3.0979e-04,  2.6868e-03,  2.1129e-03, -2.3807e-03, -5.2208e-03,\n",
      "         -3.8885e-03,  0.0000e+00, -5.8661e-04,  7.7040e-04,  4.5711e-04,\n",
      "          9.7712e-04,  9.1094e-03, -0.0000e+00,  2.1146e-03, -5.4584e-03,\n",
      "         -1.1304e-03, -3.8004e-03,  0.0000e+00,  1.4457e-03, -3.4136e-03,\n",
      "          2.5770e-03,  1.2519e-03, -0.0000e+00,  2.7847e-03,  0.0000e+00,\n",
      "         -3.2946e-04,  0.0000e+00, -3.5337e-03,  9.4473e-03, -0.0000e+00,\n",
      "          2.4200e-03, -8.1310e-03,  4.4472e-03, -5.4097e-03, -8.0258e-04,\n",
      "         -2.2609e-03,  5.8203e-03, -9.8634e-04,  0.0000e+00,  8.2908e-03,\n",
      "         -1.6874e-03,  5.4670e-03,  2.4172e-03,  2.3633e-03, -5.2189e-03,\n",
      "          3.2993e-03,  6.4097e-03, -7.9971e-03,  2.3189e-03, -0.0000e+00,\n",
      "          8.6218e-04, -1.9208e-03,  1.6302e-03, -6.8595e-03,  3.0974e-03,\n",
      "         -7.3950e-03,  1.5770e-03, -1.1592e-03, -6.2801e-03,  2.3427e-03,\n",
      "          3.8784e-03,  4.0023e-03,  1.0232e-03,  6.7321e-03, -1.4473e-04,\n",
      "          5.4863e-03,  6.6090e-03,  5.1537e-03,  5.5681e-03,  9.5478e-04,\n",
      "         -2.0550e-06,  3.9193e-04, -0.0000e+00, -5.4180e-03, -4.1852e-03,\n",
      "         -4.3940e-03, -2.7224e-03,  4.2055e-03, -4.4951e-03,  7.3179e-03,\n",
      "         -8.2991e-04,  0.0000e+00,  7.6689e-03, -2.3934e-03,  2.4456e-03,\n",
      "         -1.1481e-03, -1.6204e-03,  1.2368e-03,  0.0000e+00,  1.5728e-03,\n",
      "          7.2806e-04,  3.4286e-03,  2.0093e-03, -2.2902e-03, -2.2561e-03,\n",
      "         -3.3371e-03,  5.0628e-03, -7.4348e-03, -2.2428e-03, -5.6516e-03,\n",
      "         -2.9727e-03, -5.7999e-04, -5.1028e-03,  8.1727e-04, -3.3982e-03,\n",
      "         -4.5100e-03,  1.1689e-03,  8.1638e-04,  8.9725e-04,  1.3735e-03,\n",
      "          2.1220e-03, -6.0254e-03,  1.8049e-03, -7.6330e-04,  4.8286e-03,\n",
      "          1.0210e-03,  3.4699e-03,  4.9240e-03, -0.0000e+00,  0.0000e+00,\n",
      "          2.1432e-04, -2.3901e-04, -1.6550e-03, -3.9300e-03, -1.5778e-03,\n",
      "          0.0000e+00, -3.3819e-03,  0.0000e+00, -1.2904e-03, -0.0000e+00,\n",
      "          1.8271e-03, -5.0667e-03, -1.2223e-03,  3.3357e-03, -1.2257e-04,\n",
      "         -1.0381e-03, -6.8007e-03,  3.2799e-03,  5.2112e-03,  3.4228e-04,\n",
      "         -9.8231e-03, -1.2522e-03, -4.1754e-04, -6.4953e-03,  5.2981e-03,\n",
      "         -4.9889e-03,  3.3281e-03, -5.4701e-04, -1.1387e-03, -7.4988e-03,\n",
      "          0.0000e+00, -6.3715e-04, -2.2673e-03,  3.7334e-03,  1.6076e-03,\n",
      "         -4.2288e-03, -1.2993e-03,  6.9785e-04, -5.1249e-03,  1.2206e-03,\n",
      "         -8.2201e-03,  8.8824e-03, -8.3213e-03, -5.5281e-04,  3.8960e-04,\n",
      "         -2.7399e-03, -5.8711e-03, -0.0000e+00,  7.0986e-03, -1.1184e-03,\n",
      "          9.8762e-04, -5.3883e-03,  0.0000e+00,  3.5934e-03, -8.4668e-03,\n",
      "          5.5657e-04,  2.2639e-03,  6.1640e-03, -6.4979e-04, -3.9923e-03,\n",
      "          2.4297e-03, -8.1785e-03, -8.8641e-05,  7.7304e-04, -1.5526e-03,\n",
      "          7.1673e-03, -4.6421e-03,  0.0000e+00,  2.0105e-03,  1.0210e-03,\n",
      "          2.5969e-03,  6.9973e-03,  9.5017e-04,  6.5019e-03,  0.0000e+00,\n",
      "         -2.5859e-03,  4.4740e-03,  5.6011e-03,  0.0000e+00,  2.9147e-04,\n",
      "          2.4635e-03,  7.6342e-03, -3.1891e-03,  0.0000e+00, -2.9168e-03,\n",
      "         -5.2404e-03,  4.0968e-03, -3.4612e-04, -4.1226e-03,  4.7620e-03,\n",
      "          7.7036e-03, -7.6851e-03,  4.3535e-04,  4.4850e-03,  0.0000e+00,\n",
      "          4.1582e-03,  5.0633e-03, -5.9149e-03,  6.4539e-03,  3.4998e-03,\n",
      "          0.0000e+00,  2.1264e-03,  7.9531e-04, -4.1731e-04, -1.6150e-03,\n",
      "         -2.6800e-03,  9.6857e-04,  4.1508e-03,  7.5212e-03,  1.1790e-03,\n",
      "          1.1090e-03, -1.2242e-03, -3.7901e-03,  5.4054e-04,  2.6450e-03,\n",
      "          3.3458e-04, -3.0698e-04,  0.0000e+00, -1.1880e-03,  1.7927e-03,\n",
      "         -6.4547e-04, -1.5849e-03, -1.2541e-03,  4.2492e-03, -8.8633e-03,\n",
      "          0.0000e+00, -9.7045e-04,  1.0942e-03,  1.0007e-04,  6.8810e-03,\n",
      "          3.6263e-03, -3.9007e-03,  0.0000e+00,  6.6148e-03,  4.9705e-03,\n",
      "          2.5472e-04,  1.3067e-03, -6.8361e-03,  1.0100e-03, -3.9836e-03,\n",
      "         -5.3885e-03,  4.2719e-03,  1.8233e-03, -1.3502e-03, -4.8496e-05,\n",
      "         -3.9657e-03, -4.9247e-03,  3.0271e-04, -1.6886e-03, -4.6852e-04,\n",
      "         -3.2715e-04, -3.6958e-03,  9.4716e-04, -3.6557e-03,  2.6161e-03,\n",
      "          7.0865e-03, -1.7248e-03,  1.4995e-03, -3.2970e-03,  1.3896e-03,\n",
      "         -5.0801e-03,  1.1635e-03,  0.0000e+00,  0.0000e+00,  2.8814e-03,\n",
      "         -5.6862e-03,  0.0000e+00, -4.6810e-03,  1.9343e-03, -3.5212e-03,\n",
      "         -3.9782e-04, -2.8241e-03,  1.6245e-03,  7.9427e-03, -1.4574e-03,\n",
      "          3.7093e-03, -2.7786e-04,  6.8588e-03, -2.4195e-03, -2.2360e-03,\n",
      "          0.0000e+00, -5.5082e-03, -1.4450e-03,  5.4595e-03, -8.3021e-03,\n",
      "          3.4592e-03, -2.4906e-03, -4.9013e-03,  2.8996e-03, -4.6942e-03,\n",
      "         -5.3464e-03, -4.0114e-03, -6.8739e-05,  1.9553e-03, -0.0000e+00,\n",
      "         -4.7297e-03,  1.8586e-03,  6.1713e-03, -1.2346e-03, -3.0512e-03,\n",
      "         -0.0000e+00,  4.6390e-04, -6.6263e-04, -1.4020e-03, -2.4769e-03,\n",
      "          5.9244e-03,  2.7102e-04, -2.2852e-03,  0.0000e+00, -6.6990e-04,\n",
      "          2.8098e-03, -7.6354e-03, -3.7076e-03, -4.7705e-03, -4.6274e-03,\n",
      "         -2.4633e-03,  0.0000e+00,  8.4996e-03, -8.3103e-04, -1.3081e-03,\n",
      "         -8.6893e-04,  3.7473e-04,  0.0000e+00,  4.0981e-03,  2.8865e-06,\n",
      "          3.2133e-03,  2.4291e-03,  0.0000e+00, -1.5364e-03,  2.2642e-03,\n",
      "         -7.6986e-04, -1.7685e-04, -1.9624e-03, -6.8897e-03, -3.1918e-04,\n",
      "         -1.0909e-03,  4.3105e-04,  5.8410e-03, -5.9278e-04, -3.5685e-03,\n",
      "          7.7907e-03,  0.0000e+00,  6.9224e-03,  1.1993e-03,  4.3315e-05,\n",
      "          5.1311e-03,  2.5444e-04, -5.9648e-04, -2.6680e-03,  4.3632e-03,\n",
      "         -4.7493e-03,  1.8273e-03,  1.0071e-03,  9.7074e-03,  4.4745e-03,\n",
      "         -2.0765e-03,  1.9749e-03,  1.5384e-05, -3.3479e-03,  4.8635e-03,\n",
      "          0.0000e+00, -6.7215e-04, -1.0867e-03, -0.0000e+00, -1.2075e-03,\n",
      "         -7.2335e-03,  5.9683e-03,  5.9938e-03,  5.2045e-03,  2.4623e-03,\n",
      "         -4.9040e-03, -9.3457e-05, -1.3183e-03, -0.0000e+00, -7.1815e-03,\n",
      "         -3.1472e-03, -4.9546e-03, -0.0000e+00,  6.0502e-03, -2.6636e-03,\n",
      "          2.4549e-03,  8.1274e-04,  5.1494e-04, -0.0000e+00, -5.2328e-03,\n",
      "          6.6364e-03, -0.0000e+00, -2.7042e-03, -3.3882e-03,  2.4554e-03,\n",
      "         -8.9712e-03, -3.7909e-03,  4.2875e-03, -4.6566e-04, -2.9737e-04,\n",
      "          4.1238e-04, -4.7958e-03, -0.0000e+00, -3.2354e-03, -8.6557e-04,\n",
      "          1.3930e-03,  3.2231e-03, -6.7562e-03, -4.3602e-03, -9.1344e-03,\n",
      "          1.3890e-03, -3.0235e-03,  1.0907e-03,  3.9723e-03,  1.8444e-03,\n",
      "          4.7718e-03, -1.9841e-03, -2.7358e-03,  2.8079e-03, -2.4223e-03,\n",
      "          3.9127e-03, -2.3377e-03, -4.7603e-03,  0.0000e+00,  2.1309e-04,\n",
      "         -1.7379e-03, -0.0000e+00,  2.9777e-03,  3.0300e-03,  4.0092e-03,\n",
      "         -1.1317e-03, -5.6977e-03,  3.9617e-03, -3.3834e-03, -4.2830e-03,\n",
      "          1.9597e-03, -6.2841e-03,  2.8789e-03, -3.3343e-04,  3.5469e-04,\n",
      "          4.0657e-03,  9.7613e-05, -1.6015e-03,  4.2477e-04, -7.7029e-04,\n",
      "         -7.5641e-03,  5.5254e-03,  3.1706e-03,  2.0020e-03,  2.8131e-03,\n",
      "          4.8673e-03,  1.9320e-04,  6.5326e-04,  3.3724e-03, -1.9548e-03,\n",
      "         -5.4528e-03,  6.3700e-03, -2.7362e-03,  2.1371e-03, -1.6269e-03,\n",
      "          3.5184e-03, -4.1821e-03,  7.0604e-04, -3.7930e-03,  2.7375e-03,\n",
      "          1.2680e-03, -4.6114e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0003,  0.0005, -0.0007,  ...,  0.0008, -0.0009, -0.0008]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0003,  0.0005, -0.0007,  ...,  0.0008, -0.0009, -0.0008]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-4.1754e-04, -2.3260e-03, -1.4207e-03,  1.1732e-02,  2.3008e-04,\n",
      "          5.5905e-03, -3.1977e-03, -7.4668e-03,  8.5434e-03,  1.2617e-02,\n",
      "          1.6389e-03, -2.5221e-03,  1.2382e-03,  2.4139e-04,  0.0000e+00,\n",
      "          0.0000e+00,  5.0229e-03,  5.5707e-03, -5.7153e-03,  2.7294e-03,\n",
      "         -9.3059e-04,  3.0213e-03,  2.0462e-04, -7.5442e-03,  0.0000e+00,\n",
      "          1.2036e-03, -3.0039e-04, -8.6027e-04, -1.2124e-03, -6.2613e-03,\n",
      "          1.8948e-03,  9.2658e-04,  1.7505e-03, -2.6946e-03, -7.1574e-03,\n",
      "         -3.2266e-03,  0.0000e+00, -1.4024e-03,  1.5836e-03,  4.7092e-04,\n",
      "          2.3576e-04,  1.1737e-02, -0.0000e+00,  4.4454e-03, -6.1565e-03,\n",
      "          1.0552e-03, -5.4158e-03,  0.0000e+00,  8.9478e-04, -2.8464e-03,\n",
      "          1.6138e-03,  1.3464e-03,  0.0000e+00,  2.7483e-03,  0.0000e+00,\n",
      "         -2.1983e-03,  0.0000e+00, -5.0897e-03,  1.0892e-02, -0.0000e+00,\n",
      "          2.6992e-03, -1.0188e-02,  6.5267e-03, -7.4652e-03, -2.3709e-03,\n",
      "         -7.8493e-04,  7.1599e-03, -2.2394e-03,  0.0000e+00,  7.8565e-03,\n",
      "         -1.1683e-03,  7.0748e-03,  4.0429e-03,  5.7242e-03, -6.7395e-03,\n",
      "          3.6749e-03,  7.6243e-03, -9.4139e-03,  2.3470e-03, -0.0000e+00,\n",
      "          8.2584e-04, -3.7041e-03,  1.8979e-03, -9.0684e-03,  1.4685e-03,\n",
      "         -1.0348e-02,  1.1622e-03, -9.9964e-04, -7.0587e-03,  2.5447e-03,\n",
      "          6.5033e-03,  6.2591e-03,  1.1001e-03,  7.7527e-03,  7.3160e-04,\n",
      "          5.5355e-03,  1.0157e-02,  5.2103e-03,  6.9082e-03,  4.7544e-04,\n",
      "          1.6824e-03,  1.5513e-03, -0.0000e+00, -4.9277e-03, -4.7510e-03,\n",
      "         -5.6293e-03, -3.8394e-03,  4.3502e-03, -6.4137e-03,  8.8873e-03,\n",
      "         -1.0734e-03,  0.0000e+00,  8.9188e-03, -3.8611e-03,  2.7296e-03,\n",
      "         -8.9722e-04,  4.6009e-04,  5.8359e-04,  0.0000e+00, -2.4662e-04,\n",
      "          7.9621e-04,  5.2145e-03, -5.5545e-05, -1.8261e-03, -2.3536e-03,\n",
      "         -5.6331e-03,  7.4036e-03, -8.5705e-03, -3.4740e-03, -6.7368e-03,\n",
      "         -2.2136e-03,  1.6692e-03, -4.5125e-03,  3.0578e-03, -4.9507e-03,\n",
      "         -5.6679e-03,  1.8175e-03,  1.0070e-03,  1.5881e-03,  4.0016e-03,\n",
      "          2.9412e-03, -7.9556e-03,  8.4815e-04,  9.5910e-04,  4.9476e-03,\n",
      "          6.1665e-04,  3.9436e-03,  5.1802e-03, -0.0000e+00,  0.0000e+00,\n",
      "         -1.0821e-03, -1.5401e-03, -1.0915e-03, -4.4154e-03, -3.0941e-03,\n",
      "          0.0000e+00, -5.6358e-03,  0.0000e+00, -1.2220e-03, -0.0000e+00,\n",
      "          2.1104e-03, -4.7092e-03, -3.2087e-03,  5.9328e-03, -4.5660e-04,\n",
      "         -1.2884e-03, -8.2973e-03,  4.4876e-03,  5.4276e-03, -9.0565e-04,\n",
      "         -1.3046e-02, -9.0045e-04,  7.8451e-04, -8.3248e-03,  5.7431e-03,\n",
      "         -4.6153e-03,  2.4674e-03,  2.2393e-04, -6.9856e-04, -1.0113e-02,\n",
      "          0.0000e+00, -4.2007e-04, -4.1529e-03,  5.8109e-03,  2.4122e-03,\n",
      "         -5.9030e-03, -1.1954e-03,  7.1492e-04, -4.4533e-03,  6.9194e-04,\n",
      "         -1.0511e-02,  1.1643e-02, -9.9807e-03, -1.1728e-03,  1.7377e-03,\n",
      "         -4.5307e-03, -7.7758e-03, -0.0000e+00,  4.6336e-03,  4.3091e-04,\n",
      "          1.7544e-03, -5.3687e-03,  0.0000e+00,  5.9765e-03, -1.1234e-02,\n",
      "          1.6147e-03,  3.2429e-03,  8.1228e-03, -1.5414e-03, -5.3360e-03,\n",
      "          3.1936e-03, -8.2429e-03, -9.3696e-04,  1.9689e-03, -3.8467e-03,\n",
      "          5.1802e-03, -5.8475e-03,  0.0000e+00,  1.1264e-03,  1.2485e-03,\n",
      "          2.5592e-03,  8.8492e-03,  6.7732e-04,  8.8926e-03,  0.0000e+00,\n",
      "         -2.1698e-03,  3.6903e-03,  6.0182e-03,  0.0000e+00,  3.8635e-04,\n",
      "          1.6550e-03,  8.9558e-03, -4.2780e-03,  0.0000e+00, -3.4013e-03,\n",
      "         -5.3541e-03,  4.3556e-03, -8.5287e-04, -3.9470e-03,  5.3010e-03,\n",
      "          9.2790e-03, -1.1888e-02,  1.3888e-03,  5.1388e-03, -0.0000e+00,\n",
      "          5.8874e-03,  6.3180e-03, -8.5945e-03,  1.0003e-02,  5.6546e-03,\n",
      "          0.0000e+00,  3.8957e-03,  2.3508e-03, -2.1001e-03, -2.7559e-04,\n",
      "         -1.4315e-03,  1.7265e-03,  5.7122e-03,  8.0728e-03,  8.6171e-04,\n",
      "          9.3726e-04, -1.0022e-03, -2.9630e-03,  7.0749e-04,  2.7347e-03,\n",
      "          7.6843e-05, -1.8610e-03, -0.0000e+00, -1.4902e-03,  3.4317e-03,\n",
      "         -3.4928e-04, -2.7470e-03, -3.1452e-03,  3.3142e-03, -1.0065e-02,\n",
      "          0.0000e+00, -3.6022e-03,  2.0651e-03,  2.2345e-03,  1.0075e-02,\n",
      "          4.3663e-03, -5.6956e-03,  0.0000e+00,  8.0907e-03,  6.8919e-03,\n",
      "          1.4799e-03,  1.3844e-03, -7.2087e-03,  1.8836e-03, -5.0853e-03,\n",
      "         -7.9116e-03,  6.1875e-03,  8.9520e-04, -2.1830e-03,  2.3501e-04,\n",
      "         -6.7832e-03, -6.5954e-03, -1.0815e-03, -9.2599e-04, -1.2133e-03,\n",
      "         -3.5160e-04, -4.4271e-03,  2.1705e-04, -3.6065e-03,  4.6742e-03,\n",
      "          1.0353e-02, -2.6690e-03,  1.7580e-04, -2.4187e-03,  2.9844e-03,\n",
      "         -7.5813e-03,  2.8441e-03,  0.0000e+00,  0.0000e+00,  5.0643e-03,\n",
      "         -6.1109e-03,  0.0000e+00, -4.6886e-03,  2.1545e-03, -3.7275e-03,\n",
      "         -1.9070e-03, -3.4060e-03,  2.9489e-03,  9.2557e-03, -1.9939e-03,\n",
      "          1.8564e-03,  1.4283e-03,  8.8296e-03, -2.4085e-03, -3.2609e-03,\n",
      "          0.0000e+00, -7.2396e-03, -3.2005e-04,  7.0221e-03, -9.8931e-03,\n",
      "          2.6757e-03, -2.0465e-03, -5.4630e-03,  5.1114e-03, -4.9866e-03,\n",
      "         -7.4282e-03, -5.4278e-03,  9.1959e-04,  2.0516e-03, -0.0000e+00,\n",
      "         -5.7625e-03,  2.0072e-03,  8.9286e-03, -2.2558e-03, -5.3615e-03,\n",
      "         -0.0000e+00, -9.7496e-05, -1.4676e-03, -1.4355e-03, -3.2036e-03,\n",
      "          7.3050e-03, -1.8156e-04, -2.4178e-03,  0.0000e+00, -1.1140e-03,\n",
      "          3.7020e-03, -9.9244e-03, -4.0697e-03, -5.2773e-03, -5.5745e-03,\n",
      "         -1.9829e-03,  0.0000e+00,  9.6753e-03, -1.1777e-03, -1.7768e-03,\n",
      "         -1.1564e-03, -6.1917e-05,  0.0000e+00,  5.2716e-03,  1.8299e-03,\n",
      "          5.1879e-03,  3.6906e-03,  0.0000e+00, -1.3368e-03,  4.0265e-03,\n",
      "          4.9636e-04, -7.3237e-04, -4.4675e-03, -9.5760e-03, -3.6492e-04,\n",
      "         -2.7559e-03,  1.4767e-03,  7.0132e-03, -4.1063e-04, -4.8984e-03,\n",
      "          1.0375e-02,  0.0000e+00,  7.2197e-03,  2.8816e-03,  1.1079e-03,\n",
      "          6.2599e-03,  1.0454e-03, -1.1078e-03, -2.8100e-03,  4.2871e-03,\n",
      "         -5.5714e-03,  1.1992e-03, -4.0993e-04,  1.2650e-02,  5.2801e-03,\n",
      "         -3.1058e-03,  2.9343e-03,  8.0501e-04, -3.7773e-03,  4.5247e-03,\n",
      "         -0.0000e+00, -1.5571e-03, -1.2231e-04,  0.0000e+00, -2.3044e-03,\n",
      "         -7.3922e-03,  8.7493e-03,  8.9262e-03,  6.4068e-03,  3.8926e-03,\n",
      "         -7.6803e-03, -1.5976e-03, -9.1789e-04, -0.0000e+00, -8.5341e-03,\n",
      "         -3.8477e-03, -4.2581e-03, -0.0000e+00,  5.9073e-03, -3.9932e-03,\n",
      "          2.9814e-03,  2.2935e-03,  3.5201e-03, -0.0000e+00, -5.3033e-03,\n",
      "          8.3071e-03, -0.0000e+00, -3.6882e-03, -5.1320e-03,  3.2810e-03,\n",
      "         -1.0318e-02, -5.0014e-03,  5.1620e-03, -7.5693e-04, -4.9391e-04,\n",
      "          2.3545e-04, -5.6427e-03, -0.0000e+00, -4.7146e-03, -4.2890e-04,\n",
      "          2.1920e-03,  6.5917e-03, -7.7900e-03, -6.5962e-03, -1.0859e-02,\n",
      "          1.2071e-03, -3.0742e-03,  1.4156e-03,  6.5713e-03,  2.0574e-03,\n",
      "          5.8700e-03, -2.2034e-03, -2.7629e-03,  4.4133e-03, -2.0315e-03,\n",
      "          1.6559e-03, -3.9436e-03, -5.8887e-03,  0.0000e+00,  3.2719e-04,\n",
      "          5.4739e-04,  0.0000e+00,  3.4893e-03,  2.6765e-03,  5.4816e-03,\n",
      "         -1.9471e-04, -5.8943e-03,  4.9766e-03, -2.8850e-03, -5.1748e-03,\n",
      "          2.9879e-03, -7.9244e-03,  5.1059e-03, -4.6918e-04,  2.7199e-03,\n",
      "          5.7884e-03, -1.0387e-03, -1.8422e-03,  4.3051e-04, -2.0188e-03,\n",
      "         -6.5313e-03,  5.3263e-03,  4.1938e-03,  3.0783e-03,  1.5771e-03,\n",
      "          6.3679e-03,  5.3139e-05, -1.0177e-03,  2.9279e-03, -6.6508e-04,\n",
      "         -6.6898e-03,  7.0483e-03, -2.1500e-03,  2.9253e-03, -4.4385e-03,\n",
      "          5.3461e-03, -4.5199e-03,  2.9695e-04, -4.5724e-03,  2.9469e-03,\n",
      "          2.3950e-03, -5.9411e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0003,  0.0004, -0.0010,  ...,  0.0016, -0.0016, -0.0015],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0003,  0.0004, -0.0010,  ...,  0.0016, -0.0016, -0.0015],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0001, -0.0012, -0.0020,  ...,  0.0024,  0.0021, -0.0070],\n",
      "        [ 0.0000,  0.0009, -0.0009,  ...,  0.0025,  0.0002, -0.0017]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 3.3549e-04,  2.5873e-04, -1.2586e-03,  ...,  2.3950e-03,\n",
      "         -1.9740e-03, -1.8429e-03],\n",
      "        [ 3.3928e-05,  2.7866e-04, -2.0871e-04,  ...,  8.5441e-05,\n",
      "         -1.7599e-04, -2.7287e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 3.3549e-04,  2.5873e-04, -1.2586e-03,  ...,  2.3950e-03,\n",
      "         -1.9740e-03, -1.8429e-03],\n",
      "        [ 3.3928e-05,  2.7866e-04, -2.0871e-04,  ...,  8.5441e-05,\n",
      "         -1.7599e-04, -2.7287e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0004, -0.0003, -0.0030,  ...,  0.0013,  0.0020, -0.0081],\n",
      "        [ 0.0000,  0.0018, -0.0016,  ...,  0.0034,  0.0022, -0.0029],\n",
      "        [ 0.0007,  0.0000, -0.0009,  ...,  0.0025,  0.0002, -0.0017]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 5.6821e-04,  3.6758e-04, -1.4060e-03,  ...,  3.2012e-03,\n",
      "         -2.3637e-03, -2.0656e-03],\n",
      "        [ 8.0515e-05,  7.8313e-04, -5.8448e-04,  ...,  4.4184e-04,\n",
      "         -5.6494e-04, -6.3455e-05],\n",
      "        [ 9.2662e-05,  1.1199e-04, -1.6335e-04,  ...,  2.2572e-04,\n",
      "         -2.8887e-04, -1.1944e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 5.6821e-04,  3.6758e-04, -1.4060e-03,  ...,  3.2012e-03,\n",
      "         -2.3637e-03, -2.0656e-03],\n",
      "        [ 8.0515e-05,  7.8313e-04, -5.8448e-04,  ...,  4.4184e-04,\n",
      "         -5.6494e-04, -6.3455e-05],\n",
      "        [ 9.2662e-05,  1.1199e-04, -1.6335e-04,  ...,  2.2572e-04,\n",
      "         -2.8887e-04, -1.1944e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-7.6583e-04, -1.2392e-03, -3.6754e-03,  ...,  2.3942e-03,\n",
      "          1.6204e-03, -8.3115e-03],\n",
      "        [-0.0000e+00,  1.0264e-03, -4.2945e-05,  ...,  3.4536e-03,\n",
      "          2.1379e-03, -4.8635e-03],\n",
      "        [-5.3208e-04,  0.0000e+00, -2.3397e-03,  ...,  2.5189e-03,\n",
      "          3.9369e-03, -2.8339e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0004,  0.0007, -0.0016,  ...,  0.0035, -0.0026, -0.0024],\n",
      "        [ 0.0005,  0.0012, -0.0007,  ...,  0.0010, -0.0007, -0.0003],\n",
      "        [ 0.0003,  0.0002, -0.0003,  ...,  0.0007, -0.0010, -0.0004]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0004,  0.0007, -0.0016,  ...,  0.0035, -0.0026, -0.0024],\n",
      "        [ 0.0005,  0.0012, -0.0007,  ...,  0.0010, -0.0007, -0.0003],\n",
      "        [ 0.0003,  0.0002, -0.0003,  ...,  0.0007, -0.0010, -0.0004]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0001, -0.0011, -0.0025,  ...,  0.0020,  0.0023, -0.0096],\n",
      "        [-0.0000,  0.0010, -0.0008,  ...,  0.0032,  0.0025, -0.0053],\n",
      "        [ 0.0012,  0.0000, -0.0021,  ...,  0.0027,  0.0044, -0.0033]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 2.8394e-04,  8.5452e-04, -1.9163e-03,  ...,  3.8936e-03,\n",
      "         -2.8754e-03, -2.7428e-03],\n",
      "        [ 8.2409e-04,  1.3889e-03, -9.1994e-04,  ...,  1.7161e-03,\n",
      "         -1.1121e-03, -6.3944e-04],\n",
      "        [ 4.6588e-04,  2.6274e-05, -4.8736e-04,  ...,  1.2698e-03,\n",
      "         -1.5529e-03, -5.8614e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 2.8394e-04,  8.5452e-04, -1.9163e-03,  ...,  3.8936e-03,\n",
      "         -2.8754e-03, -2.7428e-03],\n",
      "        [ 8.2409e-04,  1.3889e-03, -9.1994e-04,  ...,  1.7161e-03,\n",
      "         -1.1121e-03, -6.3944e-04],\n",
      "        [ 4.6588e-04,  2.6274e-05, -4.8736e-04,  ...,  1.2698e-03,\n",
      "         -1.5529e-03, -5.8614e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0006, -0.0020, -0.0028,  ...,  0.0026,  0.0031, -0.0104],\n",
      "        [ 0.0000,  0.0022, -0.0030,  ...,  0.0030,  0.0021, -0.0044],\n",
      "        [ 0.0001,  0.0000, -0.0019,  ...,  0.0027,  0.0039, -0.0049]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 2.4909e-04,  7.2061e-04, -2.3331e-03,  ...,  4.3528e-03,\n",
      "         -2.9076e-03, -2.9558e-03],\n",
      "        [ 1.0444e-03,  1.2612e-03, -1.0155e-03,  ...,  2.2208e-03,\n",
      "         -1.7461e-03, -9.6869e-04],\n",
      "        [ 9.4120e-04,  3.3317e-06, -1.0312e-03,  ...,  2.1288e-03,\n",
      "         -2.1739e-03, -9.6352e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 2.4909e-04,  7.2061e-04, -2.3331e-03,  ...,  4.3528e-03,\n",
      "         -2.9076e-03, -2.9558e-03],\n",
      "        [ 1.0444e-03,  1.2612e-03, -1.0155e-03,  ...,  2.2208e-03,\n",
      "         -1.7461e-03, -9.6869e-04],\n",
      "        [ 9.4120e-04,  3.3317e-06, -1.0312e-03,  ...,  2.1288e-03,\n",
      "         -2.1739e-03, -9.6352e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0017, -0.0019, -0.0022,  ...,  0.0023,  0.0022, -0.0101],\n",
      "        [ 0.0000,  0.0015, -0.0022,  ...,  0.0029,  0.0019, -0.0055],\n",
      "        [-0.0002,  0.0000, -0.0011,  ...,  0.0016,  0.0042, -0.0048]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 0 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-6.2347e-05,  6.1063e-04, -2.6934e-03,  ...,  4.7033e-03,\n",
      "         -3.0298e-03, -3.0434e-03],\n",
      "        [ 1.1751e-03,  1.1402e-03, -1.2165e-03,  ...,  2.5743e-03,\n",
      "         -2.1641e-03, -9.8163e-04],\n",
      "        [ 1.4436e-03, -1.3383e-04, -1.5989e-03,  ...,  3.0046e-03,\n",
      "         -2.7079e-03, -1.2106e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-6.2347e-05,  6.1063e-04, -2.6934e-03,  ...,  4.7033e-03,\n",
      "         -3.0298e-03, -3.0434e-03],\n",
      "        [ 1.1751e-03,  1.1402e-03, -1.2165e-03,  ...,  2.5743e-03,\n",
      "         -2.1641e-03, -9.8163e-04],\n",
      "        [ 1.4436e-03, -1.3383e-04, -1.5989e-03,  ...,  3.0046e-03,\n",
      "         -2.7079e-03, -1.2106e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0005, -0.0015, -0.0009,  ...,  0.0021,  0.0017, -0.0105],\n",
      "        [ 0.0000,  0.0007, -0.0009,  ...,  0.0026,  0.0013, -0.0073],\n",
      "        [-0.0008,  0.0000, -0.0002,  ...,  0.0018,  0.0030, -0.0066]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[ 7.4705e-04,  1.3877e-03,  1.1235e-04,  ...,  2.1415e-04,\n",
      "          -3.1266e-04,  1.8048e-03],\n",
      "         [-4.5942e-04,  1.4379e-03,  4.9720e-05,  ..., -5.2312e-04,\n",
      "          -3.1393e-04,  1.0721e-03],\n",
      "         [-0.0000e+00,  1.3571e-03,  8.2477e-04,  ..., -4.0681e-04,\n",
      "          -3.1035e-04,  8.4927e-04]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[-0.0002,  0.0004, -0.0029,  ...,  0.0051, -0.0031, -0.0031],\n",
      "         [ 0.0013,  0.0009, -0.0013,  ...,  0.0030, -0.0025, -0.0010],\n",
      "         [ 0.0017, -0.0004, -0.0019,  ...,  0.0038, -0.0031, -0.0014]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "stacked_sequence_output.shape = torch.Size([2, 3, 10, 1024])\n",
      "final_states = (tensor([[[ 0.0000,  0.0011, -0.0009,  ...,  0.0021,  0.0017, -0.0105],\n",
      "         [ 0.0049,  0.0010, -0.0017,  ...,  0.0026,  0.0013, -0.0073],\n",
      "         [ 0.0045, -0.0001, -0.0016,  ...,  0.0018,  0.0030, -0.0066]],\n",
      "\n",
      "        [[-0.0002,  0.0011,  0.0000,  ...,  0.0002, -0.0003,  0.0018],\n",
      "         [-0.0007, -0.0005, -0.0005,  ..., -0.0005, -0.0003,  0.0011],\n",
      "         [-0.0009, -0.0004, -0.0002,  ..., -0.0004, -0.0003,  0.0008]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>), tensor([[[-1.5307e-02, -2.2940e-03,  1.9252e-02,  ...,  1.8517e-02,\n",
      "          -6.0162e-02, -5.8304e-03],\n",
      "         [-9.8792e-03, -6.4998e-03,  1.0482e-02,  ...,  1.8837e-02,\n",
      "          -5.9147e-02, -8.3583e-03],\n",
      "         [-1.0465e-02, -4.3223e-03,  9.5679e-03,  ...,  2.1493e-02,\n",
      "          -6.2026e-02, -9.6677e-03]],\n",
      "\n",
      "        [[-2.6428e-05,  1.2669e-03, -1.7266e-04,  ...,  5.0580e-03,\n",
      "          -3.0605e-03, -3.1318e-03],\n",
      "         [ 2.7449e-03,  1.8384e-03,  9.2600e-04,  ...,  2.9555e-03,\n",
      "          -2.5362e-03, -1.0311e-03],\n",
      "         [ 2.2733e-03,  4.9089e-04, -1.4160e-03,  ...,  3.8366e-03,\n",
      "          -3.1327e-03, -1.4149e-03]]], device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "stacked\n",
      "num_layers = 2\n",
      "num_valid = 3\n",
      "returned_timesteps = 10\n",
      "encoder_dim = 1024\n",
      "num_valid < batch_size -> False\n",
      "sequence_length_difference = total_sequence_length - returned_timesteps\n",
      "sequence_length_difference = 0\n",
      "sequence_length_difference is larger than 0? : False\n",
      "UPDATE STATES... inputs: final_states, restoration_indices\n",
      "_EncoderBase의 `_update_states` 메서드 실행\n",
      "inputs:\n",
      "final_states = (tensor([[[ 0.0000,  0.0011, -0.0009,  ...,  0.0021,  0.0017, -0.0105],\n",
      "         [ 0.0049,  0.0010, -0.0017,  ...,  0.0026,  0.0013, -0.0073],\n",
      "         [ 0.0045, -0.0001, -0.0016,  ...,  0.0018,  0.0030, -0.0066]],\n",
      "\n",
      "        [[-0.0002,  0.0011,  0.0000,  ...,  0.0002, -0.0003,  0.0018],\n",
      "         [-0.0007, -0.0005, -0.0005,  ..., -0.0005, -0.0003,  0.0011],\n",
      "         [-0.0009, -0.0004, -0.0002,  ..., -0.0004, -0.0003,  0.0008]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>), tensor([[[-1.5307e-02, -2.2940e-03,  1.9252e-02,  ...,  1.8517e-02,\n",
      "          -6.0162e-02, -5.8304e-03],\n",
      "         [-9.8792e-03, -6.4998e-03,  1.0482e-02,  ...,  1.8837e-02,\n",
      "          -5.9147e-02, -8.3583e-03],\n",
      "         [-1.0465e-02, -4.3223e-03,  9.5679e-03,  ...,  2.1493e-02,\n",
      "          -6.2026e-02, -9.6677e-03]],\n",
      "\n",
      "        [[-2.6428e-05,  1.2669e-03, -1.7266e-04,  ...,  5.0580e-03,\n",
      "          -3.0605e-03, -3.1318e-03],\n",
      "         [ 2.7449e-03,  1.8384e-03,  9.2600e-04,  ...,  2.9555e-03,\n",
      "          -2.5362e-03, -1.0311e-03],\n",
      "         [ 2.2733e-03,  4.9089e-04, -1.4160e-03,  ...,  3.8366e-03,\n",
      "          -3.1327e-03, -1.4149e-03]]], device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "new_unsorted_states = [tensor([[[ 0.0049,  0.0010, -0.0017,  ...,  0.0026,  0.0013, -0.0073],\n",
      "         [ 0.0045, -0.0001, -0.0016,  ...,  0.0018,  0.0030, -0.0066],\n",
      "         [ 0.0000,  0.0011, -0.0009,  ...,  0.0021,  0.0017, -0.0105]],\n",
      "\n",
      "        [[-0.0007, -0.0005, -0.0005,  ..., -0.0005, -0.0003,  0.0011],\n",
      "         [-0.0009, -0.0004, -0.0002,  ..., -0.0004, -0.0003,  0.0008],\n",
      "         [-0.0002,  0.0011,  0.0000,  ...,  0.0002, -0.0003,  0.0018]]],\n",
      "       device='cuda:0', grad_fn=<IndexSelectBackward>), tensor([[[-9.8792e-03, -6.4998e-03,  1.0482e-02,  ...,  1.8837e-02,\n",
      "          -5.9147e-02, -8.3583e-03],\n",
      "         [-1.0465e-02, -4.3223e-03,  9.5679e-03,  ...,  2.1493e-02,\n",
      "          -6.2026e-02, -9.6677e-03],\n",
      "         [-1.5307e-02, -2.2940e-03,  1.9252e-02,  ...,  1.8517e-02,\n",
      "          -6.0162e-02, -5.8304e-03]],\n",
      "\n",
      "        [[ 2.7449e-03,  1.8384e-03,  9.2600e-04,  ...,  2.9555e-03,\n",
      "          -2.5362e-03, -1.0311e-03],\n",
      "         [ 2.2733e-03,  4.9089e-04, -1.4160e-03,  ...,  3.8366e-03,\n",
      "          -3.1327e-03, -1.4149e-03],\n",
      "         [-2.6428e-05,  1.2669e-03, -1.7266e-04,  ...,  5.0580e-03,\n",
      "          -3.0605e-03, -3.1318e-03]]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)]\n",
      "self._states is None = True\n",
      "이전 상태가 존재하지 않습니다. new_unsorted_states로 새롭게 만들어 줍니다.\n",
      "STATES: (tensor([[[ 0.0049,  0.0010, -0.0017,  ...,  0.0026,  0.0013, -0.0073],\n",
      "         [ 0.0045, -0.0001, -0.0016,  ...,  0.0018,  0.0030, -0.0066],\n",
      "         [ 0.0000,  0.0011, -0.0009,  ...,  0.0021,  0.0017, -0.0105]],\n",
      "\n",
      "        [[-0.0007, -0.0005, -0.0005,  ..., -0.0005, -0.0003,  0.0011],\n",
      "         [-0.0009, -0.0004, -0.0002,  ..., -0.0004, -0.0003,  0.0008],\n",
      "         [-0.0002,  0.0011,  0.0000,  ...,  0.0002, -0.0003,  0.0018]]],\n",
      "       device='cuda:0'), tensor([[[-9.8792e-03, -6.4998e-03,  1.0482e-02,  ...,  1.8837e-02,\n",
      "          -5.9147e-02, -8.3583e-03],\n",
      "         [-1.0465e-02, -4.3223e-03,  9.5679e-03,  ...,  2.1493e-02,\n",
      "          -6.2026e-02, -9.6677e-03],\n",
      "         [-1.5307e-02, -2.2940e-03,  1.9252e-02,  ...,  1.8517e-02,\n",
      "          -6.0162e-02, -5.8304e-03]],\n",
      "\n",
      "        [[ 2.7449e-03,  1.8384e-03,  9.2600e-04,  ...,  2.9555e-03,\n",
      "          -2.5362e-03, -1.0311e-03],\n",
      "         [ 2.2733e-03,  4.9089e-04, -1.4160e-03,  ...,  3.8366e-03,\n",
      "          -3.1327e-03, -1.4149e-03],\n",
      "         [-2.6428e-05,  1.2669e-03, -1.7266e-04,  ...,  5.0580e-03,\n",
      "          -3.0605e-03, -3.1318e-03]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(token_embedding, Variable(masks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD!!!!**************\n",
      "batch_size = 3\n",
      "total_sequence_length = 10\n",
      "_EncoderBase.sort_and_run_forward 메서드 실시...\n",
      "\tbatch_size = 3, num_valid = 3\n",
      "\tsequence_lengths = tensor([ 7,  6, 10], device='cuda:0')\n",
      "\t1. sorted_inputs.shape = torch.Size([3, 10, 512])\n",
      "\t2. sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "\t3. restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "\t4. sorting_indices = tensor([2, 0, 1], device='cuda:0')\n",
      "\t             sorted_inputs.shape  = torch.Size([3, 10, 512])\n",
      "\tpacked_sequence_input.data.shape  = torch.Size([23, 512])\n",
      "\tpacked_sequence_input.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n",
      "\tself.stateful is True\n",
      "\tstateful is True,\n",
      "\t\tConduct `_get_initial_states`\n",
      "\tRUN `_lstm_forward`... by initial_states\n",
      "\t\tinitial_state is None? False\n",
      "\t\tinitial is not None and it's size equal to forward_layers' length,\n",
      "\t\tthen hidden_states is\n",
      "\t\t A = initial_state[0].split(1, 0) = (tensor([[[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., -0., 0.,  ..., -0., 0., 0.]]], device='cuda:0'), tensor([[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.]]], device='cuda:0'))\n",
      "\t\t B = initial_state[1].split(1, 0) = (tensor([[[-0.0352,  0.0154,  0.0243,  ..., -0.0686, -0.0066,  0.0070],\n",
      "         [-0.0378,  0.0112,  0.0200,  ..., -0.0543, -0.0072,  0.0046],\n",
      "         [-0.0366,  0.0168,  0.0212,  ..., -0.0568, -0.0052,  0.0023]]],\n",
      "       device='cuda:0'), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))\n",
      "\t\t hidden_states = list(zip(A, B))\n",
      "\t\t               = [(tensor([[[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., -0., 0.,  ..., -0., 0., 0.]]], device='cuda:0'), tensor([[[-0.0352,  0.0154,  0.0243,  ..., -0.0686, -0.0066,  0.0070],\n",
      "         [-0.0378,  0.0112,  0.0200,  ..., -0.0543, -0.0072,  0.0046],\n",
      "         [-0.0366,  0.0168,  0.0212,  ..., -0.0568, -0.0052,  0.0023]]],\n",
      "       device='cuda:0')), (tensor([[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.]]], device='cuda:0'), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))]\n",
      "\t\tinputs is `PackedSequence`\n",
      "\t\ttype(inputs) = <class 'torch.nn.utils.rnn.PackedSequence'>\n",
      "\t\t\tinputs.data.shape = torch.Size([23, 512])\n",
      "\t\t\tinputs.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n",
      "\t\t\tinputs.sorted_indices = None\n",
      "\t\t\tinputs.unsorted_indices = None\n",
      "\t\tRestore PAD_char to inputs...\n",
      "\t\t바뀐 inputs의 정보 출력\n",
      "\t\ttype(inputs) = <class 'torch.Tensor'>\n",
      "\t\t\tinputs.shape = torch.Size([3, 10, 512])\n",
      "\t\tbatch_lengths = tensor([10,  7,  6])\n",
      "\t\tAssign forward_output_sequence = backward_output_sequence = inputs\n",
      "\t\tSet final_states, sequqnce_outputs as empty list, []\n",
      "\t\tGet a forward layer and backward layer at layer 1\n",
      "\t\tCaching...: output_sequence to cache both forward and backward\n",
      "\t\tstate is None? False\n",
      "\t\t\tAlright, Set hidden_state/memory_state for both forward and backward\n",
      "\t\t\tstate[0](hidden_state) = tensor([[[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., -0., 0.,  ..., -0., 0., 0.]]], device='cuda:0')\n",
      "\t\t\tstate[1](memory_state) = tensor([[[-0.0352,  0.0154,  0.0243,  ..., -0.0686, -0.0066,  0.0070],\n",
      "         [-0.0378,  0.0112,  0.0200,  ..., -0.0543, -0.0072,  0.0046],\n",
      "         [-0.0366,  0.0168,  0.0212,  ..., -0.0568, -0.0052,  0.0023]]],\n",
      "       device='cuda:0')\n",
      "\t\tGet a forward layer and backward layer at layer 2\n",
      "\t\tCaching...: output_sequence to cache both forward and backward\n",
      "\t\tstate is None? False\n",
      "\t\t\tAlright, Set hidden_state/memory_state for both forward and backward\n",
      "\t\t\tstate[0](hidden_state) = tensor([[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.]]], device='cuda:0')\n",
      "\t\t\tstate[1](memory_state) = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "stacked_sequence_output.shape = torch.Size([2, 3, 10, 1024])\n",
      "final_states = (tensor([[[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., -0., 0.,  ..., -0., 0., 0.]],\n",
      "\n",
      "        [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward>), tensor([[[-0.0367,  0.0161,  0.0254,  ..., -0.0716, -0.0068,  0.0073],\n",
      "         [-0.0419,  0.0125,  0.0223,  ..., -0.0604, -0.0080,  0.0051],\n",
      "         [-0.0421,  0.0193,  0.0245,  ..., -0.0655, -0.0060,  0.0027]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "stacked\n",
      "num_layers = 2\n",
      "num_valid = 3\n",
      "returned_timesteps = 10\n",
      "encoder_dim = 1024\n",
      "num_valid < batch_size -> False\n",
      "sequence_length_difference = total_sequence_length - returned_timesteps\n",
      "sequence_length_difference = 0\n",
      "sequence_length_difference is larger than 0? : False\n",
      "UPDATE STATES... inputs: final_states, restoration_indices\n",
      "_EncoderBase의 `_update_states` 메서드 실행\n",
      "inputs:\n",
      "final_states = (tensor([[[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., -0., 0.,  ..., -0., 0., 0.]],\n",
      "\n",
      "        [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.]]], device='cuda:0',\n",
      "       grad_fn=<CatBackward>), tensor([[[-0.0367,  0.0161,  0.0254,  ..., -0.0716, -0.0068,  0.0073],\n",
      "         [-0.0419,  0.0125,  0.0223,  ..., -0.0604, -0.0080,  0.0051],\n",
      "         [-0.0421,  0.0193,  0.0245,  ..., -0.0655, -0.0060,  0.0027]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "new_unsorted_states = [tensor([[[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., -0., 0.,  ..., -0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., -0., 0., 0.]],\n",
      "\n",
      "        [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "         [-0., -0., -0.,  ..., -0., -0., -0.]]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>), tensor([[[-0.0419,  0.0125,  0.0223,  ..., -0.0604, -0.0080,  0.0051],\n",
      "         [-0.0421,  0.0193,  0.0245,  ..., -0.0655, -0.0060,  0.0027],\n",
      "         [-0.0367,  0.0161,  0.0254,  ..., -0.0716, -0.0068,  0.0073]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0', grad_fn=<IndexSelectBackward>)]\n",
      "self._states is None = False\n",
      "이전 상태가 존재합니다. 현재 상태와 입력받은 final_state로 새로운 상태를 update합니다.\n",
      "current_state_batch_size = 3 = self._states[0].size(1)\n",
      "new_state_batch_size = 3 = final_states[0].size(1)\n"
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(token_embedding, Variable(masks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
