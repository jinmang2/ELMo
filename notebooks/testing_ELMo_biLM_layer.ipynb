{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# *~ coding convention ~*\n",
    "from overrides import overrides\n",
    "from typing import Callable\n",
    "\n",
    "# Python Standard Library\n",
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Python Installed Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction: dict to namedtuple\n",
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)\n",
    "\n",
    "# input your directories path\n",
    "model_dir = 'C:\\workspace\\implement_elmo\\elmo\\configs'\n",
    "args2 = dict2namedtuple(\n",
    "    json.load(\n",
    "        codecs.open(\n",
    "            os.path.join(model_dir, 'config.json'), \n",
    "            'r', encoding='utf-8')\n",
    "    )\n",
    ")\n",
    "\n",
    "# args2.config_path == 'cnn_50_100_512_4096_sample.json'\n",
    "\n",
    "# load config\n",
    "with open(os.path.join(model_dir, args2.config_path), 'r') as fin:\n",
    "    config = json.load(fin)\n",
    "    \n",
    "token_embedding = torch.load('token_embedding.pt') \n",
    "masks = [torch.load(f'mask[{ix}].pt') for ix in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size = 512\n",
      "hidden_size = 512\n",
      "cell_size = 4096\n",
      "num_layers = 2\n",
      "memory_cell_clip_value = 3\n",
      "state_projection_clip_value = 3\n",
      "recurrent_dropout_probability = 0.1\n"
     ]
    }
   ],
   "source": [
    "# _EncoderBase\n",
    "stateful = False\n",
    "_states = None\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "input_size = config['encoder']['projection_dim']\n",
    "hidden_size = config['encoder']['projection_dim']\n",
    "cell_size = config['encoder']['dim']\n",
    "num_layers = config['encoder']['n_layers']\n",
    "memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "state_projection_clip_value = config['encoder']['proj_clip']\n",
    "recurrent_dropout_probability = config['dropout']\n",
    "\n",
    "print(f\"input_size = {input_size}\")\n",
    "print(f\"hidden_size = {hidden_size}\")\n",
    "print(f\"cell_size = {cell_size}\")\n",
    "print(f\"num_layers = {num_layers}\")\n",
    "print(f\"memory_cell_clip_value = {memory_cell_clip_value}\")\n",
    "print(f\"state_projection_clip_value = {state_projection_clip_value}\")\n",
    "print(f\"recurrent_dropout_probability = {config['dropout']}\")\n",
    "\n",
    "forward_layers = []\n",
    "backward_layers = []\n",
    "\n",
    "lstm_input_size = input_size\n",
    "go_forward = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-de9662a3a37d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable\n",
    "import logging\n",
    "import itertools\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def get_lengths_from_binary_sequence_mask(mask: torch.Tensor):\n",
    "    return mask.long().sum(-1)\n",
    "\n",
    "def get_dropout_mask(dropout_probability: float,\n",
    "                     tensor_for_masking: Variable):\n",
    "    print('*-*** get_dropout_mask ***-*')\n",
    "    binary_mask = tensor_for_masking.clone()\n",
    "    print('binary_mask', binary_mask)\n",
    "    binary_mask.data.copy_(torch.rand(tensor_for_masking.size()) > dropout_probability)\n",
    "    print(f'binary_mask = {torch.rand(tensor_for_masking.size()) > dropout_probability}')\n",
    "    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)\n",
    "    print(f\"Calc 1.0 / (1 - p) or 0.0\")\n",
    "    print(f\"dropout_mask = {dropout_mask}\")\n",
    "    print('*-*** ---------------- ***-*')\n",
    "    return dropout_mask\n",
    "\n",
    "def block_orthogonal(tensor: torch.Tensor,\n",
    "                     split_sizes: List[int],\n",
    "                     gain: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    An initializer which allows initaliizing model parametes in \"block\".\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, Variable):\n",
    "    # in pytorch 4.0, Variable equals Tensor\n",
    "    #     block_orthogonal(tensor.data, split_sizes, gain)\n",
    "    # else:\n",
    "        sizes = list(tensor.size())\n",
    "        if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n",
    "            raise ConfigurationError(\n",
    "                \"tensor dimentions must be divisible by their respective \"\n",
    "                f\"split_sizes. Found size: {size} and split_sizes: {split_sizes}\")\n",
    "        indexes = [list(range(0, max_size, split))\n",
    "                   for max_size, split in zip(sizes, split_sizes)]\n",
    "        # Iterate over all possible blocks within the tensor.\n",
    "        for block_start_indices in itertools.product(*indexes):\n",
    "            index_and_step_tuples = zip(block_start_indices, split_sizes)\n",
    "            block_slice = tuple([slice(start_index, start_index + step)\n",
    "                                 for start_index, step in index_and_step_tuples])\n",
    "            tensor[block_slice] = nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)\n",
    "            \n",
    "def sort_batch_by_length(tensor: torch.autograd.Variable,\n",
    "                         sequence_lengths: torch.autograd.Variable):\n",
    "    if not isinstance(tensor, Variable) or not isinstance(sequence_lengths, Variable):\n",
    "        raise Exception(\"Both the tensor and sequence lengths must be torch.autograd.Variables.\")\n",
    "        \n",
    "    sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n",
    "    sorted_tensor = tensor.index_select(0, permutation_index)\n",
    "    \n",
    "    # This is ugly, but required - we are creating a new variable at runtime, so we\n",
    "    # must ensure it has the correct CUDA vs non-CUDA type. We do this by cloning and\n",
    "    # refilling one of the inputs to the function.\n",
    "    index_range = sequence_lengths.data.clone().copy_(torch.arange(0, len(sequence_lengths)))\n",
    "    # This is the equivalent of zipping with index, sorting by the original\n",
    "    # sequence lengths and returning the now sorted indices.\n",
    "    index_range = Variable(index_range.long())\n",
    "    _, reverse_mapping = permutation_index.sort(0, descending=False)\n",
    "    restoration_indices = index_range.index_select(0, reverse_mapping)\n",
    "    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아직 코드 리뷰안한 코드!\n",
    "from typing import Optional, Tuple, List, Callable, Union\n",
    "\n",
    "import h5py\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# We have two types here for the state, because storing the state in something\n",
    "# which is Iterable (like a tuple, below), is helpful for internal manipulation\n",
    "# - however, the states are consumed as either Tensors or a Tuple of Tensors, so\n",
    "# returning them in this format is unhelpful.\n",
    "RnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]  # pylint: disable=invalid-name\n",
    "RnnStateStorage = Tuple[torch.Tensor, ...]  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "class _EncoderBase(nn.Module):\n",
    "    # pyling: disable=abstract-method\n",
    "    \"\"\"\n",
    "    This abstract class serves as a base for the 3 ``Encoder`` abstractions in AllenNLP.\n",
    "    - :class:`~allennlp.modules.seq2seq_encoders.Seq2SeqEncoders`\n",
    "    - :class:`~allennlp.modules.seq2vec_encoders.Seq2VecEncoders`\n",
    "    Additionally, this class provides functionality for sorting sequences by length\n",
    "    so they can be consumed by Pytorch RNN classes, which require their inputs to be\n",
    "    sorted by length. Finally, it also provides optional statefulness to all of it's\n",
    "    subclasses by allowing the caching and retrieving of the hidden states of RNNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, stateful: bool = False) -> None:\n",
    "        super(_EncoderBase, self).__init__()\n",
    "        self.stateful = stateful\n",
    "        self._states: Optional[RnnStateStorage] = None\n",
    "\n",
    "    def sort_and_run_forward(self,\n",
    "                             module: Callable[[PackedSequence, Optional[RnnState]],\n",
    "                                              Tuple[Union[PackedSequence, torch.Tensor], RnnState]],\n",
    "                             inputs: torch.Tensor,\n",
    "                             mask: torch.Tensor,\n",
    "                             hidden_state: Optional[RnnState] = None):\n",
    "        \"\"\"\n",
    "        Pytorch RNNs는 input이 passing되기 전에 정렬되있어야 함\n",
    "        Seq2xxxEncoders가 이러한 기능을 모두 사용하기에 base class로 제공\n",
    "        \"\"\"\n",
    "        # In some circumstances you may have sequences of zero length. ``pack_padded_sequence``\n",
    "        # requires all sequence lengths to be > 0, so remove sequences of zero length before\n",
    "        # calling self._module, then fill with zeros.\n",
    "\n",
    "        # First count how many sequences are empty.\n",
    "        batch_size = mask.size(0)\n",
    "        num_valid = torch.sum(mask[:, 0]).int().item()\n",
    "        print(f\"\\tbatch_size = {batch_size}, num_valid = {num_valid}\")\n",
    "\n",
    "        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "        print(f\"\\tsequence_lengths = {sequence_lengths}\")\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n",
    "            sort_batch_by_length(inputs, sequence_lengths)\n",
    "        print(f\"\\t1. sorted_inputs.shape = {sorted_inputs.shape}\")\n",
    "        print(f\"\\t2. sorted_sequence_lengths = {sorted_sequence_lengths}\")\n",
    "        print(f\"\\t3. restoration_indices = {restoration_indices}\")\n",
    "        print(f\"\\t4. sorting_indices = {sorting_indices}\")\n",
    "        # Now create a PackedSequence with only the non-empty, sorted sequences.\n",
    "        # pad token 제외, 유의미한 값들만 packing\n",
    "        packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n",
    "                                                     sorted_sequence_lengths[:num_valid].data.tolist(),\n",
    "                                                     batch_first=True)\n",
    "        print(f\"\\t             sorted_inputs.shape  = {sorted_inputs.shape}\")\n",
    "        print(f\"\\tpacked_sequence_input.data.shape  = {packed_sequence_input.data.shape}\")\n",
    "        print(f\"\\tpacked_sequence_input.batch_sizes = {packed_sequence_input.batch_sizes}\")\n",
    "        # Prepare teh initial states.\n",
    "        print(f\"\\tself.stateful is {self.stateful}\")\n",
    "        if not self.stateful:\n",
    "            print(\"\\tstateful is False,\", end='')\n",
    "            print(\"If hidden_state is \", end='')\n",
    "            if hidden_state == None:\n",
    "                print(\"None,\\n\\t\\tinitial_states = hidden_state\")\n",
    "                initial_states = hidden_state\n",
    "            elif isinstance(hidden_state, tuple):\n",
    "                print(\"tuple,\\n\\t\\tinitial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :] for state in hidden_state]\")\n",
    "                initial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :]\n",
    "                                  for state in hidden_state]\n",
    "            else:\n",
    "                print(\"not both None and tuple,\\n\\t\\tConduct `_get_initial_states`\")\n",
    "                initial_states = self._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "        else:\n",
    "            print(\"\\tstateful is True,\\n\\t\\tConduct `_get_initial_states`\")\n",
    "            initial_states = self._get_initial_states(batch_size, num_valid, sorting_indices)\n",
    "\n",
    "        # Actually call the module on the sorted PackedSequence\n",
    "        print(\"\\tRUN `_lstm_forward`... by initial_states\")\n",
    "        module_output, final_states = module(packed_sequence_input, initial_states)\n",
    "        \n",
    "\n",
    "        return module_output, final_states, restoration_indices\n",
    "\n",
    "    def _get_initial_states(self,\n",
    "                            batch_size: int,\n",
    "                            num_valid: int,\n",
    "                            sorting_indices: torch.LongTensor) -> Optional[RnnState]:\n",
    "        \"\"\"\n",
    "        RNN의 초기 상태를 반환\n",
    "        추가적으로, 이 메서드는 batch의 새로운 요소의 초기 상태를 추가하기 위해 상태를 변경하여(mutate)\n",
    "            호출시 batch size를 처리\n",
    "        또한 이 메서드는\n",
    "            1. 배치의 요소 seq. length로 상태를 정렬하는 것과\n",
    "            2. pad가 끝난 row 제거도 처리\n",
    "        중요한 것은 현재의 배치 크기가 이전에 호출되었을 때보다 더 크면 이 상태를 \"혼합\"하는 것이다.\n",
    "\n",
    "        이 메서드는 (1) 처음 호출되어 아무 상태가 없는 경우 (2) RNN이 heterogeneous state를 가질 때\n",
    "        의 경우를 처리해야 하기 때문에 return값이 복잡함\n",
    "\n",
    "        (1) module이 처음 호출됬을 때 ``module``의 타입이 무엇이든 ``None`` 반환\n",
    "        (2) Otherwise,\n",
    "            - LSTM의 경우 tuple of ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "                 and ``(num_layers, num_valid, memory_size)``\n",
    "            - GRU의 경우  single ``torch.Tensor``\n",
    "              shape: ``(num_layers, num_valid, state_size)``\n",
    "        \"\"\"\n",
    "        # We don't know the state sizes the first time calling forward,\n",
    "        # so we let the module define what it's initial hidden state looks like.\n",
    "        if self._states is None:\n",
    "            return None\n",
    "\n",
    "        # Otherwise, we have some previous states.\n",
    "        if batch_size > self._states[0].size(1):\n",
    "            # This batch is larger than the all previous states.\n",
    "            # If so, resize the states.\n",
    "            num_states_to_concat = batch_size - self._states[0].size(1)\n",
    "            resized_states = []\n",
    "            # state has shape (num_layers, batch_size, hidden_size)\n",
    "            for state in self._states:\n",
    "                # This _must_ be inside the loop because some\n",
    "                # RNNs have states with different last dimension sizes.\n",
    "                zeros = state.data.new(state.size(0),\n",
    "                                       num_states_to_concat,\n",
    "                                       state.size(2)).fill_(0)\n",
    "                zeros = Variable(zeros)\n",
    "                resized_states.append(torch.cat([state, zeros], 1))\n",
    "            self._states = tuple(resized_states)\n",
    "            correctly_shaped_states = self._states\n",
    "        elif batch_size < self._states[0].size(1):\n",
    "            # This batch is smaller than the previous one.\n",
    "            correctly_shaped_states = tuple(staet[:, :batch_size, :] for state in self._states)\n",
    "        else:\n",
    "            correctly_shaped_states = self._states\n",
    "\n",
    "        # At this point, out states are of shape (num_layers, batch_size, hidden_size).\n",
    "        # However, the encoder uses sorted sequences and additionally removes elements\n",
    "        # of the batch which are fully padded. We need the states to match up to these\n",
    "        # sorted and filtered sequences, so we do that in the next two blocks before\n",
    "        # returning the states.\n",
    "        if len(self._states) == 1:\n",
    "            # GRU\n",
    "            correctly_shaped_state = correctly_shaped_states[0]\n",
    "            sorted_state = correctly_shaped_state.index_select(1, sorting_indices)\n",
    "            return sorted_state[:, :num_valid, :]\n",
    "        else:\n",
    "            # LSTM\n",
    "            sorted_states = [state.index_select(1, sorting_indices)\n",
    "                             for state in correctly_shaped_states]\n",
    "            return tuple(state[:, :num_valid, :] for state in sorted_states)\n",
    "\n",
    "    def _update_states(self,\n",
    "                       final_states: RnnStateStorage,\n",
    "                       restoration_indices: torch.LongTensor) -> None:\n",
    "        \"\"\"\n",
    "        RNN forward 동작 후에 state를 update\n",
    "        새로운 state로 update하며 몇 가지 book-keeping을 실시\n",
    "        즉, 상태를 해제하고 완전히 padding된 state가 업데이트되지 않도록 함\n",
    "        마지막으로 graph가 매 batch iteration후에 gc되도록 계산 그래프에서\n",
    "        state variable을 떼어냄.\n",
    "        \"\"\"\n",
    "        # TODO(Mark)L seems weird to sort here, but append zeros in the subclasses.\n",
    "        # which way around is best?\n",
    "        print('_EncoderBase의 `_update_states` 메서드 실행')\n",
    "        print(f'inputs:\\nfinal_states = {final_states}\\nrestoration_indices = {restoration_indices}')\n",
    "        new_unsorted_states = [state.index_select(1, restoration_indices)\n",
    "                               for state in final_states]\n",
    "        print(f\"new_unsorted_states = {new_unsorted_states}\")\n",
    "        print(f\"self._states is None = {self._states is None}\")\n",
    "        if self._states is None:\n",
    "            print(\"이전 상태가 존재하지 않습니다. new_unsorted_states로 새롭게 만들어 줍니다.\")\n",
    "            # We don't already have states, so just set the\n",
    "            # ones we receive to be the current state.\n",
    "            self._states = tuple([Variable(state.data)\n",
    "                                  for state in new_unsorted_states])\n",
    "            print('STATES:', self._states)\n",
    "        else:\n",
    "            print(\"이전 상태가 존재합니다. 현재 상태와 입력받은 final_state로 새로운 상태를 update합니다.\")\n",
    "            # Now we've sorted the states back so that they correspond to the original\n",
    "            # indices, we need to figure out what states we need to update, because if we\n",
    "            # didn't use a state for a particular row, we want to preserve its state.\n",
    "            # Thankfully, the rows which are all zero in the state correspond exactly\n",
    "            # to those which aren't used, so we create masks of shape (new_batch_size,),\n",
    "            # denoting which states were used in the RNN computation.\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            print(f\"current_state_batch_size = {current_state_batch_size} = self._states[0].size(1)\")\n",
    "            print(f\"new_state_batch_size = {new_state_batch_size} = final_states[0].size(1)\")\n",
    "            # Masks for the unused states of shape (1, new_batch_size, 1)\n",
    "            used_new_rows_mask = [(state[0, :, :].sum(-1)\n",
    "                                   != 0.0).float().view(1, new_state_batch_size, 1)\n",
    "                                  for state in new_unsorted_states]\n",
    "            new_states = []\n",
    "            if current_state_batch_size > new_state_batch_size:\n",
    "                # The new state is smaller than the old one,\n",
    "                # so just update the indices which we used.\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows in the previous state\n",
    "                    # which _were_ used in the current state.\n",
    "                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(old_state.data))\n",
    "            else:\n",
    "                # The states are the same size, so we just have to\n",
    "                # deal with the possibility that some rows weren't used.\n",
    "                new_states = []\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # zero out all rows which _were_ used in the current state.\n",
    "                    masked_old_state = old_state * (1 - used_mask)\n",
    "                    # The old state is larger, so update the relevant parts of it.\n",
    "                    new_state += masked_old_state\n",
    "                    # Detatch the Variable.\n",
    "                    new_states.append(torch.autograd.Variable(new_state.data))\n",
    "\n",
    "            # It looks like there should be another case handled here - when\n",
    "            # the current_state_batch_size < new_state_batch_size. However,\n",
    "            # this never happens, because the states themeselves are mutated\n",
    "            # by appending zeros when calling _get_inital_states, meaning that\n",
    "            # the new states are either of equal size, or smaller, in the case\n",
    "            # that there are some unused elements (zero-length) for the RNN computation.\n",
    "            self._states = tuple(new_states)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._states = None\n",
    "\n",
    "\n",
    "class ElmobiLm(_EncoderBase):\n",
    "    def __init__(self, config, use_cuda=False):\n",
    "        super(ElmobiLm, self).__init__(stateful=True)\n",
    "        self.config = config\n",
    "        self.use_cuda = use_cuda\n",
    "        input_size = config['encoder']['projection_dim']\n",
    "        hidden_size = config['encoder']['projection_dim']\n",
    "        cell_size = config['encoder']['dim']\n",
    "        num_layers = config['encoder']['n_layers']\n",
    "        memory_cell_clip_value = config['encoder']['cell_clip']\n",
    "        state_projection_clip_value = config['encoder']['proj_clip']\n",
    "        recurrent_dropout_probability = config['dropout']\n",
    "        \n",
    "        print('ELMo biLM layer params')\n",
    "        print(f\"\\tinput_size = {input_size}\")\n",
    "        print(f\"\\thidden_size = {hidden_size}\")\n",
    "        print(f\"\\tcell_size = {cell_size}\")\n",
    "        print(f\"\\tnum_layers = {num_layers}\")\n",
    "        print(f\"\\tmemory_cell_clip_value = {memory_cell_clip_value}\")\n",
    "        print(f\"\\tstate_projection_clip_value = {state_projection_clip_value}\")\n",
    "#         print(f\"\\trecurrent_dropout_probability = {config['dropout']}\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        forward_layers = []\n",
    "        backward_layers = []\n",
    "\n",
    "        lstm_input_size = input_size\n",
    "        go_forward = True\n",
    "        for layer_index in range(num_layers):\n",
    "            forward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                                   hidden_size,\n",
    "                                                   cell_size,\n",
    "                                                   go_forward,\n",
    "                                                   recurrent_dropout_probability,\n",
    "                                                   memory_cell_clip_value,\n",
    "                                                   state_projection_clip_value).cuda()\n",
    "            backward_layer = LstmCellWithProjection(lstm_input_size,\n",
    "                                                    hidden_size,\n",
    "                                                    cell_size,\n",
    "                                                    not go_forward,\n",
    "                                                    recurrent_dropout_probability,\n",
    "                                                    memory_cell_clip_value,\n",
    "                                                    state_projection_clip_value).cuda()\n",
    "            if use_cuda:\n",
    "                forward_layer = forward_layer.cuda()\n",
    "                backward_layer = backward_layer.cuda()\n",
    "            lstm_input_size = hidden_size\n",
    "\n",
    "            self.add_module('forward_layer_{}'.format(layer_index), forward_layer)\n",
    "            self.add_module('backward_layer_{}'.format(layer_index), backward_layer)\n",
    "            forward_layers.append(forward_layer)\n",
    "            backward_layers.append(backward_layer)\n",
    "        self.forward_layers = forward_layers\n",
    "        self.backward_layers = backward_layers\n",
    "        print(f\"forward_layers = {forward_layers}\")\n",
    "        print(f\"backward_layers = {backward_layers}\")\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        print('FORWARD!!!!**************')\n",
    "        batch_size, total_sequence_length = mask.size()\n",
    "        print(f\"batch_size = {batch_size}\")\n",
    "        print(f\"total_sequence_length = {total_sequence_length}\")\n",
    "        print(\"_EncoderBase.sort_and_run_forward 메서드 실시...\")\n",
    "        stacked_sequence_output, final_states, restoration_indices = \\\n",
    "            self.sort_and_run_forward(self._lstm_forward, inputs, mask)\n",
    "        print(f\"stacked_sequence_output.shape = {stacked_sequence_output.shape}\")\n",
    "        print(f\"final_states = {final_states}\")\n",
    "        print(f\"restoration_indices = {restoration_indices}\")\n",
    "        num_layers, num_valid, returned_timesteps, encoder_dim = stacked_sequence_output.size()\n",
    "        # Add back invalid rows which were removed in the call to sort_and_run_forward.\n",
    "        print(\"stacked\")\n",
    "        print(f\"num_layers = {num_layers}\")\n",
    "        print(f\"num_valid = {num_valid}\")\n",
    "        print(f\"returned_timesteps = {returned_timesteps}\")\n",
    "        print(f\"encoder_dim = {encoder_dim}\")\n",
    "        print(f\"num_valid < batch_size -> {num_valid < batch_size}\")\n",
    "        if num_valid < batch_size:\n",
    "            zeros = stacked_sequence_output.data.new(num_layers,\n",
    "                                                     batch_size - num_valid,\n",
    "                                                     returned_timesteps,\n",
    "                                                     encoder_dim).fill_(0)\n",
    "            zeros = Variable(zeros)\n",
    "            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 1)\n",
    "\n",
    "            # The states also need to have invalid rows added back.\n",
    "            new_states = []\n",
    "            for state in final_states:\n",
    "                state_dim = state.size(-1)\n",
    "                zeros = state.data.new(num_layers, batch_size - num_valid, state_dim).fill_(0)\n",
    "                zeros = Variable(zeros)\n",
    "                new_states.append(torch.cat([state, zeros], 1))\n",
    "            final_states = new_states\n",
    "\n",
    "        # It's possible to need to pass sequences which are padded to longer than the\n",
    "        # max length of the sequence to a Seq2StackEncoder. However, packing and unpacking\n",
    "        # the sequences mean that the returned tensor won't include these dimensions, because\n",
    "        # the RNN did not need to process them. We add them back on in the form of zeros here.\n",
    "        sequence_length_difference = total_sequence_length - returned_timesteps\n",
    "        print(\"sequence_length_difference = total_sequence_length - returned_timesteps\")\n",
    "        print(f\"sequence_length_difference = {sequence_length_difference}\")\n",
    "        print(f\"sequence_length_difference is larger than 0? : {sequence_length_difference > 0}\")\n",
    "        if sequence_length_difference > 0:\n",
    "            zeros = stacked_sequence_output.data.new(num_layers,\n",
    "                                                     batch_size,\n",
    "                                                     sequence_length_difference,\n",
    "                                                     stacked_sequence_output[0].size(-1)).fill_(0)\n",
    "            zeros = Variable(zeros)\n",
    "            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)\n",
    "        print('UPDATE STATES... inputs: final_states, restoration_indices')\n",
    "        self._update_states(final_states, restoration_indices)\n",
    "\n",
    "        # Restore the original indices and return the sequence.\n",
    "        # Has shape (num_layers, batch_size, sequence_length, hidden_size)\n",
    "        return stacked_sequence_output.index_select(1, restoration_indices)\n",
    "\n",
    "\n",
    "    def _lstm_forward(self,\n",
    "                      inputs: PackedSequence,\n",
    "                      initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> \\\n",
    "        Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        print(f\"\\t\\tinitial_state is None? {initial_state is None}\")\n",
    "        if initial_state is None:\n",
    "            print(\"\\t\\tOops, Assign hidden_state = [None] * len(self.forward_layers)\")\n",
    "            hidden_states: List[Optional[Tuple[torch.Tensor,\n",
    "                                         torch.Tensor]]] = [None] * len(self.forward_layers)\n",
    "            print(f\"\\t\\thidden_states = {hidden_states}\")\n",
    "        elif initial_state[0].size()[0] != len(self.forward_layers):\n",
    "            print(f\"\\t\\tinitial_state[0].size()[0] = {initial_state[0].size()[0]}\")\n",
    "            print(f\"\\t\\tlen(self.forward_layers) = {len(self.forward_layers)}\")\n",
    "            raise Exception(\"Initial states were passed to forward() but the number of \"\n",
    "                            \"initial states does not match the number of layers.\")\n",
    "        else:\n",
    "            print(\"\\t\\tinitial is not None and it's size equal to forward_layers' length,\")\n",
    "            print(\"\\t\\tthen hidden_states is\")\n",
    "            print(f\"\\t\\t A = initial_state[0].split(1, 0) = {initial_state[0].split(1, 0)}\")\n",
    "            print(f\"\\t\\t B = initial_state[1].split(1, 0) = {initial_state[1].split(1, 0)}\")\n",
    "            print(\"\\t\\t hidden_states = list(zip(A, B))\")\n",
    "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
    "                                     initial_state[1].split(1, 0)))\n",
    "            print(f\"\\t\\t               = {hidden_states}\")\n",
    "        \n",
    "        print(\"\\t\\tinputs is `PackedSequence`\")\n",
    "        print(f\"\\t\\ttype(inputs) = {type(inputs)}\")\n",
    "        print(f\"\\t\\t\\tinputs.data.shape = {inputs.data.shape}\")\n",
    "        print(f\"\\t\\t\\tinputs.batch_sizes = {inputs.batch_sizes}\")\n",
    "        print(f\"\\t\\t\\tinputs.sorted_indices = {inputs.sorted_indices}\")\n",
    "        print(f\"\\t\\t\\tinputs.unsorted_indices = {inputs.unsorted_indices}\")\n",
    "        \n",
    "        print(\"\\t\\tRestore PAD_char to inputs...\")\n",
    "        inputs, batch_lengths = pad_packed_sequence(inputs, batch_first=True)\n",
    "        print(\"\\t\\t바뀐 inputs의 정보 출력\")\n",
    "        print(f\"\\t\\ttype(inputs) = {type(inputs)}\")\n",
    "        print(f\"\\t\\t\\tinputs.shape = {inputs.shape}\")\n",
    "        print(f\"\\t\\tbatch_lengths = {batch_lengths}\")\n",
    "        print(\"\\t\\tAssign forward_output_sequence = backward_output_sequence = inputs\")\n",
    "        forward_output_sequence = inputs\n",
    "        backward_output_sequence = inputs\n",
    "        \n",
    "        print(\"\\t\\tSet final_states, sequqnce_outputs as empty list, []\")\n",
    "        final_states = []\n",
    "        sequence_outputs = []\n",
    "        for layer_index, state in enumerate(hidden_states):\n",
    "            print(f\"\\t\\tGet a forward layer and backward layer at layer {layer_index+1}\")\n",
    "            forward_layer = getattr(self, 'forward_layer_{}'.format(layer_index))\n",
    "            backward_layer = getattr(self, 'backward_layer_{}'.format(layer_index))\n",
    "            \n",
    "            print(\"\\t\\tCaching...: output_sequence to cache both forward and backward\")\n",
    "            forward_cache = forward_output_sequence\n",
    "            backward_cache = backward_output_sequence\n",
    "            \n",
    "            print(f\"\\t\\tstate is None? {state is None}\")\n",
    "            if state is not None:\n",
    "                print(\"\\t\\t\\tAlright, Set hidden_state/memory_state for both forward and backward\")\n",
    "                print(f\"\\t\\t\\tstate[0](hidden_state) = {state[0]}\")\n",
    "                print(f\"\\t\\t\\tstate[1](memory_state) = {state[1]}\")\n",
    "                forward_hidden_state, backward_hidden_state = state[0].split(self.hidden_size, 2)\n",
    "                forward_memory_state, backward_memory_state = state[1].split(self.cell_size, 2)\n",
    "                forward_state = (forward_hidden_state, forward_memory_state)\n",
    "                backward_state = (backward_hidden_state, backward_memory_state)\n",
    "            else:\n",
    "                print(\"\\t\\t\\tOops, then forward and backward state is also 'None'\")\n",
    "                forward_state = None\n",
    "                backward_state = None\n",
    "                \n",
    "            print(\"\\t\\tRUN forward_layer.forward method...\")\n",
    "            forward_output_sequence, forward_state = forward_layer(forward_output_sequence,\n",
    "                                                                   batch_lengths,\n",
    "                                                                   forward_state)\n",
    "            print(\"\\t\\tRUN backward_layer.forward method...\")\n",
    "            backward_output_sequence, backward_state = backward_layer(backward_output_sequence,\n",
    "                                                                      batch_lengths,\n",
    "                                                                      backward_state)\n",
    "            # Skip connections, just adding the input to the output.\n",
    "            if layer_index != 0:\n",
    "                print('\\t\\tsince layer_index != 0, adding cache to output sequence')\n",
    "                forward_output_sequence += forward_cache\n",
    "                backward_output_sequence += backward_cache\n",
    "            \n",
    "            \n",
    "            sequence_outputs.append(torch.cat([forward_output_sequence,\n",
    "                                               backward_output_sequence], -1))\n",
    "            # Append the state tuples in a list, so that we can return\n",
    "            # the final states for all the layers.\n",
    "            final_states.append((torch.cat([forward_state[0], backward_state[0]], dim=-1),\n",
    "                                 torch.cat([forward_state[1], backward_state[1]], dim=-1)))\n",
    "\n",
    "        stacked_sequence_outputs: torch.FloatTensor = torch.stack(sequence_outputs)\n",
    "        # Stack the hidden state and memory for each layer into 2 tensors of shape\n",
    "        # (num_layers, batch_size, hidden_size) and (num_layers, batch_size, cell_size)\n",
    "        # respectively.\n",
    "        final_hidden_states, final_memory_states = zip(*final_states)\n",
    "        final_state_tuple: Tuple[torch.FloatTensor,\n",
    "                                 torch.FloatTensor] = (torch.cat(final_hidden_states, 0),\n",
    "                                                       torch.cat(final_memory_states, 0))\n",
    "        return stacked_sequence_outputs, final_state_tuple\n",
    "    \n",
    "class LstmCellWithProjection(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 cell_size: int,\n",
    "                 go_forward: bool = True,\n",
    "                 recurrent_dropout_probability: float = 0.0,\n",
    "                 memory_cell_clip_value: Optional[float] = None,\n",
    "                 state_projection_clip_value: Optional[float] = None) -> None:\n",
    "        super(LstmCellWithProjection, self).__init__()\n",
    "        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`.\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        self.go_forward = go_forward\n",
    "        self.state_projection_clip_value = state_projection_clip_value\n",
    "        self.memory_cell_clip_value = memory_cell_clip_value\n",
    "        self.recurrent_dropout_probability = recurrent_dropout_probability\n",
    "\n",
    "        # We do the projections for all the gates all at once.\n",
    "        self.input_linearity = nn.Linear(input_size, 4 * cell_size, bias=False)\n",
    "        self.state_linearity = nn.Linear(hidden_size, 4 * cell_size, bias=True)\n",
    "\n",
    "        # Additional projection matrix for making the hidden state smaller.\n",
    "        self.state_projection = nn.Linear(cell_size, hidden_size, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Use sensible default initializations for parameters.\n",
    "        block_orthogonal(self.input_linearity.weight.data, [self.cell_size, self.input_size])\n",
    "        block_orthogonal(self.state_linearity.weight.data, [self.cell_size, self.hidden_size])\n",
    "\n",
    "        self.state_linearity.bias.data.fill_(0.0)\n",
    "        # Initialize forget gate biases to 1.0 as per An Empirical\n",
    "        # Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n",
    "        self.state_linearity.bias.data[self.cell_size:2 * self.cell_size].fill_(1.0)\n",
    "\n",
    "    def forward(self,  # pylint: disable=arguments-differ\n",
    "                inputs: torch.FloatTensor,\n",
    "                batch_lengths: List[int],\n",
    "                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        print(f\"\\t\\t\\tinputs.size() = {inputs.size()}\")\n",
    "        batch_size = inputs.size()[0]\n",
    "        total_timesteps = inputs.size()[1]\n",
    "        print('\\t\\t\\tUnpacking batch_size, total_timesteps = inputs.size()')\n",
    "        print(f'\\t\\t\\tbatch_size = {batch_size}, total_timesteps = {total_timesteps}')\n",
    "\n",
    "        # We have to use this '.data.new().fill_' pattern to create tensors with the correct\n",
    "        # type - forward has no knowledge of whether these are torch.Tensors or torch.cuda.Tensors.\n",
    "        output_accumulator = Variable(inputs.data.new(batch_size,\n",
    "                                                      total_timesteps,\n",
    "                                                      self.hidden_size).fill_(0))\n",
    "        print(f\"\\t\\t\\tCreate tensor(output_accumulator) which has ({batch_size}, {total_timesteps}, {self.hidden_size}) shape, filling 0.\")\n",
    "        print(f\"\\t\\t\\tis `initial_state` is None? {initial_state is None}\")\n",
    "        if initial_state is None:\n",
    "            print(\"\\t\\t\\t\\tOh, then create full_batch_previous memory and state by \"\n",
    "                  f\"({batch_size}, {self.cell_size}) tensor filling 0.\")\n",
    "            full_batch_previous_memory = Variable(inputs.data.new(batch_size,\n",
    "                                                                  self.cell_size).fill_(0))\n",
    "            full_batch_previous_state = Variable(inputs.data.new(batch_size,\n",
    "                                                                 self.hidden_size).fill_(0))\n",
    "        else:\n",
    "            print(\"\\t\\t\\t\\tOk, Using `initial_state`, create full_batch_previous memory and state.\")\n",
    "            print(f\"\\t\\t\\t\\t(previous_state) = initial_state[0] = {initial_state[0]}\")\n",
    "            print(f\"\\t\\t\\t\\tfull_batch_previous_state = initial_state[0].squeeze(0) = {initial_state[0].squeeze(0)}\")\n",
    "            full_batch_previous_state = initial_state[0].squeeze(0)\n",
    "            print(f\"\\t\\t\\t\\t(previous_memory) = initial_state[1] = {initial_state[1]}\")\n",
    "            print(f\"\\t\\t\\t\\tfull_batch_previous_memory = initial_state[1].squeeze(0) = {initial_state[1].squeeze(0)}\")\n",
    "            full_batch_previous_memory = initial_state[1].squeeze(0)\n",
    "\n",
    "        current_length_index = batch_size - 1 if self.go_forward else 0\n",
    "        print(f\"\\t\\t\\t\\tSet current_length_index... is it forward?? {self.go_forward}\")\n",
    "        if self.go_forward:\n",
    "            print(f\"\\t\\t\\t\\tOk, forward!! current_length_index = batch_size - 1 = {batch_size - 1}\")\n",
    "            current_length_index = batch_size - 1\n",
    "        else:\n",
    "            print(f\"\\t\\t\\t\\tOops, backward!! current_length_index = 0\")\n",
    "            current_length_index = 0\n",
    "            \n",
    "        print('\\t\\t\\t\\tis recurrent_dropout_probability is larger than 0?', self.recurrent_dropout_probability > 0.0)\n",
    "        print('\\t\\t\\t\\tand is training?', self.training)\n",
    "        if self.recurrent_dropout_probability > 0.0 and self.training:\n",
    "            print('\\t\\t\\t\\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!')\n",
    "            dropout_mask = get_dropout_mask(self.recurrent_dropout_probability,\n",
    "                                            full_batch_previous_state)\n",
    "        else:\n",
    "            print('\\t\\t\\t\\toh, is not trainig. then dropout_mask = None.')\n",
    "            dropout_mask = None\n",
    "            \n",
    "        print(f\"\\t\\t\\t\\tStarting Loops with {total_timesteps}...\")\n",
    "        for timestep in range(total_timesteps):\n",
    "            # The index depends on which end we start.\n",
    "            index = timestep if self.go_forward else total_timesteps - timestep - 1\n",
    "            print(f\"\\t\\t\\t\\tindex = {index} since {'forward' if self.go_forward else 'backward'}\")\n",
    "\n",
    "            # What we are doing here is finding the index into the batch dimension\n",
    "            # which we need to use for this timestep, because the sequences have\n",
    "            # variable length, so once the index is greater than the length of this\n",
    "            # particular batch sequence, we no longer need to do the computation for\n",
    "            # this sequence. The key thing to recognise here is that the batch inputs\n",
    "            # must be _ordered_ by length from longest (first in batch) to shortest\n",
    "            # (last) so initially, we are going forwards with every sequence and as we\n",
    "            # pass the index at which the shortest elements of the batch finish,\n",
    "            # we stop picking them up for the computation.\n",
    "            if self.go_forward:\n",
    "                print('\\t\\t\\t\\tIn case forward')\n",
    "                print(f\"\\t\\t\\t\\tbatch_lengths[current_length_index] <= index = {batch_lengths[current_length_index] <= index}\")\n",
    "                while batch_lengths[current_length_index] <= index:\n",
    "                    print(\"\\t\\t\\t\\tcurrent_length_index -= 1\")\n",
    "                    current_length_index -= 1\n",
    "            # If we're going backwards, we are _picking up_ more indices.\n",
    "            else:\n",
    "                # First conditional: Are we already at the maximum number of elements in the batch?\n",
    "                # Second conditional: Does the next shortest sequence beyond the current batch\n",
    "                # index require computation use this timestep?\n",
    "                print('\\t\\t\\t\\tIn case backward,')\n",
    "                print(f\"\\t\\t\\t\\tbatch_lengths[current_length_index] <= index = {batch_lengths[current_length_index] <= index}\")\n",
    "                while current_length_index < (len(batch_lengths) - 1) and \\\n",
    "                                batch_lengths[current_length_index + 1] > index:\n",
    "                    print(\"\\t\\t\\t\\tcurrent_length_index += 1\")\n",
    "                    current_length_index += 1\n",
    "            print(f'\\t\\t\\t\\tbatch_lengths[length_index] is {batch_lengths[current_length_index]}')\n",
    "\n",
    "            # Actually get the slices of the batch which we\n",
    "            # need for the computation at this timestep.\n",
    "            # shape (batch_size, cell_size)\n",
    "            print(\"\\t\\t\\t\\tGet a previous memory...\")\n",
    "            print(full_batch_previous_memory[0: current_length_index + 1])\n",
    "            previous_memory = full_batch_previous_memory[0: current_length_index + 1].clone()\n",
    "            print(previous_memory.shape)\n",
    "            # Shape (batch_size, hidden_size)\n",
    "            print(\"\\t\\t\\t\\tGet a previous state...\")\n",
    "            print(full_batch_previous_memory[0: current_length_index + 1])\n",
    "            previous_state = full_batch_previous_state[0: current_length_index + 1].clone()\n",
    "            print(previous_state.shape)\n",
    "            # Shape (batch_size, input_size)\n",
    "            timestep_input = inputs[0: current_length_index + 1, index]\n",
    "            print(\"\\t\\t\\t\\tGet a timestep input...\")\n",
    "            print(timestep_input)\n",
    "            print(timestep_input.shape)\n",
    "\n",
    "            # Do the projections for all the gates all at once.\n",
    "            # Both have shape (batch_size, 4 * cell_size)\n",
    "            print(\"\\t\\t\\t\\tProjection to 4*cell_size...\")\n",
    "            projected_input = self.input_linearity(timestep_input)\n",
    "            print(\"\\t\\t\\t\\t`input_linearity`: W1 * timestep_input\")\n",
    "            print(f\"\\t\\t\\t\\tprojected_input.shape = {projected_input.shape}\")\n",
    "            projected_state = self.state_linearity(previous_state)\n",
    "            print(\"\\t\\t\\t\\t`state_linearity`: W2 * previous_state + b\")\n",
    "            print(f\"\\t\\t\\t\\tprojected_state.shape = {projected_state.shape}\")\n",
    "\n",
    "            # Main LSTM equations using relevant chunks of the big linear\n",
    "            # projections of the hidden state and inputs.\n",
    "            print(\"\\t\\t\\t\\tCalc LSTM hidden unit...\")\n",
    "            input_gate = torch.sigmoid(projected_input[:, (0 * self.cell_size):(1 * self.cell_size)] +\n",
    "                                       projected_state[:, (0 * self.cell_size):(1 * self.cell_size)])\n",
    "            forget_gate = torch.sigmoid(projected_input[:, (1 * self.cell_size):(2 * self.cell_size)] +\n",
    "                                        projected_state[:, (1 * self.cell_size):(2 * self.cell_size)])\n",
    "            memory_init = torch.tanh(projected_input[:, (2 * self.cell_size):(3 * self.cell_size)] +\n",
    "                                     projected_state[:, (2 * self.cell_size):(3 * self.cell_size)])\n",
    "            output_gate = torch.sigmoid(projected_input[:, (3 * self.cell_size):(4 * self.cell_size)] +\n",
    "                                        projected_state[:, (3 * self.cell_size):(4 * self.cell_size)])\n",
    "            memory = input_gate * memory_init + forget_gate * previous_memory\n",
    "\n",
    "            # Here is the non-standard part of this LSTM cell; first, we clip the\n",
    "            # memory cell, then we project the output of the timestep to a smaller size\n",
    "            # and again clip it.\n",
    "            print(f\"\\t\\t\\t\\tis memory_cell_clip_value is exist? {'Yes' if self.memory_cell_clip_value else 'No'}\")\n",
    "            if self.memory_cell_clip_value:\n",
    "                print(f\"\\t\\t\\t\\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value\", end='')\n",
    "                print(self.memory_cell_clip_value)\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                memory = torch.clamp(memory, -self.memory_cell_clip_value, self.memory_cell_clip_value)\n",
    "            else:\n",
    "                print(\"\\t\\t\\t\\tOh, it's None. passing the way.\")\n",
    "\n",
    "            print(\"\\t\\t\\t\\tCalc next timestep output...\")\n",
    "            # shape (current_length_index, cell_size)\n",
    "            pre_projection_timestep_output = output_gate * torch.tanh(memory)\n",
    "\n",
    "            # shape (current_length_index, hidden_size)\n",
    "            timestep_output = self.state_projection(pre_projection_timestep_output)\n",
    "            print(f\"\\t\\t\\t\\tstate_projection_clip_value is exist? {'Yes' if self.state_projection_clip_value else 'No'}\")\n",
    "            if self.state_projection_clip_value:\n",
    "                print(f\"\\t\\t\\t\\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value\", end='')\n",
    "                print(self.state_projection_clip_value)\n",
    "                # pylint: disable=invalid-unary-operand-type\n",
    "                timestep_output = torch.clamp(timestep_output,\n",
    "                                              -self.state_projection_clip_value,\n",
    "                                              self.state_projection_clip_value)\n",
    "            else:\n",
    "                print(\"\\t\\t\\t\\tOh, it's None. passing the way.\")\n",
    "\n",
    "            # Only do dropout if the dropout prob is > 0.0 and we are in training mode.\n",
    "            print(\"\\t\\t\\t\\tIf dropout_mask exists, Adjust.\")\n",
    "            if dropout_mask is not None:\n",
    "                timestep_output = timestep_output * dropout_mask[0: current_length_index + 1]\n",
    "\n",
    "            # We've been doing computation with less than the full batch, so here we create a new\n",
    "            # variable for the the whole batch at this timestep and insert the result for the\n",
    "            # relevant elements of the batch into it.\n",
    "            print('\\t\\t\\t\\tset full_batch_previous memory/state!!')\n",
    "            full_batch_previous_memory = Variable(full_batch_previous_memory.data.clone())\n",
    "            full_batch_previous_state = Variable(full_batch_previous_state.data.clone())\n",
    "            full_batch_previous_memory[0:current_length_index + 1] = memory\n",
    "            full_batch_previous_state[0:current_length_index + 1] = timestep_output\n",
    "            output_accumulator[0:current_length_index + 1, index] = timestep_output\n",
    "\n",
    "        # Mimic the pytorch API by returning state in the following shape:\n",
    "        # (num_layers * num_directions, batch_size, ...). As this\n",
    "        # LSTM cell cannot be stacked, the first dimension here is just 1.\n",
    "        final_state = (full_batch_previous_state.unsqueeze(0),\n",
    "                       full_batch_previous_memory.unsqueeze(0))\n",
    "        print(f\"\\t\\t\\t\\tfinal_state = {final_state}\")\n",
    "\n",
    "        return output_accumulator, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo biLM layer params\n",
      "\tinput_size = 512\n",
      "\thidden_size = 512\n",
      "\tcell_size = 4096\n",
      "\tnum_layers = 2\n",
      "\tmemory_cell_clip_value = 3\n",
      "\tstate_projection_clip_value = 3\n",
      "forward_layers = [LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "), LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      ")]\n",
      "backward_layers = [LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "), LstmCellWithProjection(\n",
      "  (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "  (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "  (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "encoder = ElmobiLm(config, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD!!!!**************\n",
      "batch_size = 3\n",
      "total_sequence_length = 10\n",
      "_EncoderBase.sort_and_run_forward 메서드 실시...\n",
      "\tbatch_size = 3, num_valid = 3\n",
      "\tsequence_lengths = tensor([ 7,  6, 10], device='cuda:0')\n",
      "\t1. sorted_inputs.shape = torch.Size([3, 10, 512])\n",
      "\t2. sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "\t3. restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "\t4. sorting_indices = tensor([2, 0, 1], device='cuda:0')\n",
      "\t             sorted_inputs.shape  = torch.Size([3, 10, 512])\n",
      "\tpacked_sequence_input.data.shape  = torch.Size([23, 512])\n",
      "\tpacked_sequence_input.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n",
      "\tself.stateful is True\n",
      "\tstateful is True,\n",
      "\t\tConduct `_get_initial_states`\n",
      "\tRUN `_lstm_forward`... by initial_states\n",
      "\t\tinitial_state is None? True\n",
      "\t\tOops, Assign hidden_state = [None] * len(self.forward_layers)\n",
      "\t\thidden_states = [None, None]\n",
      "\t\tinputs is `PackedSequence`\n",
      "\t\ttype(inputs) = <class 'torch.nn.utils.rnn.PackedSequence'>\n",
      "\t\t\tinputs.data.shape = torch.Size([23, 512])\n",
      "\t\t\tinputs.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n",
      "\t\t\tinputs.sorted_indices = None\n",
      "\t\t\tinputs.unsorted_indices = None\n",
      "\t\tRestore PAD_char to inputs...\n",
      "\t\t바뀐 inputs의 정보 출력\n",
      "\t\ttype(inputs) = <class 'torch.Tensor'>\n",
      "\t\t\tinputs.shape = torch.Size([3, 10, 512])\n",
      "\t\tbatch_lengths = tensor([10,  7,  6])\n",
      "\t\tAssign forward_output_sequence = backward_output_sequence = inputs\n",
      "\t\tSet final_states, sequqnce_outputs as empty list, []\n",
      "\t\tGet a forward layer and backward layer at layer 1\n",
      "\t\tCaching...: output_sequence to cache both forward and backward\n",
      "\t\tstate is None? True\n",
      "\t\t\tOops, then forward and backward state is also 'None'\n",
      "\t\tRUN forward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? True\n",
      "\t\t\t\tOh, then create full_batch_previous memory and state by (3, 4096) tensor filling 0.\n",
      "\t\t\t\tSet current_length_index... is it forward?? True\n",
      "\t\t\t\tOk, forward!! current_length_index = batch_size - 1 = 2\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "binary_mask = tensor([[False,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True, False,  True,  ...,  True,  True, False],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 0 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0061, -0.0125,  0.0025,  ...,  0.0001, -0.0005, -0.0002],\n",
      "        [-0.0061, -0.0125,  0.0025,  ...,  0.0001, -0.0005, -0.0002],\n",
      "        [-0.0061, -0.0125,  0.0025,  ...,  0.0001, -0.0005, -0.0002]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0061, -0.0125,  0.0025,  ...,  0.0001, -0.0005, -0.0002],\n",
      "        [-0.0061, -0.0125,  0.0025,  ...,  0.0001, -0.0005, -0.0002],\n",
      "        [-0.0061, -0.0125,  0.0025,  ...,  0.0001, -0.0005, -0.0002]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0078, -0.0260, -0.0205,  ...,  0.0145,  0.0510, -0.0102],\n",
      "        [ 0.0290,  0.0047, -0.0227,  ...,  0.0162,  0.0292, -0.0161],\n",
      "        [ 0.0056, -0.0269, -0.0216,  ..., -0.0044,  0.0573, -0.0453]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-1.1798e-02, -1.7650e-02,  7.1126e-03,  ...,  1.1403e-04,\n",
      "          6.3481e-05,  1.4643e-03],\n",
      "        [-7.5590e-03, -1.7541e-02,  8.7538e-05,  ...,  3.1995e-03,\n",
      "          3.9151e-04, -2.6707e-03],\n",
      "        [-6.4149e-03, -2.0240e-02,  5.1118e-03,  ..., -1.3528e-03,\n",
      "          4.2668e-03, -5.0392e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-1.1798e-02, -1.7650e-02,  7.1126e-03,  ...,  1.1403e-04,\n",
      "          6.3481e-05,  1.4643e-03],\n",
      "        [-7.5590e-03, -1.7541e-02,  8.7538e-05,  ...,  3.1995e-03,\n",
      "          3.9151e-04, -2.6707e-03],\n",
      "        [-6.4149e-03, -2.0240e-02,  5.1118e-03,  ..., -1.3528e-03,\n",
      "          4.2668e-03, -5.0392e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0439, -0.0200, -0.0223,  ...,  0.0114,  0.0445, -0.0566],\n",
      "        [-0.0055, -0.0275, -0.0182,  ...,  0.0095,  0.0294, -0.0096],\n",
      "        [ 0.0261, -0.0046, -0.0160,  ...,  0.0313,  0.0107, -0.0573]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0117, -0.0129,  0.0098,  ..., -0.0066, -0.0024,  0.0031],\n",
      "        [-0.0111, -0.0166,  0.0047,  ...,  0.0031,  0.0061, -0.0025],\n",
      "        [-0.0110, -0.0235, -0.0007,  ..., -0.0015, -0.0028, -0.0124]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0117, -0.0129,  0.0098,  ..., -0.0066, -0.0024,  0.0031],\n",
      "        [-0.0111, -0.0166,  0.0047,  ...,  0.0031,  0.0061, -0.0025],\n",
      "        [-0.0110, -0.0235, -0.0007,  ..., -0.0015, -0.0028, -0.0124]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0195,  0.0024, -0.0233,  ...,  0.0120,  0.0189, -0.0226],\n",
      "        [ 0.0259, -0.0076, -0.0144,  ...,  0.0053,  0.0274, -0.0328],\n",
      "        [ 0.0088, -0.0010, -0.0169,  ...,  0.0276,  0.0261, -0.0068]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0148, -0.0153,  0.0063,  ..., -0.0105, -0.0097,  0.0008],\n",
      "        [-0.0143, -0.0137,  0.0067,  ...,  0.0037,  0.0026, -0.0106],\n",
      "        [-0.0093, -0.0236,  0.0037,  ...,  0.0005, -0.0013, -0.0132]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0148, -0.0153,  0.0063,  ..., -0.0105, -0.0097,  0.0008],\n",
      "        [-0.0143, -0.0137,  0.0067,  ...,  0.0037,  0.0026, -0.0106],\n",
      "        [-0.0093, -0.0236,  0.0037,  ...,  0.0005, -0.0013, -0.0132]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0267,  0.0010, -0.0196,  ...,  0.0005,  0.0283, -0.0203],\n",
      "        [ 0.0260, -0.0238,  0.0250,  ...,  0.0017,  0.0297, -0.0235],\n",
      "        [-0.0157,  0.0032, -0.0359,  ...,  0.0232,  0.0049, -0.0217]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0132, -0.0174,  0.0075,  ..., -0.0071, -0.0054,  0.0002],\n",
      "        [-0.0080, -0.0167,  0.0060,  ...,  0.0068,  0.0036, -0.0131],\n",
      "        [-0.0133, -0.0180,  0.0049,  ...,  0.0091,  0.0026, -0.0116]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0132, -0.0174,  0.0075,  ..., -0.0071, -0.0054,  0.0002],\n",
      "        [-0.0080, -0.0167,  0.0060,  ...,  0.0068,  0.0036, -0.0131],\n",
      "        [-0.0133, -0.0180,  0.0049,  ...,  0.0091,  0.0026, -0.0116]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-6.8748e-03, -1.3605e-02,  7.1700e-03,  ...,  1.2522e-02,\n",
      "          2.0355e-02, -2.4865e-02],\n",
      "        [-2.9532e-03, -1.3722e-02, -2.7846e-02,  ..., -3.4922e-03,\n",
      "          1.9039e-03, -3.1346e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-1.4413e-02, -1.6767e-02,  7.9120e-03,  ..., -9.9524e-05,\n",
      "         -2.3517e-03, -2.9015e-03],\n",
      "        [-9.6493e-03, -1.7545e-02,  4.0518e-03,  ...,  9.9050e-03,\n",
      "          4.2411e-03, -1.4561e-02]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-1.4413e-02, -1.6767e-02,  7.9120e-03,  ..., -9.9524e-05,\n",
      "         -2.3517e-03, -2.9015e-03],\n",
      "        [-9.6493e-03, -1.7545e-02,  4.0518e-03,  ...,  9.9050e-03,\n",
      "          4.2411e-03, -1.4561e-02]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 7.8088e-03, -2.3229e-02, -1.9114e-02,  ...,  2.5953e-02,\n",
      "          5.1098e-02, -5.5057e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0115, -0.0170,  0.0084,  ...,  0.0012, -0.0025,  0.0024]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0115, -0.0170,  0.0084,  ...,  0.0012, -0.0025,  0.0024]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 3.9943e-02, -6.3473e-04, -2.4333e-02, -2.6465e-02,  3.6282e-02,\n",
      "         -8.1245e-02,  2.0234e-02, -4.7533e-02, -1.4706e-02,  1.8934e-02,\n",
      "          4.9761e-04, -6.6569e-02,  3.0510e-02,  8.3242e-03,  3.4185e-03,\n",
      "          4.2514e-02,  3.6875e-02,  1.1155e-02, -6.4569e-02, -2.6975e-02,\n",
      "          1.4409e-02, -8.3901e-03,  1.4958e-02,  1.4861e-02,  1.6609e-02,\n",
      "          4.6618e-02,  8.1440e-02,  6.3781e-02,  4.3447e-02,  5.9358e-03,\n",
      "         -5.9265e-04,  3.1957e-02, -3.8824e-03,  6.6324e-02,  1.1476e-03,\n",
      "          3.0135e-02,  2.8017e-02, -2.6136e-03, -8.7879e-02,  8.0971e-03,\n",
      "          4.2865e-02,  2.4699e-02, -1.6333e-03,  4.5484e-02,  4.8548e-02,\n",
      "         -2.7261e-02,  4.9371e-02, -1.1094e-02, -6.2017e-02,  6.3433e-02,\n",
      "          1.1362e-02, -3.7070e-02, -2.8188e-02, -3.8195e-02, -5.8539e-02,\n",
      "          1.2449e-02, -2.7576e-03,  3.2095e-02,  7.0868e-03,  7.0801e-03,\n",
      "         -2.9167e-02, -6.1884e-02,  3.3293e-02,  7.4093e-03,  8.6460e-03,\n",
      "          2.1394e-02,  2.8104e-02, -2.3261e-02, -4.2063e-02,  2.1559e-02,\n",
      "         -7.4343e-03, -4.5396e-02, -1.7797e-02, -8.0800e-03, -5.4489e-02,\n",
      "         -7.8084e-03,  5.2327e-03, -1.9146e-02, -4.8832e-02,  2.1798e-02,\n",
      "          9.2141e-03, -3.6777e-02,  6.6118e-03,  3.2340e-02,  1.9615e-02,\n",
      "          2.1029e-02, -2.2645e-02, -2.0520e-02, -8.7323e-02, -4.5272e-02,\n",
      "         -2.7091e-02, -2.3780e-02, -2.8575e-02,  7.5322e-02,  2.6836e-02,\n",
      "         -1.4969e-02,  1.4021e-02,  3.6706e-03, -6.0353e-02, -9.9927e-03,\n",
      "         -7.4608e-02, -6.0827e-03,  4.9744e-02, -1.1646e-02, -1.5283e-02,\n",
      "          3.9851e-02,  1.9263e-02, -2.9827e-02,  3.6977e-02,  4.5320e-02,\n",
      "         -4.4115e-02, -5.5769e-02,  1.1291e-02,  3.4049e-02, -4.0742e-02,\n",
      "         -7.8004e-02, -3.9623e-02, -6.5474e-02, -5.7309e-02,  3.2225e-03,\n",
      "          4.3883e-03,  2.9382e-02,  7.0717e-02,  6.7704e-02, -2.4883e-02,\n",
      "          2.6200e-02, -1.4513e-02,  2.5271e-02,  3.6185e-02, -6.2445e-03,\n",
      "         -1.5884e-02, -5.3081e-03, -5.1873e-03,  4.8760e-02,  4.0506e-02,\n",
      "          4.3785e-02,  1.0459e-02,  5.6056e-02, -4.2227e-03,  4.8874e-02,\n",
      "         -2.1245e-02, -3.1359e-03,  1.8048e-02, -2.3473e-02,  5.5935e-02,\n",
      "         -3.8959e-02,  1.0918e-02, -8.7178e-02,  3.0119e-03, -2.1534e-02,\n",
      "          3.2873e-03,  7.5276e-02, -3.9145e-02, -2.3037e-02,  1.6807e-02,\n",
      "          5.1875e-02, -6.6744e-05, -2.1719e-02,  3.4947e-02, -1.9906e-02,\n",
      "         -4.2533e-02, -1.4287e-02, -1.7697e-02, -3.5453e-02, -2.6380e-02,\n",
      "          3.0872e-03, -3.2254e-02, -5.6165e-02, -1.4399e-02,  6.4918e-04,\n",
      "         -4.0651e-02,  5.3044e-02,  5.6464e-02, -7.1317e-02,  2.5132e-02,\n",
      "         -2.7497e-02, -1.5007e-02, -4.0442e-02,  3.2662e-02, -3.9329e-02,\n",
      "          1.1346e-02,  5.2525e-02,  2.5804e-02,  2.6124e-02, -8.0331e-02,\n",
      "          1.4954e-02,  2.4630e-02,  1.5274e-02, -4.8547e-02,  3.7759e-02,\n",
      "         -2.4851e-02, -6.2086e-02,  1.5069e-02,  5.3204e-02,  1.5821e-02,\n",
      "         -2.3344e-02, -1.3930e-03,  1.4708e-02,  1.1591e-02, -7.3345e-02,\n",
      "         -3.6105e-02, -2.8722e-02,  7.6852e-04, -2.2699e-02, -8.3008e-02,\n",
      "          7.0183e-02, -3.0014e-02, -9.3609e-03, -2.8232e-02,  6.1581e-02,\n",
      "         -2.4002e-02,  1.5176e-02,  5.0153e-02, -9.7399e-04, -5.9783e-02,\n",
      "          3.4617e-02, -2.9192e-03,  2.7485e-02,  6.9439e-02, -1.1364e-02,\n",
      "         -1.3452e-01,  2.2476e-02, -1.4151e-02, -7.1442e-03, -1.2852e-02,\n",
      "         -2.2974e-02,  2.5305e-02,  9.9605e-03,  2.9039e-03, -3.2136e-02,\n",
      "          1.0917e-02,  3.9640e-02, -1.0201e-02, -1.9363e-02, -1.1269e-02,\n",
      "          9.5938e-03,  5.2384e-02, -5.6701e-02, -8.5196e-03,  1.2629e-02,\n",
      "         -4.5141e-02, -2.6792e-02, -1.0696e-02,  2.9739e-02, -5.2543e-02,\n",
      "          4.4625e-03,  3.8448e-02, -1.4611e-03,  7.8076e-02, -1.0361e-02,\n",
      "         -1.6192e-02, -7.5027e-03,  5.8933e-02, -8.2104e-03, -1.0303e-02,\n",
      "         -1.0491e-02, -6.1503e-02,  3.2754e-02, -1.5981e-02, -2.7549e-02,\n",
      "         -5.1394e-02, -3.0974e-02,  4.4842e-02,  1.3567e-02, -4.8374e-03,\n",
      "          4.6250e-02,  4.9839e-02, -1.7988e-02,  3.7572e-02, -4.8327e-02,\n",
      "         -1.2389e-01,  9.6038e-03,  2.2187e-02,  1.4403e-02,  4.4363e-03,\n",
      "         -7.8948e-03, -3.4436e-02, -5.8862e-02, -3.2297e-02,  2.4687e-03,\n",
      "          1.3516e-02, -1.9046e-02,  6.2428e-02, -2.4486e-02,  2.5915e-02,\n",
      "         -4.7463e-02,  4.1579e-02, -1.3121e-02, -2.5279e-02, -1.6883e-02,\n",
      "         -1.2437e-02, -6.8095e-02,  3.2792e-02, -3.9081e-02,  2.3718e-02,\n",
      "         -7.6230e-03,  2.5899e-02, -9.0227e-03, -4.4538e-02,  1.8885e-02,\n",
      "          6.7445e-03,  2.3967e-02,  3.6491e-02,  6.1667e-02,  2.9088e-02,\n",
      "         -4.6740e-02,  4.2268e-02,  5.3146e-03,  3.4716e-03,  1.3579e-02,\n",
      "         -3.4293e-02, -7.6553e-03, -3.0861e-02,  2.1517e-02, -1.5814e-02,\n",
      "          1.6505e-02,  2.4900e-02,  3.6272e-02, -1.1502e-02,  2.1404e-02,\n",
      "         -1.9012e-02,  4.1999e-02, -2.3130e-02, -1.2259e-03, -3.5337e-02,\n",
      "         -6.3823e-02, -1.7515e-02, -4.7187e-02, -2.1176e-02,  2.9516e-02,\n",
      "         -5.6702e-02, -3.0155e-02, -4.3931e-02,  5.9581e-02, -4.8076e-02,\n",
      "          3.2992e-02,  5.8642e-02,  4.0638e-02, -4.9440e-03, -2.6222e-02,\n",
      "         -1.7771e-02, -4.0846e-02,  2.8135e-02, -4.8641e-02, -3.6589e-02,\n",
      "         -4.3364e-03, -4.0931e-02,  1.2174e-02, -2.9099e-02, -3.0528e-02,\n",
      "          1.0307e-02,  6.1238e-02, -5.8316e-02, -1.3105e-02,  9.0719e-03,\n",
      "         -1.7113e-02, -1.6299e-02, -4.1770e-02,  5.2599e-02, -2.1581e-02,\n",
      "          1.3731e-02,  3.1022e-02, -9.9482e-03,  2.3787e-02, -7.2789e-02,\n",
      "          3.9918e-03, -1.1121e-01,  6.5336e-02, -3.4317e-02,  6.3555e-02,\n",
      "          4.7264e-02, -2.1690e-02,  3.2510e-02, -2.9224e-03,  8.1941e-03,\n",
      "          2.2115e-03,  4.4054e-02, -2.8647e-02, -1.7062e-02,  3.1700e-02,\n",
      "          1.3082e-02, -7.8945e-03,  5.9706e-02, -3.6144e-03,  1.2505e-02,\n",
      "          3.5541e-02,  5.5393e-02,  7.4438e-03,  3.9664e-02, -2.1253e-02,\n",
      "         -6.8311e-03, -4.3957e-02,  3.7919e-02, -2.8258e-02, -8.0768e-03,\n",
      "         -4.2647e-02,  3.2717e-02, -2.9899e-02, -9.3153e-03,  1.5475e-02,\n",
      "          3.0740e-02,  1.8109e-02,  2.8607e-02, -3.8829e-02, -4.7982e-02,\n",
      "         -7.1243e-03, -2.4547e-02,  8.3619e-04, -1.8965e-02,  1.3804e-02,\n",
      "          1.0712e-02, -3.4088e-02, -7.9899e-02, -7.0846e-03,  6.9593e-03,\n",
      "         -2.1685e-02,  5.5655e-02, -1.2060e-02,  2.3431e-03,  7.2531e-02,\n",
      "         -6.0682e-02, -7.4575e-02, -1.7317e-02,  5.8419e-03, -2.3470e-02,\n",
      "          9.9523e-03, -6.2580e-02,  7.6685e-02,  2.9135e-02, -3.8954e-02,\n",
      "          1.4604e-02,  3.1968e-03,  3.2509e-02, -2.8317e-03,  2.2350e-02,\n",
      "         -4.7101e-02, -5.0960e-02,  3.1412e-03, -1.5378e-02, -2.6794e-02,\n",
      "         -1.9578e-02,  1.0552e-02, -6.9772e-02,  2.0636e-02, -2.9148e-03,\n",
      "         -4.0716e-02,  2.5176e-03,  7.4695e-03, -3.4024e-04,  3.2252e-02,\n",
      "         -2.8218e-02, -1.9674e-02, -2.1514e-02, -9.1328e-03,  2.0247e-02,\n",
      "          3.0113e-02, -7.1527e-02,  4.6293e-02,  2.1005e-02, -2.0541e-02,\n",
      "          4.0748e-02,  4.6423e-03, -2.4288e-02, -1.5478e-02, -3.5585e-02,\n",
      "          2.1604e-02,  3.8710e-02,  1.4737e-02, -3.1023e-02, -5.9019e-02,\n",
      "          3.0318e-02,  3.5039e-02, -1.1337e-03,  3.8222e-02,  1.5961e-02,\n",
      "          2.4988e-02,  1.0752e-02,  7.9247e-02,  3.4722e-02, -7.4136e-02,\n",
      "          3.9050e-02, -1.8820e-02,  6.0536e-02,  6.0405e-03, -1.8136e-02,\n",
      "          4.7733e-02,  4.5106e-03, -5.9822e-03, -1.5836e-02,  1.4583e-03,\n",
      "         -2.1234e-02, -2.3092e-02, -6.0511e-03,  7.6471e-02, -1.0707e-02,\n",
      "         -3.3524e-02,  3.2486e-02, -7.2459e-03, -6.4545e-02, -4.4839e-02,\n",
      "          3.7363e-02, -2.0615e-02, -1.1821e-01, -2.0830e-02,  6.9055e-02,\n",
      "         -1.0918e-02,  5.4751e-02,  2.7437e-02,  3.1381e-02,  1.9435e-02,\n",
      "          5.1162e-02, -3.3509e-02]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0101, -0.0158,  0.0068,  ..., -0.0005, -0.0053, -0.0032]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0101, -0.0158,  0.0068,  ..., -0.0005, -0.0053, -0.0032]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0225,  0.0021, -0.0073, -0.0453,  0.0067, -0.1035,  0.0007, -0.0384,\n",
      "         -0.0241, -0.0236, -0.0056, -0.0469,  0.0058, -0.0028,  0.0082,  0.0394,\n",
      "          0.0411,  0.0036, -0.0933, -0.0109,  0.0350, -0.0014,  0.0195,  0.0090,\n",
      "          0.0102,  0.0144,  0.0910,  0.0898,  0.0680, -0.0267,  0.0296,  0.0123,\n",
      "         -0.0376,  0.0462,  0.0312, -0.0027,  0.0219, -0.0156, -0.0906, -0.0108,\n",
      "          0.0491, -0.0022, -0.0330,  0.0454,  0.0668, -0.0107,  0.0695, -0.0285,\n",
      "         -0.1164,  0.0746,  0.0207,  0.0046, -0.0736, -0.0499, -0.0580, -0.0040,\n",
      "          0.0032,  0.0278,  0.0149, -0.0144,  0.0139, -0.0536,  0.0244,  0.0268,\n",
      "         -0.0166, -0.0146, -0.0104, -0.0225, -0.0341,  0.0614, -0.0234, -0.0311,\n",
      "         -0.0392,  0.0299, -0.0474, -0.0211, -0.0228, -0.0282, -0.0288, -0.0021,\n",
      "         -0.0283, -0.0399,  0.0053,  0.0269, -0.0246,  0.0394, -0.0482, -0.0131,\n",
      "         -0.0672, -0.0286, -0.0382, -0.0050, -0.0136,  0.0675,  0.0515,  0.0383,\n",
      "          0.0347, -0.0290, -0.0956,  0.0055, -0.0542,  0.0374,  0.0811, -0.0625,\n",
      "          0.0048,  0.0439,  0.0274, -0.0130,  0.0037,  0.0702, -0.0189, -0.0653,\n",
      "          0.0338,  0.0484, -0.0758, -0.1118, -0.0982, -0.0586, -0.1017,  0.0323,\n",
      "         -0.0187,  0.0306,  0.0579,  0.0252, -0.0435,  0.0251, -0.0279,  0.0555,\n",
      "          0.0822,  0.0015,  0.0172,  0.0274, -0.0081,  0.0368,  0.0218,  0.0329,\n",
      "          0.0185,  0.0088, -0.0144,  0.0301,  0.0175, -0.0075,  0.0198,  0.0053,\n",
      "          0.0708,  0.0166, -0.0065, -0.0641, -0.0199, -0.0439,  0.0324,  0.0846,\n",
      "         -0.0156, -0.0623,  0.0368,  0.0376, -0.0219, -0.0321,  0.0116, -0.0214,\n",
      "         -0.0275, -0.0293, -0.0180, -0.0346, -0.0491,  0.0416, -0.0381, -0.0820,\n",
      "         -0.0298,  0.0151, -0.0111,  0.0488,  0.0482, -0.0697,  0.0230, -0.0346,\n",
      "         -0.0385, -0.0320,  0.0022, -0.0195,  0.0382,  0.0669,  0.0117, -0.0143,\n",
      "         -0.0986,  0.0230, -0.0114,  0.0136, -0.0391,  0.0525, -0.0245, -0.0297,\n",
      "         -0.0252,  0.0744,  0.0637,  0.0055,  0.0261,  0.0363, -0.0025, -0.0683,\n",
      "         -0.0255, -0.0456,  0.0402, -0.0391, -0.0926,  0.0581, -0.0183, -0.0362,\n",
      "         -0.0535,  0.0409, -0.0179,  0.0289,  0.0655,  0.0294, -0.0775,  0.0396,\n",
      "          0.0239,  0.0403,  0.0801, -0.0544, -0.1100, -0.0068, -0.0387, -0.0217,\n",
      "          0.0214, -0.0616, -0.0089, -0.0123,  0.0534, -0.0370,  0.0211,  0.0167,\n",
      "         -0.0111, -0.0011,  0.0025,  0.0404,  0.0642, -0.0452, -0.0113,  0.0538,\n",
      "         -0.0520, -0.0056,  0.0366,  0.0759, -0.0529, -0.0374,  0.0588,  0.0106,\n",
      "          0.1057, -0.0457, -0.0078,  0.0133,  0.0720, -0.0095, -0.0401,  0.0218,\n",
      "         -0.0456,  0.0502,  0.0087, -0.0067, -0.0664, -0.0569,  0.0319,  0.0456,\n",
      "          0.0117,  0.0440,  0.0148, -0.0104,  0.0672, -0.0583, -0.1598,  0.0023,\n",
      "          0.0538, -0.0143,  0.0224,  0.0161, -0.0428, -0.0923, -0.0291, -0.0308,\n",
      "         -0.0071, -0.0245,  0.0439, -0.0270,  0.0261, -0.0623,  0.0007, -0.0537,\n",
      "         -0.0007, -0.0388,  0.0227, -0.0308,  0.0428, -0.0017,  0.0260, -0.0640,\n",
      "         -0.0064, -0.0349, -0.0219,  0.0544,  0.0239,  0.0471,  0.0232,  0.0261,\n",
      "          0.0282, -0.0696,  0.0527,  0.0118,  0.0263, -0.0006, -0.0661,  0.0209,\n",
      "          0.0058,  0.0030, -0.0387,  0.0064,  0.0166,  0.0501, -0.0120,  0.0489,\n",
      "          0.0016,  0.0325, -0.0278,  0.0218,  0.0103, -0.0845, -0.0082, -0.0481,\n",
      "         -0.0449,  0.0369, -0.0878, -0.0075, -0.0770,  0.0889, -0.0595,  0.0261,\n",
      "          0.0722,  0.0245, -0.0125, -0.0249, -0.0267, -0.0498, -0.0126, -0.0515,\n",
      "         -0.0232, -0.0248, -0.0696,  0.0107, -0.0039,  0.0008,  0.0189,  0.0703,\n",
      "         -0.0633,  0.0129,  0.0091, -0.0232,  0.0251, -0.0279,  0.1155, -0.0159,\n",
      "         -0.0343,  0.0638, -0.0056,  0.0369, -0.0636, -0.0179, -0.1186,  0.0721,\n",
      "         -0.0307,  0.0792,  0.0270,  0.0300,  0.0747,  0.0376,  0.0251,  0.0128,\n",
      "          0.0466, -0.0187, -0.0412,  0.0287,  0.0269, -0.0115,  0.0786,  0.0066,\n",
      "         -0.0067,  0.0407,  0.0497,  0.0495,  0.0253, -0.0395, -0.0038, -0.0613,\n",
      "          0.0059, -0.0225,  0.0075,  0.0116,  0.0301, -0.0510,  0.0483, -0.0205,\n",
      "          0.0458,  0.0350,  0.0831, -0.0208, -0.0651,  0.0033, -0.0349,  0.0098,\n",
      "         -0.0437,  0.0113, -0.0011, -0.0237, -0.0102,  0.0026,  0.0296, -0.0294,\n",
      "          0.0504,  0.0353, -0.0013,  0.0868, -0.0338, -0.1060, -0.0469, -0.0076,\n",
      "         -0.0594, -0.0139, -0.1025,  0.0495,  0.0576, -0.0711,  0.0156, -0.0270,\n",
      "          0.0126,  0.0289,  0.0422, -0.0147, -0.0343,  0.0242, -0.0238,  0.0042,\n",
      "         -0.0163, -0.0254, -0.0318, -0.0033, -0.0148, -0.0554, -0.0152,  0.0177,\n",
      "         -0.0006,  0.0078,  0.0026, -0.0279, -0.0466, -0.0133, -0.0026,  0.0167,\n",
      "         -0.0890,  0.0875, -0.0195, -0.0232,  0.0426, -0.0253, -0.0016, -0.0328,\n",
      "          0.0101,  0.0372,  0.0486, -0.0026, -0.0226, -0.0536,  0.0217,  0.0102,\n",
      "         -0.0430,  0.0425,  0.0180, -0.0253,  0.0030,  0.0580,  0.0355, -0.0872,\n",
      "          0.0397, -0.0145,  0.0678,  0.0079, -0.0342,  0.0471,  0.0262,  0.0146,\n",
      "         -0.0287, -0.0009, -0.0416,  0.0084, -0.0227,  0.0526, -0.0619, -0.0201,\n",
      "          0.0613,  0.0342, -0.0878, -0.0522,  0.0243, -0.0385, -0.0821, -0.0479,\n",
      "          0.0745, -0.0005,  0.0558,  0.0333, -0.0184,  0.0317,  0.0303, -0.0447]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 9 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0146, -0.0205,  0.0057,  ..., -0.0067, -0.0101, -0.0116]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0146, -0.0205,  0.0057,  ..., -0.0067, -0.0101, -0.0116]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.0999e-03, -6.1420e-03, -4.0220e-03, -4.2812e-02,  2.2545e-03,\n",
      "         -5.5843e-02, -3.3867e-03, -2.4588e-02, -5.0965e-02,  5.2278e-03,\n",
      "          3.2208e-03, -6.6673e-02, -4.4035e-03, -3.9993e-02, -1.1946e-02,\n",
      "          5.1611e-02,  9.1874e-03, -3.4062e-03, -6.2161e-02, -1.2382e-02,\n",
      "          1.7805e-02,  3.6729e-03,  4.2529e-03,  1.7206e-02, -1.2278e-02,\n",
      "          1.4176e-02,  6.9620e-02,  5.3863e-02,  2.9146e-02,  1.3687e-04,\n",
      "          4.2660e-03,  2.6004e-02, -2.9728e-04,  2.5874e-02,  5.3049e-04,\n",
      "         -9.1188e-03, -2.0923e-03, -3.1675e-02, -6.1669e-02,  2.7857e-02,\n",
      "          3.7919e-02,  4.7040e-02, -2.9004e-02,  2.8061e-02,  2.2440e-02,\n",
      "         -1.7007e-02,  4.6762e-02, -9.9050e-03, -6.0391e-02,  4.4008e-02,\n",
      "          2.7127e-02, -1.0071e-02, -4.4653e-02, -3.4113e-02, -5.4936e-02,\n",
      "         -2.7576e-02, -1.1772e-02,  1.6111e-02, -2.4791e-02, -1.2392e-02,\n",
      "         -4.5851e-02, -7.0682e-02,  6.8990e-02,  1.5804e-02, -1.8523e-02,\n",
      "          1.7020e-02,  3.7107e-03, -3.7832e-02, -9.7512e-03,  2.5285e-02,\n",
      "         -1.8639e-02, -1.1800e-02, -3.4856e-02,  2.3652e-03, -4.5387e-02,\n",
      "         -2.9169e-02, -7.3532e-03, -5.6582e-02, -4.0548e-02,  2.6095e-02,\n",
      "         -4.1448e-03, -7.7175e-03, -4.9508e-03,  5.8557e-02,  7.4085e-03,\n",
      "          2.0493e-02, -2.6433e-02, -5.9150e-02, -8.5825e-02, -3.6332e-02,\n",
      "         -6.7253e-02, -1.8667e-02,  1.9783e-02,  4.1228e-02,  3.5272e-02,\n",
      "         -1.5310e-02,  2.6563e-02,  9.0099e-03, -5.8591e-02, -1.2875e-02,\n",
      "         -4.5589e-02,  3.7654e-02,  5.1700e-02, -9.7946e-03, -2.8365e-02,\n",
      "          3.8642e-02,  3.2772e-02, -7.0115e-03,  1.7229e-02,  4.8634e-02,\n",
      "         -4.3900e-02, -5.7408e-02,  4.0625e-02,  4.4493e-02, -5.2082e-02,\n",
      "         -7.4520e-02, -5.9377e-02, -6.7939e-02, -7.3847e-02,  1.3183e-02,\n",
      "         -4.9373e-03,  1.8830e-02,  9.7497e-02,  1.3256e-02,  1.6883e-03,\n",
      "          1.4859e-02, -2.3568e-03,  5.6624e-02,  5.3152e-02, -1.2295e-02,\n",
      "          1.2090e-02, -1.3728e-05,  1.0096e-02,  3.3864e-02,  4.2704e-02,\n",
      "          5.4648e-02,  4.1818e-03,  3.7166e-03, -1.5428e-02,  4.5995e-02,\n",
      "          2.1937e-02, -2.8221e-02,  1.6410e-02, -4.7979e-04,  4.1428e-02,\n",
      "         -2.7716e-02,  1.6255e-03, -6.3316e-02, -1.8258e-02, -4.5491e-02,\n",
      "          1.1397e-02,  7.0264e-02,  1.8543e-02, -4.3941e-02,  1.5809e-02,\n",
      "          1.0576e-02, -1.1876e-02,  1.1684e-02, -1.4857e-03, -1.3847e-02,\n",
      "         -3.5052e-03, -2.9399e-02, -3.4140e-02, -3.0366e-02, -4.2807e-02,\n",
      "          2.2627e-02,  9.7901e-06, -3.6235e-02, -5.8573e-03,  2.0378e-02,\n",
      "         -6.4172e-02,  3.1886e-02,  1.5496e-02, -2.8181e-02,  3.5096e-02,\n",
      "         -6.8952e-03, -2.5324e-02, -3.1501e-02,  7.9062e-03, -2.8681e-02,\n",
      "         -7.9037e-03,  3.5255e-02,  2.4252e-02,  2.9042e-02, -6.2465e-02,\n",
      "          1.4517e-02, -1.9319e-02, -2.7564e-02, -2.5307e-02,  3.7366e-02,\n",
      "         -3.2373e-02, -6.7099e-02,  1.8414e-02,  2.4672e-02,  1.2600e-02,\n",
      "         -5.6666e-03,  2.5549e-03,  4.0709e-03,  1.9000e-02, -6.1543e-02,\n",
      "          1.2990e-02, -2.5736e-02,  1.1150e-02, -2.5157e-02, -8.1259e-02,\n",
      "          2.5197e-02,  1.6732e-03,  2.1115e-02, -4.1658e-02,  6.6255e-02,\n",
      "         -2.5587e-02, -3.2157e-04,  1.2388e-02,  4.0023e-02, -6.1048e-02,\n",
      "          6.0717e-02,  2.1476e-02,  5.8257e-03,  4.3190e-02, -5.3267e-03,\n",
      "         -1.1518e-01,  5.6670e-03, -2.2334e-02, -2.0324e-02,  1.9542e-03,\n",
      "         -3.7446e-02,  1.5559e-02, -7.3694e-03,  2.4515e-02, -4.5967e-02,\n",
      "         -3.7894e-03,  3.2286e-02,  1.0811e-02,  4.4440e-03,  4.8463e-03,\n",
      "          2.5042e-02,  4.7359e-02, -6.8103e-02, -6.5093e-03,  7.0785e-02,\n",
      "         -3.5528e-02,  4.7189e-03, -1.0189e-02,  4.2352e-02, -2.0553e-02,\n",
      "         -1.7234e-02,  1.3394e-02,  1.8244e-02,  6.2473e-02, -4.8055e-02,\n",
      "          1.2645e-02, -7.0589e-03,  5.6752e-02, -3.4373e-02, -3.1496e-02,\n",
      "         -5.9777e-03, -2.4881e-02,  7.9201e-02,  1.3747e-02, -2.5393e-03,\n",
      "         -3.2503e-02, -3.1110e-02,  5.7813e-02, -4.5621e-03, -1.0531e-02,\n",
      "          4.2530e-02,  3.0264e-02, -6.1265e-03,  4.5427e-02, -5.1061e-02,\n",
      "         -1.5275e-01,  1.0616e-02,  2.6280e-02,  1.8947e-02,  3.2359e-02,\n",
      "         -1.4984e-02,  3.7450e-03, -6.5668e-02,  2.4765e-03, -3.2025e-02,\n",
      "          2.9947e-02,  4.5328e-03,  3.0595e-02, -5.2844e-03,  7.7454e-02,\n",
      "         -1.9671e-02,  2.9783e-02, -3.6750e-02, -1.6449e-02,  6.7979e-03,\n",
      "          6.7633e-03, -3.4163e-02,  2.9027e-02, -1.8871e-02,  3.0973e-02,\n",
      "         -3.5600e-02, -2.3323e-02, -2.5930e-02, -4.0542e-02,  2.5499e-02,\n",
      "          3.3001e-02,  1.2359e-02,  2.5706e-02,  3.0320e-02,  4.6401e-02,\n",
      "         -2.3879e-02,  2.0202e-02, -2.2375e-03,  8.2037e-03, -2.0483e-02,\n",
      "         -7.8328e-02, -1.1248e-02, -9.1449e-03,  7.5573e-03, -2.2487e-02,\n",
      "         -1.6329e-02,  1.8263e-02,  3.7522e-02,  1.9167e-02,  1.4377e-02,\n",
      "         -1.4204e-02,  3.2154e-02, -4.2945e-02,  5.7657e-03, -8.0327e-04,\n",
      "         -6.6083e-02, -2.8194e-04, -1.8378e-02, -4.8475e-02,  2.3617e-02,\n",
      "         -3.3750e-02,  1.1834e-02, -1.0890e-02,  2.5747e-02, -3.7289e-02,\n",
      "          5.2489e-02,  3.3673e-02,  2.8601e-02,  4.2754e-03, -4.7038e-02,\n",
      "         -3.0422e-02, -1.7425e-02,  1.3590e-02, -5.1289e-02, -2.6229e-02,\n",
      "         -1.0611e-02, -7.7437e-03,  2.7641e-02, -4.1801e-03,  2.6006e-02,\n",
      "         -3.3880e-03,  3.3067e-03, -5.1195e-02,  1.0351e-02, -5.3868e-03,\n",
      "         -3.0990e-02,  2.9896e-02, -2.5370e-02,  7.7419e-02, -1.1452e-02,\n",
      "         -2.1355e-02,  2.0652e-02,  5.2692e-02,  3.8496e-02, -5.2882e-02,\n",
      "          2.0792e-03, -8.6128e-02,  3.2290e-02, -1.0719e-02,  1.2896e-02,\n",
      "          3.0134e-02,  1.5601e-02,  3.8173e-02,  3.0548e-02,  4.3263e-02,\n",
      "          3.2726e-02,  1.9825e-02, -1.0034e-02, -2.1873e-02,  1.3308e-02,\n",
      "          3.9475e-02, -1.2140e-02,  8.5712e-02,  4.2187e-02, -2.3477e-02,\n",
      "         -1.9578e-02,  3.7402e-02,  1.4542e-03,  3.4245e-02, -1.5425e-02,\n",
      "         -2.8997e-03, -3.4361e-02,  3.4428e-02, -3.5852e-02,  1.1834e-02,\n",
      "         -3.3657e-03, -3.2083e-04, -5.2636e-02,  3.2873e-02, -6.1226e-04,\n",
      "          2.0334e-02,  3.3753e-02,  5.1543e-02,  4.3593e-03, -4.9230e-02,\n",
      "         -1.4788e-02, -2.5702e-02, -9.4918e-04, -9.5153e-03,  9.0001e-03,\n",
      "          2.0273e-02, -2.8655e-02, -2.4118e-02,  2.3451e-02,  2.7414e-02,\n",
      "         -3.3920e-02,  5.2001e-02,  3.6443e-02, -6.0504e-03,  7.4200e-02,\n",
      "         -5.9112e-02, -5.3881e-02, -1.7870e-02,  7.4283e-03, -2.7031e-02,\n",
      "         -1.5785e-02, -6.3814e-02,  4.7019e-02,  1.4489e-02, -3.2238e-02,\n",
      "          5.6374e-03, -6.0349e-03,  6.9934e-03,  2.6128e-02,  1.4643e-02,\n",
      "          1.8811e-03,  1.8494e-02, -9.8517e-04, -2.6079e-02, -2.9486e-02,\n",
      "         -7.7439e-02, -1.8391e-02, -4.0349e-02,  8.9021e-03,  2.3187e-02,\n",
      "         -5.5234e-02, -2.4531e-03,  7.9741e-03,  8.4657e-03,  4.5370e-03,\n",
      "         -2.5644e-02,  2.0494e-03, -3.9706e-02, -5.1241e-02,  1.2272e-02,\n",
      "         -8.8885e-03, -6.7159e-02,  5.1499e-02,  1.8548e-02, -5.7593e-02,\n",
      "          4.5964e-02,  2.3069e-02, -1.5816e-02, -1.7899e-02, -2.6860e-02,\n",
      "          3.9363e-02,  4.0149e-02,  3.6070e-03,  9.7411e-03, -8.0640e-02,\n",
      "          5.5356e-02,  1.6593e-02, -3.2203e-03,  7.8833e-02,  4.0068e-02,\n",
      "          4.2472e-02, -1.1705e-02,  7.8580e-02,  3.7629e-02, -6.7491e-02,\n",
      "          1.6742e-02, -1.4671e-02,  4.7917e-02, -2.8045e-02, -2.8992e-02,\n",
      "          1.3161e-03, -6.2237e-03, -1.4460e-02, -1.6555e-03,  1.5189e-02,\n",
      "          1.8578e-02, -3.5313e-02, -9.5090e-05,  7.6177e-02,  2.5247e-03,\n",
      "         -3.2401e-02,  2.0006e-02,  1.6701e-02, -4.7967e-02, -4.5037e-02,\n",
      "          2.0186e-02, -3.6078e-03, -8.5892e-02, -3.5775e-02,  4.8689e-02,\n",
      "          4.3603e-03,  6.5434e-02,  1.7247e-02, -1.3526e-02,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[-0.0056,  0.0097, -0.0002,  ...,  0.0010,  0.0012, -0.0051],\n",
      "         [-0.0059,  0.0085, -0.0015,  ...,  0.0023,  0.0019, -0.0056],\n",
      "         [-0.0075,  0.0084,  0.0003,  ...,  0.0040,  0.0011, -0.0061]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>), tensor([[[-0.0175, -0.0187,  0.0046,  ..., -0.0043, -0.0085, -0.0041],\n",
      "         [-0.0145, -0.0161,  0.0036,  ...,  0.0083,  0.0021, -0.0064],\n",
      "         [-0.0167, -0.0166,  0.0035,  ...,  0.0076,  0.0003, -0.0039]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tRUN backward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? True\n",
      "\t\t\t\tOh, then create full_batch_previous memory and state by (3, 4096) tensor filling 0.\n",
      "\t\t\t\tSet current_length_index... is it forward?? False\n",
      "\t\t\t\tOops, backward!! current_length_index = 0\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "binary_mask = tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True, False,  ...,  True,  True,  True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 0.0000, 1.1111, 1.1111],\n",
      "        [1.1111, 0.0000, 1.1111,  ..., 1.1111, 0.0000, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 9 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 4.0999e-03, -6.1420e-03, -4.0220e-03, -4.2812e-02,  2.2545e-03,\n",
      "         -5.5843e-02, -3.3867e-03, -2.4588e-02, -5.0965e-02,  5.2278e-03,\n",
      "          3.2208e-03, -6.6673e-02, -4.4035e-03, -3.9993e-02, -1.1946e-02,\n",
      "          5.1611e-02,  9.1874e-03, -3.4062e-03, -6.2161e-02, -1.2382e-02,\n",
      "          1.7805e-02,  3.6729e-03,  4.2529e-03,  1.7206e-02, -1.2278e-02,\n",
      "          1.4176e-02,  6.9620e-02,  5.3863e-02,  2.9146e-02,  1.3687e-04,\n",
      "          4.2660e-03,  2.6004e-02, -2.9728e-04,  2.5874e-02,  5.3049e-04,\n",
      "         -9.1188e-03, -2.0923e-03, -3.1675e-02, -6.1669e-02,  2.7857e-02,\n",
      "          3.7919e-02,  4.7040e-02, -2.9004e-02,  2.8061e-02,  2.2440e-02,\n",
      "         -1.7007e-02,  4.6762e-02, -9.9050e-03, -6.0391e-02,  4.4008e-02,\n",
      "          2.7127e-02, -1.0071e-02, -4.4653e-02, -3.4113e-02, -5.4936e-02,\n",
      "         -2.7576e-02, -1.1772e-02,  1.6111e-02, -2.4791e-02, -1.2392e-02,\n",
      "         -4.5851e-02, -7.0682e-02,  6.8990e-02,  1.5804e-02, -1.8523e-02,\n",
      "          1.7020e-02,  3.7107e-03, -3.7832e-02, -9.7512e-03,  2.5285e-02,\n",
      "         -1.8639e-02, -1.1800e-02, -3.4856e-02,  2.3652e-03, -4.5387e-02,\n",
      "         -2.9169e-02, -7.3532e-03, -5.6582e-02, -4.0548e-02,  2.6095e-02,\n",
      "         -4.1448e-03, -7.7175e-03, -4.9508e-03,  5.8557e-02,  7.4085e-03,\n",
      "          2.0493e-02, -2.6433e-02, -5.9150e-02, -8.5825e-02, -3.6332e-02,\n",
      "         -6.7253e-02, -1.8667e-02,  1.9783e-02,  4.1228e-02,  3.5272e-02,\n",
      "         -1.5310e-02,  2.6563e-02,  9.0099e-03, -5.8591e-02, -1.2875e-02,\n",
      "         -4.5589e-02,  3.7654e-02,  5.1700e-02, -9.7946e-03, -2.8365e-02,\n",
      "          3.8642e-02,  3.2772e-02, -7.0115e-03,  1.7229e-02,  4.8634e-02,\n",
      "         -4.3900e-02, -5.7408e-02,  4.0625e-02,  4.4493e-02, -5.2082e-02,\n",
      "         -7.4520e-02, -5.9377e-02, -6.7939e-02, -7.3847e-02,  1.3183e-02,\n",
      "         -4.9373e-03,  1.8830e-02,  9.7497e-02,  1.3256e-02,  1.6883e-03,\n",
      "          1.4859e-02, -2.3568e-03,  5.6624e-02,  5.3152e-02, -1.2295e-02,\n",
      "          1.2090e-02, -1.3728e-05,  1.0096e-02,  3.3864e-02,  4.2704e-02,\n",
      "          5.4648e-02,  4.1818e-03,  3.7166e-03, -1.5428e-02,  4.5995e-02,\n",
      "          2.1937e-02, -2.8221e-02,  1.6410e-02, -4.7979e-04,  4.1428e-02,\n",
      "         -2.7716e-02,  1.6255e-03, -6.3316e-02, -1.8258e-02, -4.5491e-02,\n",
      "          1.1397e-02,  7.0264e-02,  1.8543e-02, -4.3941e-02,  1.5809e-02,\n",
      "          1.0576e-02, -1.1876e-02,  1.1684e-02, -1.4857e-03, -1.3847e-02,\n",
      "         -3.5052e-03, -2.9399e-02, -3.4140e-02, -3.0366e-02, -4.2807e-02,\n",
      "          2.2627e-02,  9.7901e-06, -3.6235e-02, -5.8573e-03,  2.0378e-02,\n",
      "         -6.4172e-02,  3.1886e-02,  1.5496e-02, -2.8181e-02,  3.5096e-02,\n",
      "         -6.8952e-03, -2.5324e-02, -3.1501e-02,  7.9062e-03, -2.8681e-02,\n",
      "         -7.9037e-03,  3.5255e-02,  2.4252e-02,  2.9042e-02, -6.2465e-02,\n",
      "          1.4517e-02, -1.9319e-02, -2.7564e-02, -2.5307e-02,  3.7366e-02,\n",
      "         -3.2373e-02, -6.7099e-02,  1.8414e-02,  2.4672e-02,  1.2600e-02,\n",
      "         -5.6666e-03,  2.5549e-03,  4.0709e-03,  1.9000e-02, -6.1543e-02,\n",
      "          1.2990e-02, -2.5736e-02,  1.1150e-02, -2.5157e-02, -8.1259e-02,\n",
      "          2.5197e-02,  1.6732e-03,  2.1115e-02, -4.1658e-02,  6.6255e-02,\n",
      "         -2.5587e-02, -3.2157e-04,  1.2388e-02,  4.0023e-02, -6.1048e-02,\n",
      "          6.0717e-02,  2.1476e-02,  5.8257e-03,  4.3190e-02, -5.3267e-03,\n",
      "         -1.1518e-01,  5.6670e-03, -2.2334e-02, -2.0324e-02,  1.9542e-03,\n",
      "         -3.7446e-02,  1.5559e-02, -7.3694e-03,  2.4515e-02, -4.5967e-02,\n",
      "         -3.7894e-03,  3.2286e-02,  1.0811e-02,  4.4440e-03,  4.8463e-03,\n",
      "          2.5042e-02,  4.7359e-02, -6.8103e-02, -6.5093e-03,  7.0785e-02,\n",
      "         -3.5528e-02,  4.7189e-03, -1.0189e-02,  4.2352e-02, -2.0553e-02,\n",
      "         -1.7234e-02,  1.3394e-02,  1.8244e-02,  6.2473e-02, -4.8055e-02,\n",
      "          1.2645e-02, -7.0589e-03,  5.6752e-02, -3.4373e-02, -3.1496e-02,\n",
      "         -5.9777e-03, -2.4881e-02,  7.9201e-02,  1.3747e-02, -2.5393e-03,\n",
      "         -3.2503e-02, -3.1110e-02,  5.7813e-02, -4.5621e-03, -1.0531e-02,\n",
      "          4.2530e-02,  3.0264e-02, -6.1265e-03,  4.5427e-02, -5.1061e-02,\n",
      "         -1.5275e-01,  1.0616e-02,  2.6280e-02,  1.8947e-02,  3.2359e-02,\n",
      "         -1.4984e-02,  3.7450e-03, -6.5668e-02,  2.4765e-03, -3.2025e-02,\n",
      "          2.9947e-02,  4.5328e-03,  3.0595e-02, -5.2844e-03,  7.7454e-02,\n",
      "         -1.9671e-02,  2.9783e-02, -3.6750e-02, -1.6449e-02,  6.7979e-03,\n",
      "          6.7633e-03, -3.4163e-02,  2.9027e-02, -1.8871e-02,  3.0973e-02,\n",
      "         -3.5600e-02, -2.3323e-02, -2.5930e-02, -4.0542e-02,  2.5499e-02,\n",
      "          3.3001e-02,  1.2359e-02,  2.5706e-02,  3.0320e-02,  4.6401e-02,\n",
      "         -2.3879e-02,  2.0202e-02, -2.2375e-03,  8.2037e-03, -2.0483e-02,\n",
      "         -7.8328e-02, -1.1248e-02, -9.1449e-03,  7.5573e-03, -2.2487e-02,\n",
      "         -1.6329e-02,  1.8263e-02,  3.7522e-02,  1.9167e-02,  1.4377e-02,\n",
      "         -1.4204e-02,  3.2154e-02, -4.2945e-02,  5.7657e-03, -8.0327e-04,\n",
      "         -6.6083e-02, -2.8194e-04, -1.8378e-02, -4.8475e-02,  2.3617e-02,\n",
      "         -3.3750e-02,  1.1834e-02, -1.0890e-02,  2.5747e-02, -3.7289e-02,\n",
      "          5.2489e-02,  3.3673e-02,  2.8601e-02,  4.2754e-03, -4.7038e-02,\n",
      "         -3.0422e-02, -1.7425e-02,  1.3590e-02, -5.1289e-02, -2.6229e-02,\n",
      "         -1.0611e-02, -7.7437e-03,  2.7641e-02, -4.1801e-03,  2.6006e-02,\n",
      "         -3.3880e-03,  3.3067e-03, -5.1195e-02,  1.0351e-02, -5.3868e-03,\n",
      "         -3.0990e-02,  2.9896e-02, -2.5370e-02,  7.7419e-02, -1.1452e-02,\n",
      "         -2.1355e-02,  2.0652e-02,  5.2692e-02,  3.8496e-02, -5.2882e-02,\n",
      "          2.0792e-03, -8.6128e-02,  3.2290e-02, -1.0719e-02,  1.2896e-02,\n",
      "          3.0134e-02,  1.5601e-02,  3.8173e-02,  3.0548e-02,  4.3263e-02,\n",
      "          3.2726e-02,  1.9825e-02, -1.0034e-02, -2.1873e-02,  1.3308e-02,\n",
      "          3.9475e-02, -1.2140e-02,  8.5712e-02,  4.2187e-02, -2.3477e-02,\n",
      "         -1.9578e-02,  3.7402e-02,  1.4542e-03,  3.4245e-02, -1.5425e-02,\n",
      "         -2.8997e-03, -3.4361e-02,  3.4428e-02, -3.5852e-02,  1.1834e-02,\n",
      "         -3.3657e-03, -3.2083e-04, -5.2636e-02,  3.2873e-02, -6.1226e-04,\n",
      "          2.0334e-02,  3.3753e-02,  5.1543e-02,  4.3593e-03, -4.9230e-02,\n",
      "         -1.4788e-02, -2.5702e-02, -9.4918e-04, -9.5153e-03,  9.0001e-03,\n",
      "          2.0273e-02, -2.8655e-02, -2.4118e-02,  2.3451e-02,  2.7414e-02,\n",
      "         -3.3920e-02,  5.2001e-02,  3.6443e-02, -6.0504e-03,  7.4200e-02,\n",
      "         -5.9112e-02, -5.3881e-02, -1.7870e-02,  7.4283e-03, -2.7031e-02,\n",
      "         -1.5785e-02, -6.3814e-02,  4.7019e-02,  1.4489e-02, -3.2238e-02,\n",
      "          5.6374e-03, -6.0349e-03,  6.9934e-03,  2.6128e-02,  1.4643e-02,\n",
      "          1.8811e-03,  1.8494e-02, -9.8517e-04, -2.6079e-02, -2.9486e-02,\n",
      "         -7.7439e-02, -1.8391e-02, -4.0349e-02,  8.9021e-03,  2.3187e-02,\n",
      "         -5.5234e-02, -2.4531e-03,  7.9741e-03,  8.4657e-03,  4.5370e-03,\n",
      "         -2.5644e-02,  2.0494e-03, -3.9706e-02, -5.1241e-02,  1.2272e-02,\n",
      "         -8.8885e-03, -6.7159e-02,  5.1499e-02,  1.8548e-02, -5.7593e-02,\n",
      "          4.5964e-02,  2.3069e-02, -1.5816e-02, -1.7899e-02, -2.6860e-02,\n",
      "          3.9363e-02,  4.0149e-02,  3.6070e-03,  9.7411e-03, -8.0640e-02,\n",
      "          5.5356e-02,  1.6593e-02, -3.2203e-03,  7.8833e-02,  4.0068e-02,\n",
      "          4.2472e-02, -1.1705e-02,  7.8580e-02,  3.7629e-02, -6.7491e-02,\n",
      "          1.6742e-02, -1.4671e-02,  4.7917e-02, -2.8045e-02, -2.8992e-02,\n",
      "          1.3161e-03, -6.2237e-03, -1.4460e-02, -1.6555e-03,  1.5189e-02,\n",
      "          1.8578e-02, -3.5313e-02, -9.5090e-05,  7.6177e-02,  2.5247e-03,\n",
      "         -3.2401e-02,  2.0006e-02,  1.6701e-02, -4.7967e-02, -4.5037e-02,\n",
      "          2.0186e-02, -3.6078e-03, -8.5892e-02, -3.5775e-02,  4.8689e-02,\n",
      "          4.3603e-03,  6.5434e-02,  1.7247e-02, -1.3526e-02,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0074, -0.0082,  0.0105,  ..., -0.0006,  0.0003,  0.0041]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0074, -0.0082,  0.0105,  ..., -0.0006,  0.0003,  0.0041]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0225,  0.0021, -0.0073, -0.0453,  0.0067, -0.1035,  0.0007, -0.0384,\n",
      "         -0.0241, -0.0236, -0.0056, -0.0469,  0.0058, -0.0028,  0.0082,  0.0394,\n",
      "          0.0411,  0.0036, -0.0933, -0.0109,  0.0350, -0.0014,  0.0195,  0.0090,\n",
      "          0.0102,  0.0144,  0.0910,  0.0898,  0.0680, -0.0267,  0.0296,  0.0123,\n",
      "         -0.0376,  0.0462,  0.0312, -0.0027,  0.0219, -0.0156, -0.0906, -0.0108,\n",
      "          0.0491, -0.0022, -0.0330,  0.0454,  0.0668, -0.0107,  0.0695, -0.0285,\n",
      "         -0.1164,  0.0746,  0.0207,  0.0046, -0.0736, -0.0499, -0.0580, -0.0040,\n",
      "          0.0032,  0.0278,  0.0149, -0.0144,  0.0139, -0.0536,  0.0244,  0.0268,\n",
      "         -0.0166, -0.0146, -0.0104, -0.0225, -0.0341,  0.0614, -0.0234, -0.0311,\n",
      "         -0.0392,  0.0299, -0.0474, -0.0211, -0.0228, -0.0282, -0.0288, -0.0021,\n",
      "         -0.0283, -0.0399,  0.0053,  0.0269, -0.0246,  0.0394, -0.0482, -0.0131,\n",
      "         -0.0672, -0.0286, -0.0382, -0.0050, -0.0136,  0.0675,  0.0515,  0.0383,\n",
      "          0.0347, -0.0290, -0.0956,  0.0055, -0.0542,  0.0374,  0.0811, -0.0625,\n",
      "          0.0048,  0.0439,  0.0274, -0.0130,  0.0037,  0.0702, -0.0189, -0.0653,\n",
      "          0.0338,  0.0484, -0.0758, -0.1118, -0.0982, -0.0586, -0.1017,  0.0323,\n",
      "         -0.0187,  0.0306,  0.0579,  0.0252, -0.0435,  0.0251, -0.0279,  0.0555,\n",
      "          0.0822,  0.0015,  0.0172,  0.0274, -0.0081,  0.0368,  0.0218,  0.0329,\n",
      "          0.0185,  0.0088, -0.0144,  0.0301,  0.0175, -0.0075,  0.0198,  0.0053,\n",
      "          0.0708,  0.0166, -0.0065, -0.0641, -0.0199, -0.0439,  0.0324,  0.0846,\n",
      "         -0.0156, -0.0623,  0.0368,  0.0376, -0.0219, -0.0321,  0.0116, -0.0214,\n",
      "         -0.0275, -0.0293, -0.0180, -0.0346, -0.0491,  0.0416, -0.0381, -0.0820,\n",
      "         -0.0298,  0.0151, -0.0111,  0.0488,  0.0482, -0.0697,  0.0230, -0.0346,\n",
      "         -0.0385, -0.0320,  0.0022, -0.0195,  0.0382,  0.0669,  0.0117, -0.0143,\n",
      "         -0.0986,  0.0230, -0.0114,  0.0136, -0.0391,  0.0525, -0.0245, -0.0297,\n",
      "         -0.0252,  0.0744,  0.0637,  0.0055,  0.0261,  0.0363, -0.0025, -0.0683,\n",
      "         -0.0255, -0.0456,  0.0402, -0.0391, -0.0926,  0.0581, -0.0183, -0.0362,\n",
      "         -0.0535,  0.0409, -0.0179,  0.0289,  0.0655,  0.0294, -0.0775,  0.0396,\n",
      "          0.0239,  0.0403,  0.0801, -0.0544, -0.1100, -0.0068, -0.0387, -0.0217,\n",
      "          0.0214, -0.0616, -0.0089, -0.0123,  0.0534, -0.0370,  0.0211,  0.0167,\n",
      "         -0.0111, -0.0011,  0.0025,  0.0404,  0.0642, -0.0452, -0.0113,  0.0538,\n",
      "         -0.0520, -0.0056,  0.0366,  0.0759, -0.0529, -0.0374,  0.0588,  0.0106,\n",
      "          0.1057, -0.0457, -0.0078,  0.0133,  0.0720, -0.0095, -0.0401,  0.0218,\n",
      "         -0.0456,  0.0502,  0.0087, -0.0067, -0.0664, -0.0569,  0.0319,  0.0456,\n",
      "          0.0117,  0.0440,  0.0148, -0.0104,  0.0672, -0.0583, -0.1598,  0.0023,\n",
      "          0.0538, -0.0143,  0.0224,  0.0161, -0.0428, -0.0923, -0.0291, -0.0308,\n",
      "         -0.0071, -0.0245,  0.0439, -0.0270,  0.0261, -0.0623,  0.0007, -0.0537,\n",
      "         -0.0007, -0.0388,  0.0227, -0.0308,  0.0428, -0.0017,  0.0260, -0.0640,\n",
      "         -0.0064, -0.0349, -0.0219,  0.0544,  0.0239,  0.0471,  0.0232,  0.0261,\n",
      "          0.0282, -0.0696,  0.0527,  0.0118,  0.0263, -0.0006, -0.0661,  0.0209,\n",
      "          0.0058,  0.0030, -0.0387,  0.0064,  0.0166,  0.0501, -0.0120,  0.0489,\n",
      "          0.0016,  0.0325, -0.0278,  0.0218,  0.0103, -0.0845, -0.0082, -0.0481,\n",
      "         -0.0449,  0.0369, -0.0878, -0.0075, -0.0770,  0.0889, -0.0595,  0.0261,\n",
      "          0.0722,  0.0245, -0.0125, -0.0249, -0.0267, -0.0498, -0.0126, -0.0515,\n",
      "         -0.0232, -0.0248, -0.0696,  0.0107, -0.0039,  0.0008,  0.0189,  0.0703,\n",
      "         -0.0633,  0.0129,  0.0091, -0.0232,  0.0251, -0.0279,  0.1155, -0.0159,\n",
      "         -0.0343,  0.0638, -0.0056,  0.0369, -0.0636, -0.0179, -0.1186,  0.0721,\n",
      "         -0.0307,  0.0792,  0.0270,  0.0300,  0.0747,  0.0376,  0.0251,  0.0128,\n",
      "          0.0466, -0.0187, -0.0412,  0.0287,  0.0269, -0.0115,  0.0786,  0.0066,\n",
      "         -0.0067,  0.0407,  0.0497,  0.0495,  0.0253, -0.0395, -0.0038, -0.0613,\n",
      "          0.0059, -0.0225,  0.0075,  0.0116,  0.0301, -0.0510,  0.0483, -0.0205,\n",
      "          0.0458,  0.0350,  0.0831, -0.0208, -0.0651,  0.0033, -0.0349,  0.0098,\n",
      "         -0.0437,  0.0113, -0.0011, -0.0237, -0.0102,  0.0026,  0.0296, -0.0294,\n",
      "          0.0504,  0.0353, -0.0013,  0.0868, -0.0338, -0.1060, -0.0469, -0.0076,\n",
      "         -0.0594, -0.0139, -0.1025,  0.0495,  0.0576, -0.0711,  0.0156, -0.0270,\n",
      "          0.0126,  0.0289,  0.0422, -0.0147, -0.0343,  0.0242, -0.0238,  0.0042,\n",
      "         -0.0163, -0.0254, -0.0318, -0.0033, -0.0148, -0.0554, -0.0152,  0.0177,\n",
      "         -0.0006,  0.0078,  0.0026, -0.0279, -0.0466, -0.0133, -0.0026,  0.0167,\n",
      "         -0.0890,  0.0875, -0.0195, -0.0232,  0.0426, -0.0253, -0.0016, -0.0328,\n",
      "          0.0101,  0.0372,  0.0486, -0.0026, -0.0226, -0.0536,  0.0217,  0.0102,\n",
      "         -0.0430,  0.0425,  0.0180, -0.0253,  0.0030,  0.0580,  0.0355, -0.0872,\n",
      "          0.0397, -0.0145,  0.0678,  0.0079, -0.0342,  0.0471,  0.0262,  0.0146,\n",
      "         -0.0287, -0.0009, -0.0416,  0.0084, -0.0227,  0.0526, -0.0619, -0.0201,\n",
      "          0.0613,  0.0342, -0.0878, -0.0522,  0.0243, -0.0385, -0.0821, -0.0479,\n",
      "          0.0745, -0.0005,  0.0558,  0.0333, -0.0184,  0.0317,  0.0303, -0.0447]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0095, -0.0144,  0.0128,  ..., -0.0055, -0.0050,  0.0099]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0095, -0.0144,  0.0128,  ..., -0.0055, -0.0050,  0.0099]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 3.9943e-02, -6.3473e-04, -2.4333e-02, -2.6465e-02,  3.6282e-02,\n",
      "         -8.1245e-02,  2.0234e-02, -4.7533e-02, -1.4706e-02,  1.8934e-02,\n",
      "          4.9761e-04, -6.6569e-02,  3.0510e-02,  8.3242e-03,  3.4185e-03,\n",
      "          4.2514e-02,  3.6875e-02,  1.1155e-02, -6.4569e-02, -2.6975e-02,\n",
      "          1.4409e-02, -8.3901e-03,  1.4958e-02,  1.4861e-02,  1.6609e-02,\n",
      "          4.6618e-02,  8.1440e-02,  6.3781e-02,  4.3447e-02,  5.9358e-03,\n",
      "         -5.9265e-04,  3.1957e-02, -3.8824e-03,  6.6324e-02,  1.1476e-03,\n",
      "          3.0135e-02,  2.8017e-02, -2.6136e-03, -8.7879e-02,  8.0971e-03,\n",
      "          4.2865e-02,  2.4699e-02, -1.6333e-03,  4.5484e-02,  4.8548e-02,\n",
      "         -2.7261e-02,  4.9371e-02, -1.1094e-02, -6.2017e-02,  6.3433e-02,\n",
      "          1.1362e-02, -3.7070e-02, -2.8188e-02, -3.8195e-02, -5.8539e-02,\n",
      "          1.2449e-02, -2.7576e-03,  3.2095e-02,  7.0868e-03,  7.0801e-03,\n",
      "         -2.9167e-02, -6.1884e-02,  3.3293e-02,  7.4093e-03,  8.6460e-03,\n",
      "          2.1394e-02,  2.8104e-02, -2.3261e-02, -4.2063e-02,  2.1559e-02,\n",
      "         -7.4343e-03, -4.5396e-02, -1.7797e-02, -8.0800e-03, -5.4489e-02,\n",
      "         -7.8084e-03,  5.2327e-03, -1.9146e-02, -4.8832e-02,  2.1798e-02,\n",
      "          9.2141e-03, -3.6777e-02,  6.6118e-03,  3.2340e-02,  1.9615e-02,\n",
      "          2.1029e-02, -2.2645e-02, -2.0520e-02, -8.7323e-02, -4.5272e-02,\n",
      "         -2.7091e-02, -2.3780e-02, -2.8575e-02,  7.5322e-02,  2.6836e-02,\n",
      "         -1.4969e-02,  1.4021e-02,  3.6706e-03, -6.0353e-02, -9.9927e-03,\n",
      "         -7.4608e-02, -6.0827e-03,  4.9744e-02, -1.1646e-02, -1.5283e-02,\n",
      "          3.9851e-02,  1.9263e-02, -2.9827e-02,  3.6977e-02,  4.5320e-02,\n",
      "         -4.4115e-02, -5.5769e-02,  1.1291e-02,  3.4049e-02, -4.0742e-02,\n",
      "         -7.8004e-02, -3.9623e-02, -6.5474e-02, -5.7309e-02,  3.2225e-03,\n",
      "          4.3883e-03,  2.9382e-02,  7.0717e-02,  6.7704e-02, -2.4883e-02,\n",
      "          2.6200e-02, -1.4513e-02,  2.5271e-02,  3.6185e-02, -6.2445e-03,\n",
      "         -1.5884e-02, -5.3081e-03, -5.1873e-03,  4.8760e-02,  4.0506e-02,\n",
      "          4.3785e-02,  1.0459e-02,  5.6056e-02, -4.2227e-03,  4.8874e-02,\n",
      "         -2.1245e-02, -3.1359e-03,  1.8048e-02, -2.3473e-02,  5.5935e-02,\n",
      "         -3.8959e-02,  1.0918e-02, -8.7178e-02,  3.0119e-03, -2.1534e-02,\n",
      "          3.2873e-03,  7.5276e-02, -3.9145e-02, -2.3037e-02,  1.6807e-02,\n",
      "          5.1875e-02, -6.6744e-05, -2.1719e-02,  3.4947e-02, -1.9906e-02,\n",
      "         -4.2533e-02, -1.4287e-02, -1.7697e-02, -3.5453e-02, -2.6380e-02,\n",
      "          3.0872e-03, -3.2254e-02, -5.6165e-02, -1.4399e-02,  6.4918e-04,\n",
      "         -4.0651e-02,  5.3044e-02,  5.6464e-02, -7.1317e-02,  2.5132e-02,\n",
      "         -2.7497e-02, -1.5007e-02, -4.0442e-02,  3.2662e-02, -3.9329e-02,\n",
      "          1.1346e-02,  5.2525e-02,  2.5804e-02,  2.6124e-02, -8.0331e-02,\n",
      "          1.4954e-02,  2.4630e-02,  1.5274e-02, -4.8547e-02,  3.7759e-02,\n",
      "         -2.4851e-02, -6.2086e-02,  1.5069e-02,  5.3204e-02,  1.5821e-02,\n",
      "         -2.3344e-02, -1.3930e-03,  1.4708e-02,  1.1591e-02, -7.3345e-02,\n",
      "         -3.6105e-02, -2.8722e-02,  7.6852e-04, -2.2699e-02, -8.3008e-02,\n",
      "          7.0183e-02, -3.0014e-02, -9.3609e-03, -2.8232e-02,  6.1581e-02,\n",
      "         -2.4002e-02,  1.5176e-02,  5.0153e-02, -9.7399e-04, -5.9783e-02,\n",
      "          3.4617e-02, -2.9192e-03,  2.7485e-02,  6.9439e-02, -1.1364e-02,\n",
      "         -1.3452e-01,  2.2476e-02, -1.4151e-02, -7.1442e-03, -1.2852e-02,\n",
      "         -2.2974e-02,  2.5305e-02,  9.9605e-03,  2.9039e-03, -3.2136e-02,\n",
      "          1.0917e-02,  3.9640e-02, -1.0201e-02, -1.9363e-02, -1.1269e-02,\n",
      "          9.5938e-03,  5.2384e-02, -5.6701e-02, -8.5196e-03,  1.2629e-02,\n",
      "         -4.5141e-02, -2.6792e-02, -1.0696e-02,  2.9739e-02, -5.2543e-02,\n",
      "          4.4625e-03,  3.8448e-02, -1.4611e-03,  7.8076e-02, -1.0361e-02,\n",
      "         -1.6192e-02, -7.5027e-03,  5.8933e-02, -8.2104e-03, -1.0303e-02,\n",
      "         -1.0491e-02, -6.1503e-02,  3.2754e-02, -1.5981e-02, -2.7549e-02,\n",
      "         -5.1394e-02, -3.0974e-02,  4.4842e-02,  1.3567e-02, -4.8374e-03,\n",
      "          4.6250e-02,  4.9839e-02, -1.7988e-02,  3.7572e-02, -4.8327e-02,\n",
      "         -1.2389e-01,  9.6038e-03,  2.2187e-02,  1.4403e-02,  4.4363e-03,\n",
      "         -7.8948e-03, -3.4436e-02, -5.8862e-02, -3.2297e-02,  2.4687e-03,\n",
      "          1.3516e-02, -1.9046e-02,  6.2428e-02, -2.4486e-02,  2.5915e-02,\n",
      "         -4.7463e-02,  4.1579e-02, -1.3121e-02, -2.5279e-02, -1.6883e-02,\n",
      "         -1.2437e-02, -6.8095e-02,  3.2792e-02, -3.9081e-02,  2.3718e-02,\n",
      "         -7.6230e-03,  2.5899e-02, -9.0227e-03, -4.4538e-02,  1.8885e-02,\n",
      "          6.7445e-03,  2.3967e-02,  3.6491e-02,  6.1667e-02,  2.9088e-02,\n",
      "         -4.6740e-02,  4.2268e-02,  5.3146e-03,  3.4716e-03,  1.3579e-02,\n",
      "         -3.4293e-02, -7.6553e-03, -3.0861e-02,  2.1517e-02, -1.5814e-02,\n",
      "          1.6505e-02,  2.4900e-02,  3.6272e-02, -1.1502e-02,  2.1404e-02,\n",
      "         -1.9012e-02,  4.1999e-02, -2.3130e-02, -1.2259e-03, -3.5337e-02,\n",
      "         -6.3823e-02, -1.7515e-02, -4.7187e-02, -2.1176e-02,  2.9516e-02,\n",
      "         -5.6702e-02, -3.0155e-02, -4.3931e-02,  5.9581e-02, -4.8076e-02,\n",
      "          3.2992e-02,  5.8642e-02,  4.0638e-02, -4.9440e-03, -2.6222e-02,\n",
      "         -1.7771e-02, -4.0846e-02,  2.8135e-02, -4.8641e-02, -3.6589e-02,\n",
      "         -4.3364e-03, -4.0931e-02,  1.2174e-02, -2.9099e-02, -3.0528e-02,\n",
      "          1.0307e-02,  6.1238e-02, -5.8316e-02, -1.3105e-02,  9.0719e-03,\n",
      "         -1.7113e-02, -1.6299e-02, -4.1770e-02,  5.2599e-02, -2.1581e-02,\n",
      "          1.3731e-02,  3.1022e-02, -9.9482e-03,  2.3787e-02, -7.2789e-02,\n",
      "          3.9918e-03, -1.1121e-01,  6.5336e-02, -3.4317e-02,  6.3555e-02,\n",
      "          4.7264e-02, -2.1690e-02,  3.2510e-02, -2.9224e-03,  8.1941e-03,\n",
      "          2.2115e-03,  4.4054e-02, -2.8647e-02, -1.7062e-02,  3.1700e-02,\n",
      "          1.3082e-02, -7.8945e-03,  5.9706e-02, -3.6144e-03,  1.2505e-02,\n",
      "          3.5541e-02,  5.5393e-02,  7.4438e-03,  3.9664e-02, -2.1253e-02,\n",
      "         -6.8311e-03, -4.3957e-02,  3.7919e-02, -2.8258e-02, -8.0768e-03,\n",
      "         -4.2647e-02,  3.2717e-02, -2.9899e-02, -9.3153e-03,  1.5475e-02,\n",
      "          3.0740e-02,  1.8109e-02,  2.8607e-02, -3.8829e-02, -4.7982e-02,\n",
      "         -7.1243e-03, -2.4547e-02,  8.3619e-04, -1.8965e-02,  1.3804e-02,\n",
      "          1.0712e-02, -3.4088e-02, -7.9899e-02, -7.0846e-03,  6.9593e-03,\n",
      "         -2.1685e-02,  5.5655e-02, -1.2060e-02,  2.3431e-03,  7.2531e-02,\n",
      "         -6.0682e-02, -7.4575e-02, -1.7317e-02,  5.8419e-03, -2.3470e-02,\n",
      "          9.9523e-03, -6.2580e-02,  7.6685e-02,  2.9135e-02, -3.8954e-02,\n",
      "          1.4604e-02,  3.1968e-03,  3.2509e-02, -2.8317e-03,  2.2350e-02,\n",
      "         -4.7101e-02, -5.0960e-02,  3.1412e-03, -1.5378e-02, -2.6794e-02,\n",
      "         -1.9578e-02,  1.0552e-02, -6.9772e-02,  2.0636e-02, -2.9148e-03,\n",
      "         -4.0716e-02,  2.5176e-03,  7.4695e-03, -3.4024e-04,  3.2252e-02,\n",
      "         -2.8218e-02, -1.9674e-02, -2.1514e-02, -9.1328e-03,  2.0247e-02,\n",
      "          3.0113e-02, -7.1527e-02,  4.6293e-02,  2.1005e-02, -2.0541e-02,\n",
      "          4.0748e-02,  4.6423e-03, -2.4288e-02, -1.5478e-02, -3.5585e-02,\n",
      "          2.1604e-02,  3.8710e-02,  1.4737e-02, -3.1023e-02, -5.9019e-02,\n",
      "          3.0318e-02,  3.5039e-02, -1.1337e-03,  3.8222e-02,  1.5961e-02,\n",
      "          2.4988e-02,  1.0752e-02,  7.9247e-02,  3.4722e-02, -7.4136e-02,\n",
      "          3.9050e-02, -1.8820e-02,  6.0536e-02,  6.0405e-03, -1.8136e-02,\n",
      "          4.7733e-02,  4.5106e-03, -5.9822e-03, -1.5836e-02,  1.4583e-03,\n",
      "         -2.1234e-02, -2.3092e-02, -6.0511e-03,  7.6471e-02, -1.0707e-02,\n",
      "         -3.3524e-02,  3.2486e-02, -7.2459e-03, -6.4545e-02, -4.4839e-02,\n",
      "          3.7363e-02, -2.0615e-02, -1.1821e-01, -2.0830e-02,  6.9055e-02,\n",
      "         -1.0918e-02,  5.4751e-02,  2.7437e-02,  3.1381e-02,  1.9435e-02,\n",
      "          5.1162e-02, -3.3509e-02]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0166, -0.0123,  0.0146,  ..., -0.0086, -0.0142,  0.0114],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0166, -0.0123,  0.0146,  ..., -0.0086, -0.0142,  0.0114],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 7.8088e-03, -2.3229e-02, -1.9114e-02,  ...,  2.5953e-02,\n",
      "          5.1098e-02, -5.5057e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0254, -0.0145,  0.0156,  ..., -0.0080, -0.0152,  0.0173],\n",
      "        [-0.0074, -0.0082,  0.0105,  ..., -0.0006,  0.0003,  0.0041],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0254, -0.0145,  0.0156,  ..., -0.0080, -0.0152,  0.0173],\n",
      "        [-0.0074, -0.0082,  0.0105,  ..., -0.0006,  0.0003,  0.0041],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-6.8748e-03, -1.3605e-02,  7.1700e-03,  ...,  1.2522e-02,\n",
      "          2.0355e-02, -2.4865e-02],\n",
      "        [-2.9532e-03, -1.3722e-02, -2.7846e-02,  ..., -3.4922e-03,\n",
      "          1.9039e-03, -3.1346e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0288, -0.0176,  0.0161,  ..., -0.0036, -0.0179,  0.0201],\n",
      "        [-0.0119, -0.0143,  0.0138,  ..., -0.0041, -0.0078,  0.0027],\n",
      "        [-0.0074, -0.0082,  0.0105,  ..., -0.0006,  0.0003,  0.0041]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0288, -0.0176,  0.0161,  ..., -0.0036, -0.0179,  0.0201],\n",
      "        [-0.0119, -0.0143,  0.0138,  ..., -0.0041, -0.0078,  0.0027],\n",
      "        [-0.0074, -0.0082,  0.0105,  ..., -0.0006,  0.0003,  0.0041]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0267,  0.0010, -0.0196,  ...,  0.0005,  0.0283, -0.0203],\n",
      "        [ 0.0260, -0.0238,  0.0250,  ...,  0.0017,  0.0297, -0.0235],\n",
      "        [-0.0157,  0.0032, -0.0359,  ...,  0.0232,  0.0049, -0.0217]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0267, -0.0184,  0.0145,  ..., -0.0093, -0.0186,  0.0160],\n",
      "        [-0.0162, -0.0158,  0.0151,  ..., -0.0070, -0.0072,  0.0118],\n",
      "        [-0.0139, -0.0135,  0.0160,  ..., -0.0010, -0.0061,  0.0089]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0267, -0.0184,  0.0145,  ..., -0.0093, -0.0186,  0.0160],\n",
      "        [-0.0162, -0.0158,  0.0151,  ..., -0.0070, -0.0072,  0.0118],\n",
      "        [-0.0139, -0.0135,  0.0160,  ..., -0.0010, -0.0061,  0.0089]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0195,  0.0024, -0.0233,  ...,  0.0120,  0.0189, -0.0226],\n",
      "        [ 0.0259, -0.0076, -0.0144,  ...,  0.0053,  0.0274, -0.0328],\n",
      "        [ 0.0088, -0.0010, -0.0169,  ...,  0.0276,  0.0261, -0.0068]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0227, -0.0257,  0.0167,  ..., -0.0118, -0.0202,  0.0177],\n",
      "        [-0.0179, -0.0171,  0.0119,  ..., -0.0076, -0.0041,  0.0134],\n",
      "        [-0.0206, -0.0116,  0.0175,  ..., -0.0020, -0.0093,  0.0125]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0227, -0.0257,  0.0167,  ..., -0.0118, -0.0202,  0.0177],\n",
      "        [-0.0179, -0.0171,  0.0119,  ..., -0.0076, -0.0041,  0.0134],\n",
      "        [-0.0206, -0.0116,  0.0175,  ..., -0.0020, -0.0093,  0.0125]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0439, -0.0200, -0.0223,  ...,  0.0114,  0.0445, -0.0566],\n",
      "        [-0.0055, -0.0275, -0.0182,  ...,  0.0095,  0.0294, -0.0096],\n",
      "        [ 0.0261, -0.0046, -0.0160,  ...,  0.0313,  0.0107, -0.0573]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0234, -0.0245,  0.0174,  ..., -0.0120, -0.0170,  0.0163],\n",
      "        [-0.0220, -0.0240,  0.0157,  ..., -0.0067, -0.0017,  0.0156],\n",
      "        [-0.0235, -0.0192,  0.0179,  ..., -0.0072, -0.0145,  0.0123]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0234, -0.0245,  0.0174,  ..., -0.0120, -0.0170,  0.0163],\n",
      "        [-0.0220, -0.0240,  0.0157,  ..., -0.0067, -0.0017,  0.0156],\n",
      "        [-0.0235, -0.0192,  0.0179,  ..., -0.0072, -0.0145,  0.0123]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0078, -0.0260, -0.0205,  ...,  0.0145,  0.0510, -0.0102],\n",
      "        [ 0.0290,  0.0047, -0.0227,  ...,  0.0162,  0.0292, -0.0161],\n",
      "        [ 0.0056, -0.0269, -0.0216,  ..., -0.0044,  0.0573, -0.0453]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 0 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0263, -0.0280,  0.0151,  ..., -0.0168, -0.0172,  0.0152],\n",
      "        [-0.0227, -0.0228,  0.0167,  ..., -0.0088, -0.0045,  0.0137],\n",
      "        [-0.0284, -0.0228,  0.0157,  ..., -0.0084, -0.0192,  0.0121]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0263, -0.0280,  0.0151,  ..., -0.0168, -0.0172,  0.0152],\n",
      "        [-0.0227, -0.0228,  0.0167,  ..., -0.0088, -0.0045,  0.0137],\n",
      "        [-0.0284, -0.0228,  0.0157,  ..., -0.0084, -0.0192,  0.0121]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[ 0.0071,  0.0052, -0.0002,  ..., -0.0000,  0.0089, -0.0032],\n",
      "         [ 0.0088,  0.0000, -0.0008,  ..., -0.0022,  0.0000,  0.0008],\n",
      "         [ 0.0075,  0.0055, -0.0006,  ..., -0.0018,  0.0069,  0.0007]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>), tensor([[[-0.0288, -0.0241,  0.0114,  ..., -0.0109, -0.0202,  0.0166],\n",
      "         [-0.0257, -0.0202,  0.0130,  ..., -0.0041, -0.0104,  0.0161],\n",
      "         [-0.0304, -0.0200,  0.0124,  ..., -0.0043, -0.0214,  0.0146]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tGet a forward layer and backward layer at layer 2\n",
      "\t\tCaching...: output_sequence to cache both forward and backward\n",
      "\t\tstate is None? True\n",
      "\t\t\tOops, then forward and backward state is also 'None'\n",
      "\t\tRUN forward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? True\n",
      "\t\t\t\tOh, then create full_batch_previous memory and state by (3, 4096) tensor filling 0.\n",
      "\t\t\t\tSet current_length_index... is it forward?? True\n",
      "\t\t\t\tOk, forward!! current_length_index = batch_size - 1 = 2\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "binary_mask = tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 0.0000, 1.1111,  ..., 1.1111, 0.0000, 0.0000],\n",
      "        [1.1111, 0.0000, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [0.0000, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 0 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0024,  0.0025,  0.0005,  ..., -0.0006,  0.0003, -0.0025],\n",
      "        [-0.0024,  0.0025,  0.0005,  ..., -0.0006,  0.0003, -0.0025],\n",
      "        [-0.0024,  0.0025,  0.0005,  ..., -0.0006,  0.0003, -0.0025]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 1.1606e-04, -2.0252e-04,  4.7553e-04,  ...,  7.6458e-05,\n",
      "         -1.1855e-04,  3.7646e-04],\n",
      "        [ 3.5457e-05, -3.6959e-04,  3.2306e-04,  ...,  7.4377e-05,\n",
      "         -9.8776e-05,  5.9375e-04],\n",
      "        [ 7.5841e-05, -4.8444e-04,  3.5971e-04,  ...,  1.4434e-04,\n",
      "          4.8971e-05,  4.9168e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 1.1606e-04, -2.0252e-04,  4.7553e-04,  ...,  7.6458e-05,\n",
      "         -1.1855e-04,  3.7646e-04],\n",
      "        [ 3.5457e-05, -3.6959e-04,  3.2306e-04,  ...,  7.4377e-05,\n",
      "         -9.8776e-05,  5.9375e-04],\n",
      "        [ 7.5841e-05, -4.8444e-04,  3.5971e-04,  ...,  1.4434e-04,\n",
      "          4.8971e-05,  4.9168e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0039,  0.0036, -0.0018,  ..., -0.0005, -0.0005, -0.0031],\n",
      "        [-0.0018,  0.0046,  0.0009,  ..., -0.0009,  0.0007, -0.0035],\n",
      "        [-0.0044,  0.0053,  0.0009,  ...,  0.0021, -0.0018, -0.0019]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 4.0095e-04, -4.9624e-04,  1.4743e-03,  ...,  6.0949e-04,\n",
      "         -6.4431e-04,  1.2427e-03],\n",
      "        [ 2.2609e-04, -9.0278e-04,  1.1435e-03,  ...,  3.9735e-04,\n",
      "         -1.7100e-04,  1.6491e-03],\n",
      "        [ 2.3643e-05, -1.4509e-03,  1.2192e-03,  ...,  6.4749e-04,\n",
      "          8.3528e-05,  1.4577e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 4.0095e-04, -4.9624e-04,  1.4743e-03,  ...,  6.0949e-04,\n",
      "         -6.4431e-04,  1.2427e-03],\n",
      "        [ 2.2609e-04, -9.0278e-04,  1.1435e-03,  ...,  3.9735e-04,\n",
      "         -1.7100e-04,  1.6491e-03],\n",
      "        [ 2.3643e-05, -1.4509e-03,  1.2192e-03,  ...,  6.4749e-04,\n",
      "          8.3528e-05,  1.4577e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0042,  0.0054, -0.0004,  ..., -0.0006, -0.0007, -0.0036],\n",
      "        [-0.0039,  0.0054,  0.0002,  ...,  0.0002,  0.0015, -0.0043],\n",
      "        [-0.0052,  0.0078,  0.0015,  ...,  0.0018, -0.0027, -0.0029]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0008, -0.0008,  0.0026,  ...,  0.0010, -0.0013,  0.0022],\n",
      "        [ 0.0002, -0.0013,  0.0022,  ...,  0.0008, -0.0004,  0.0031],\n",
      "        [-0.0001, -0.0023,  0.0023,  ...,  0.0014,  0.0002,  0.0024]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0008, -0.0008,  0.0026,  ...,  0.0010, -0.0013,  0.0022],\n",
      "        [ 0.0002, -0.0013,  0.0022,  ...,  0.0008, -0.0004,  0.0031],\n",
      "        [-0.0001, -0.0023,  0.0023,  ...,  0.0014,  0.0002,  0.0024]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0051,  0.0070,  0.0008,  ...,  0.0005, -0.0024, -0.0047],\n",
      "        [-0.0051,  0.0066,  0.0003,  ...,  0.0015,  0.0005, -0.0042],\n",
      "        [-0.0065,  0.0088,  0.0021,  ...,  0.0022, -0.0024, -0.0039]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 8.5672e-04, -1.2726e-03,  3.8097e-03,  ...,  1.1860e-03,\n",
      "         -1.8046e-03,  3.1271e-03],\n",
      "        [ 7.1048e-05, -1.7567e-03,  3.1405e-03,  ...,  1.1100e-03,\n",
      "         -6.6110e-04,  4.4871e-03],\n",
      "        [-2.9733e-04, -3.1962e-03,  3.3667e-03,  ...,  2.2283e-03,\n",
      "          6.5671e-05,  3.5723e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 8.5672e-04, -1.2726e-03,  3.8097e-03,  ...,  1.1860e-03,\n",
      "         -1.8046e-03,  3.1271e-03],\n",
      "        [ 7.1048e-05, -1.7567e-03,  3.1405e-03,  ...,  1.1100e-03,\n",
      "         -6.6110e-04,  4.4871e-03],\n",
      "        [-2.9733e-04, -3.1962e-03,  3.3667e-03,  ...,  2.2283e-03,\n",
      "          6.5671e-05,  3.5723e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0046,  0.0089, -0.0008,  ..., -0.0005, -0.0006, -0.0039],\n",
      "        [-0.0058,  0.0081,  0.0005,  ...,  0.0017,  0.0003, -0.0041],\n",
      "        [-0.0068,  0.0079,  0.0011,  ...,  0.0042, -0.0005, -0.0059]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0008, -0.0017,  0.0045,  ...,  0.0014, -0.0022,  0.0039],\n",
      "        [-0.0002, -0.0024,  0.0041,  ...,  0.0014, -0.0010,  0.0056],\n",
      "        [-0.0006, -0.0041,  0.0043,  ...,  0.0029, -0.0001,  0.0046]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0008, -0.0017,  0.0045,  ...,  0.0014, -0.0022,  0.0039],\n",
      "        [-0.0002, -0.0024,  0.0041,  ...,  0.0014, -0.0010,  0.0056],\n",
      "        [-0.0006, -0.0041,  0.0043,  ...,  0.0029, -0.0001,  0.0046]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0039,  0.0072, -0.0003,  ...,  0.0001,  0.0006, -0.0050],\n",
      "        [-0.0045,  0.0083, -0.0011,  ...,  0.0019,  0.0003, -0.0053],\n",
      "        [-0.0075,  0.0084,  0.0003,  ...,  0.0040,  0.0011, -0.0061]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0006, -0.0022,  0.0052,  ...,  0.0017, -0.0023,  0.0046],\n",
      "        [-0.0006, -0.0029,  0.0050,  ...,  0.0019, -0.0012,  0.0066]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0006, -0.0022,  0.0052,  ...,  0.0017, -0.0023,  0.0046],\n",
      "        [-0.0006, -0.0029,  0.0050,  ...,  0.0019, -0.0012,  0.0066]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-4.6915e-03,  7.2379e-03,  9.0794e-05,  ...,  1.2877e-03,\n",
      "          1.2330e-03, -4.6867e-03],\n",
      "        [-5.9120e-03,  8.5314e-03, -1.4924e-03,  ...,  2.2847e-03,\n",
      "          1.8585e-03, -5.6372e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0008, -0.0026,  0.0059,  ...,  0.0019, -0.0027,  0.0053]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0008, -0.0026,  0.0059,  ...,  0.0019, -0.0027,  0.0053]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-4.2139e-03,  9.2730e-03,  9.8432e-05, -1.5455e-03, -3.9501e-03,\n",
      "          0.0000e+00, -0.0000e+00, -5.8036e-03,  2.3859e-03, -8.4509e-03,\n",
      "         -9.5070e-03, -4.2622e-03, -1.2840e-03,  5.0319e-03,  0.0000e+00,\n",
      "         -1.6399e-02,  1.0882e-03, -0.0000e+00,  1.7699e-03,  5.4684e-03,\n",
      "          3.1589e-03,  1.0564e-02, -7.5921e-03, -3.8574e-03, -0.0000e+00,\n",
      "         -4.6926e-03,  7.0645e-03,  7.1241e-04, -4.0166e-03, -4.4318e-03,\n",
      "         -1.6974e-02, -3.0488e-03, -0.0000e+00, -2.1951e-03, -6.1112e-04,\n",
      "         -4.7832e-04,  1.2240e-02,  1.1406e-03,  2.9601e-03,  2.3854e-03,\n",
      "          2.8180e-03, -1.0247e-02, -1.0661e-03,  7.7246e-03, -2.0101e-03,\n",
      "          3.6751e-03, -6.2830e-03,  4.4259e-03, -1.2766e-03,  4.4688e-03,\n",
      "          1.6341e-04, -3.7877e-03,  0.0000e+00, -1.0847e-02,  1.0945e-02,\n",
      "          7.2205e-03,  1.6841e-03,  3.7869e-03,  1.2250e-02,  2.0206e-04,\n",
      "         -7.4292e-03,  5.2792e-03, -1.9214e-02,  0.0000e+00, -2.0930e-03,\n",
      "          8.6630e-03,  2.0954e-03,  1.3335e-03, -8.2439e-04, -4.9028e-03,\n",
      "          4.4089e-03,  1.4333e-02,  5.4308e-04,  8.9555e-03,  9.8485e-03,\n",
      "         -1.4580e-03,  4.6928e-03, -8.4161e-03,  1.0304e-03, -3.0302e-03,\n",
      "         -4.3311e-03,  9.7308e-03, -4.5649e-03, -1.1823e-02, -3.5070e-03,\n",
      "          1.5735e-03, -6.6684e-03,  2.2915e-03, -9.2902e-03,  0.0000e+00,\n",
      "          0.0000e+00, -7.4895e-03, -1.3458e-03,  7.3842e-03,  0.0000e+00,\n",
      "         -7.4802e-03, -9.1734e-04, -6.4046e-03, -9.7594e-03, -3.2509e-03,\n",
      "          7.3340e-03, -2.0788e-03, -1.1308e-03,  3.1904e-03, -0.0000e+00,\n",
      "          0.0000e+00,  1.9185e-03,  4.2461e-03,  2.3309e-03,  1.9464e-04,\n",
      "         -1.8321e-03, -2.8430e-03, -8.7461e-03,  3.4693e-03, -1.3414e-02,\n",
      "          6.3240e-04,  5.5925e-03, -2.2978e-03, -8.1306e-05, -4.9622e-03,\n",
      "         -3.2148e-03, -1.0336e-02, -1.0265e-03,  7.1317e-03,  6.4132e-03,\n",
      "          6.2537e-03, -1.5038e-04,  9.5372e-03,  2.6107e-03, -1.5533e-02,\n",
      "         -2.5710e-03, -7.6884e-04, -2.2255e-03, -6.8844e-03,  4.5908e-03,\n",
      "          3.3277e-03,  1.0391e-02,  3.8129e-04, -6.6748e-03, -0.0000e+00,\n",
      "          5.6212e-04, -7.4991e-03,  4.1274e-03, -1.1769e-02,  4.5995e-04,\n",
      "         -8.3619e-03, -4.8139e-03,  5.0358e-03, -2.2243e-03,  8.2355e-03,\n",
      "         -4.0164e-03, -1.2957e-03, -5.7674e-04,  5.1589e-03, -2.5991e-03,\n",
      "          8.0258e-03, -6.9367e-03,  3.7357e-03,  2.8787e-03,  2.1177e-03,\n",
      "         -5.4230e-04,  7.3980e-04, -2.9749e-03, -2.4123e-03, -2.2921e-03,\n",
      "         -1.7164e-03, -4.2599e-03,  3.7486e-03,  2.5106e-03, -1.1348e-02,\n",
      "          9.2228e-03,  2.2881e-05, -7.1275e-04, -5.9696e-04,  1.0046e-03,\n",
      "          9.5369e-03,  0.0000e+00,  3.2796e-03, -2.6790e-04, -6.6756e-03,\n",
      "         -4.9590e-03,  9.9031e-04, -1.1613e-02, -6.1325e-03, -5.3633e-03,\n",
      "          0.0000e+00,  1.2561e-03, -9.5027e-04, -5.3025e-04,  5.8833e-04,\n",
      "          1.3710e-03,  1.1182e-03,  1.6607e-03,  2.0198e-03,  8.7369e-03,\n",
      "          0.0000e+00,  1.2707e-02,  8.9507e-03, -0.0000e+00,  2.5721e-03,\n",
      "         -2.7292e-04, -1.2806e-03,  3.4589e-03,  2.0216e-03,  4.6700e-03,\n",
      "         -6.6769e-03, -0.0000e+00, -1.4086e-03,  1.2855e-03,  0.0000e+00,\n",
      "         -5.7698e-03,  3.1053e-03, -3.3426e-03,  1.5539e-03,  7.1622e-03,\n",
      "         -3.6037e-03,  1.2416e-02, -1.0187e-03, -9.1189e-03,  3.7004e-03,\n",
      "         -3.2230e-03,  5.7620e-03, -9.4394e-03, -3.6866e-03, -1.1361e-02,\n",
      "         -2.7507e-03,  5.6013e-03, -2.9847e-03,  7.3351e-03,  7.5384e-03,\n",
      "          4.3652e-03,  8.4027e-04,  3.1875e-03,  6.8758e-03, -3.8803e-04,\n",
      "          4.9418e-03, -7.7419e-03, -6.1303e-04, -1.3179e-02, -9.4443e-05,\n",
      "         -2.2679e-03, -1.7366e-03, -1.5843e-03, -1.4487e-03, -6.5650e-04,\n",
      "          1.5279e-03, -2.8305e-03, -4.4773e-03, -3.8248e-03,  2.7581e-03,\n",
      "          4.1648e-03, -3.1332e-03, -3.8307e-03, -0.0000e+00, -8.2666e-03,\n",
      "          2.4472e-03, -4.2861e-03, -0.0000e+00, -8.0986e-03, -8.8850e-03,\n",
      "         -6.7301e-05, -3.6916e-03, -5.4021e-03,  2.1056e-03, -6.7113e-03,\n",
      "         -1.1868e-02, -8.4448e-03, -9.0621e-03,  6.0899e-03, -3.9346e-03,\n",
      "          1.0633e-02, -1.5247e-03, -1.0844e-02, -1.4809e-02,  4.4817e-03,\n",
      "         -7.8971e-03, -8.4051e-03, -9.1100e-03,  1.3933e-02, -1.1991e-02,\n",
      "          3.9948e-03,  2.3458e-03,  3.0485e-03,  7.6860e-03, -1.2687e-02,\n",
      "         -9.3064e-03,  1.1898e-02, -2.3477e-05,  6.9009e-03, -7.5682e-03,\n",
      "         -2.9060e-04,  5.9557e-04,  1.0097e-02, -1.2971e-03, -3.4946e-03,\n",
      "         -9.0698e-03, -9.6887e-03, -2.0113e-03, -6.5265e-03, -5.2431e-03,\n",
      "          6.0313e-03, -2.7501e-04, -1.0464e-02, -1.5441e-03, -6.9713e-03,\n",
      "          2.7596e-03, -8.7867e-03, -9.1895e-03, -2.0963e-03,  7.3373e-03,\n",
      "          4.4674e-03, -3.8785e-03,  9.6032e-03,  2.0128e-03,  0.0000e+00,\n",
      "          6.5458e-04, -2.1916e-03,  5.6830e-03, -2.2666e-03,  3.8178e-03,\n",
      "          7.8318e-03, -4.3617e-03,  4.9484e-03, -3.5390e-03, -3.2663e-03,\n",
      "          8.9027e-03,  0.0000e+00,  3.3851e-03,  2.6477e-03, -0.0000e+00,\n",
      "          3.0262e-04,  8.1156e-03, -7.4584e-03,  9.6610e-04,  2.9094e-03,\n",
      "         -2.7200e-05,  1.0638e-02,  4.3943e-03,  1.1523e-02,  3.4248e-03,\n",
      "         -1.6919e-03, -1.5515e-02,  1.1866e-02, -5.2314e-04, -7.3575e-03,\n",
      "          3.5979e-03, -2.6515e-03, -4.5824e-03, -4.3132e-04,  1.0709e-02,\n",
      "          4.4722e-03, -6.0172e-03, -0.0000e+00,  0.0000e+00, -5.4360e-04,\n",
      "          2.4688e-03,  5.5699e-03,  2.2314e-03,  1.9064e-04, -7.0878e-03,\n",
      "          6.3473e-03, -1.3725e-03, -0.0000e+00, -1.1827e-02,  1.3658e-02,\n",
      "         -6.4867e-03,  4.8845e-03, -1.0854e-02,  5.4632e-03,  6.1090e-03,\n",
      "          7.8540e-03,  2.1731e-03, -9.1919e-03,  6.2648e-03, -6.6859e-03,\n",
      "          7.7118e-03,  5.5739e-03, -9.2641e-03, -8.2321e-03, -1.3478e-02,\n",
      "          1.2947e-02,  7.4369e-03,  5.5639e-03,  1.4775e-02,  1.2428e-02,\n",
      "          5.5733e-03,  4.1493e-03, -1.0468e-03, -2.6231e-03, -3.1361e-04,\n",
      "         -4.8292e-03, -0.0000e+00, -5.3009e-03,  2.6413e-03, -0.0000e+00,\n",
      "          9.4032e-03,  1.1040e-02, -8.4986e-03, -1.9936e-03,  1.1252e-03,\n",
      "         -2.1356e-03,  7.1927e-03,  3.8848e-03,  9.7002e-03, -1.8958e-03,\n",
      "         -6.5822e-03,  1.4005e-03, -3.7758e-03, -0.0000e+00,  2.3171e-03,\n",
      "         -0.0000e+00, -8.3479e-04, -8.8490e-04,  4.0237e-03, -2.6776e-03,\n",
      "         -5.0295e-03, -1.4095e-02,  6.7131e-03, -9.8015e-04, -1.3787e-03,\n",
      "         -2.3277e-03,  5.7197e-03, -1.0737e-02, -1.8761e-03, -7.7964e-03,\n",
      "         -2.1527e-03,  6.7593e-03,  0.0000e+00, -4.2131e-04,  6.5604e-03,\n",
      "          7.1584e-03,  0.0000e+00, -6.9126e-03, -1.1686e-02,  8.4374e-03,\n",
      "         -3.0036e-03, -0.0000e+00, -9.1677e-04, -1.6850e-02,  0.0000e+00,\n",
      "          2.4917e-03, -2.8225e-03,  8.2058e-03, -7.2859e-03, -2.3318e-03,\n",
      "          1.2066e-02,  9.0632e-05,  6.7429e-03, -0.0000e+00, -2.7447e-03,\n",
      "          7.6142e-03,  3.2614e-05, -8.5049e-03,  6.2158e-03,  4.5658e-03,\n",
      "         -8.9574e-03,  2.6915e-03,  1.0011e-03,  3.1864e-03, -2.2350e-03,\n",
      "         -2.3134e-03,  3.7110e-03,  2.2699e-03, -1.4136e-02, -1.7530e-02,\n",
      "         -1.8214e-03, -2.3412e-03, -6.5240e-04, -1.0112e-04, -1.0312e-02,\n",
      "          4.5476e-03, -8.9071e-03, -5.1146e-03,  2.1943e-04,  1.0493e-02,\n",
      "         -4.9618e-03,  4.6253e-03,  1.3320e-03,  4.1783e-03, -1.1096e-02,\n",
      "         -0.0000e+00, -1.6098e-02,  4.5748e-03,  3.6311e-03,  5.1027e-03,\n",
      "         -9.4555e-03, -8.3411e-03,  0.0000e+00, -0.0000e+00, -1.0235e-02,\n",
      "         -1.1568e-03,  9.1545e-03, -7.4746e-03, -7.9955e-03, -1.3146e-03,\n",
      "         -2.7873e-03, -1.0241e-02,  7.8926e-03, -9.1885e-03, -6.3554e-03,\n",
      "         -1.3482e-02,  1.0028e-02,  1.2998e-03, -3.6677e-03,  4.0196e-03,\n",
      "          7.3752e-03,  3.9976e-03,  2.3163e-03,  6.1503e-03,  3.0017e-04,\n",
      "          1.0216e-03, -3.1713e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0009, -0.0031,  0.0064,  ...,  0.0022, -0.0030,  0.0059]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0009, -0.0031,  0.0064,  ...,  0.0022, -0.0030,  0.0059]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-4.2502e-03,  9.9842e-03,  5.4106e-04, -8.3984e-04, -1.3035e-03,\n",
      "          0.0000e+00, -0.0000e+00, -7.9750e-03,  4.0440e-03, -9.9807e-03,\n",
      "         -9.4619e-03, -5.3118e-03, -1.2384e-03,  4.7148e-03,  0.0000e+00,\n",
      "         -1.7052e-02,  3.2067e-03, -0.0000e+00,  2.3160e-03,  5.9921e-03,\n",
      "          2.2765e-03,  1.2062e-02, -8.0673e-03, -4.6984e-03, -0.0000e+00,\n",
      "         -6.6626e-03,  9.7278e-03,  3.8566e-05, -4.5096e-03, -5.4227e-03,\n",
      "         -1.7958e-02, -3.5140e-03, -0.0000e+00, -1.4452e-03, -1.6022e-03,\n",
      "         -4.9544e-04,  1.2198e-02,  2.9027e-03,  4.2980e-03,  2.7656e-03,\n",
      "          2.2389e-03, -1.2306e-02,  6.9232e-05,  6.0707e-03, -2.3254e-03,\n",
      "          3.2074e-03, -8.0729e-03,  4.8113e-03, -1.9994e-03,  4.9139e-03,\n",
      "          1.6755e-03, -5.0434e-03,  0.0000e+00, -1.1291e-02,  9.2820e-03,\n",
      "          9.2279e-03,  9.3848e-04,  3.6407e-03,  1.1734e-02,  6.9158e-05,\n",
      "         -7.0221e-03,  7.3034e-03, -2.2658e-02,  0.0000e+00, -4.9518e-03,\n",
      "          9.0469e-03,  2.4635e-03,  1.3767e-03, -2.2006e-03, -6.8969e-03,\n",
      "          4.7360e-03,  1.6702e-02,  4.5499e-04,  8.2167e-03,  1.1208e-02,\n",
      "         -3.1555e-03,  4.1033e-03, -7.3238e-03, -2.0768e-03, -1.3729e-03,\n",
      "         -4.3427e-03,  1.0093e-02, -4.6954e-03, -1.1779e-02, -4.1503e-03,\n",
      "          2.0867e-03, -6.9891e-03,  2.3735e-03, -1.0034e-02,  0.0000e+00,\n",
      "          0.0000e+00, -6.2110e-03, -8.2379e-04,  5.8239e-03,  0.0000e+00,\n",
      "         -6.2448e-03, -5.9921e-04, -7.0649e-03, -1.0500e-02, -3.5069e-03,\n",
      "          8.9476e-03, -2.1543e-03, -2.8318e-03,  5.5171e-03, -0.0000e+00,\n",
      "          0.0000e+00,  4.6745e-04,  4.6010e-03,  4.5516e-03,  6.2785e-04,\n",
      "         -1.9399e-03, -3.3249e-03, -8.5972e-03,  5.1801e-03, -1.5090e-02,\n",
      "          2.7972e-03,  6.6753e-03, -1.5658e-03, -2.0333e-04, -4.9385e-03,\n",
      "         -4.1397e-03, -1.2288e-02, -1.2376e-03,  6.9376e-03,  5.7870e-03,\n",
      "          4.5483e-03,  2.1825e-05,  9.9418e-03,  2.5082e-03, -1.5724e-02,\n",
      "         -1.4762e-03,  6.1383e-04, -2.3288e-03, -6.5165e-03,  3.6507e-03,\n",
      "          3.1042e-03,  1.1430e-02,  2.4794e-04, -6.2276e-03, -0.0000e+00,\n",
      "          7.4698e-04, -7.4009e-03,  6.2822e-03, -9.1649e-03,  1.3660e-03,\n",
      "         -7.9672e-03, -6.1484e-03,  4.7020e-03, -3.5865e-03,  9.9768e-03,\n",
      "         -5.6335e-03, -2.4943e-03, -3.7818e-04,  3.5863e-03, -1.4604e-03,\n",
      "          9.2678e-03, -6.5539e-03,  4.9838e-03,  4.0586e-03,  3.6369e-03,\n",
      "          1.0283e-03,  1.5260e-03, -2.4935e-03, -2.1792e-03, -4.2494e-03,\n",
      "         -1.9020e-03, -3.9646e-03,  3.6392e-03,  2.4721e-03, -1.3981e-02,\n",
      "          1.0180e-02,  2.1321e-03, -3.1712e-04, -1.8672e-03,  7.7007e-04,\n",
      "          1.1928e-02,  0.0000e+00,  4.3114e-03, -7.4192e-04, -8.5683e-03,\n",
      "         -7.4411e-03,  1.2712e-03, -1.1771e-02, -8.4103e-03, -6.0974e-03,\n",
      "          0.0000e+00,  1.2990e-03, -7.3766e-04,  8.0158e-04,  1.0057e-03,\n",
      "          1.3138e-03,  7.9096e-04,  6.1894e-04,  1.0530e-03,  7.4750e-03,\n",
      "         -0.0000e+00,  1.1831e-02,  9.6789e-03, -0.0000e+00,  4.0801e-03,\n",
      "          1.7387e-04, -2.1916e-03,  3.9189e-04,  2.1928e-03,  4.9954e-03,\n",
      "         -8.3734e-03, -0.0000e+00, -4.7599e-04,  9.4152e-04,  0.0000e+00,\n",
      "         -7.5226e-03,  2.1364e-03, -2.7951e-03,  1.4498e-03,  6.7980e-03,\n",
      "         -3.4037e-03,  1.3166e-02, -1.9338e-03, -9.2096e-03,  4.0371e-03,\n",
      "         -5.5608e-03,  6.3841e-03, -1.0641e-02, -5.0697e-03, -1.2625e-02,\n",
      "         -3.1777e-03,  5.0190e-03, -5.1412e-03,  7.8584e-03,  8.4310e-03,\n",
      "          4.7760e-03,  4.9975e-04,  6.0680e-03,  6.7590e-03,  1.1322e-03,\n",
      "          5.5061e-03, -7.9154e-03, -5.5469e-04, -1.5743e-02,  1.3309e-03,\n",
      "         -1.0666e-03, -1.0555e-04, -1.7962e-03, -1.4752e-03,  1.4023e-04,\n",
      "          2.7611e-03, -3.0082e-03, -5.6294e-03, -5.5100e-03,  1.6032e-03,\n",
      "          5.1351e-03, -4.0666e-03, -2.9976e-03, -0.0000e+00, -6.3988e-03,\n",
      "          1.8859e-03, -3.8003e-03, -0.0000e+00, -1.0282e-02, -9.7620e-03,\n",
      "          1.6841e-03, -4.2728e-03, -4.1888e-03,  3.1076e-03, -8.5772e-03,\n",
      "         -1.2017e-02, -8.4050e-03, -1.0441e-02,  6.6445e-03, -5.2043e-03,\n",
      "          1.2655e-02,  1.7431e-04, -1.1265e-02, -1.4112e-02,  3.3082e-03,\n",
      "         -1.0933e-02, -7.9150e-03, -9.9164e-03,  1.6380e-02, -1.0915e-02,\n",
      "          3.1042e-03,  2.3505e-03,  2.5541e-03,  1.0610e-02, -1.1863e-02,\n",
      "         -9.1292e-03,  1.3028e-02,  1.6561e-03,  8.0568e-03, -1.0213e-02,\n",
      "          5.8645e-04, -1.4421e-03,  1.0268e-02, -2.0129e-03, -2.5725e-03,\n",
      "         -9.6662e-03, -1.1072e-02, -2.6820e-03, -7.4870e-03, -5.0980e-03,\n",
      "          7.4834e-03,  5.7383e-04, -1.0163e-02, -1.3925e-03, -5.6152e-03,\n",
      "          1.9802e-03, -9.1991e-03, -8.0849e-03, -1.2512e-03,  7.4444e-03,\n",
      "          6.9580e-03, -4.0859e-03,  1.0728e-02,  3.2829e-03, -0.0000e+00,\n",
      "          1.8869e-03, -3.3187e-03,  7.4414e-03, -2.5698e-03,  4.7634e-03,\n",
      "          1.0435e-02, -3.8198e-03,  4.5314e-03, -4.6773e-03, -4.5821e-03,\n",
      "          8.5209e-03,  0.0000e+00,  4.0802e-03,  1.5469e-03, -0.0000e+00,\n",
      "         -1.9257e-04,  8.7051e-03, -8.3828e-03,  1.4743e-03,  3.7848e-03,\n",
      "         -4.7942e-04,  9.9861e-03,  4.3036e-03,  1.2718e-02,  4.8404e-03,\n",
      "         -2.6514e-03, -1.5196e-02,  1.1612e-02, -2.6741e-03, -8.3696e-03,\n",
      "          3.1700e-03, -2.2273e-03, -4.2553e-03,  3.5862e-04,  1.0527e-02,\n",
      "          7.7428e-03, -5.7944e-03, -0.0000e+00,  0.0000e+00, -1.4640e-04,\n",
      "          1.8779e-03,  6.6011e-03,  2.3393e-03, -9.5275e-04, -8.3212e-03,\n",
      "          8.7644e-03, -1.8022e-03, -0.0000e+00, -1.0141e-02,  1.6003e-02,\n",
      "         -5.7617e-03,  4.9544e-03, -1.0327e-02,  7.1493e-03,  6.5292e-03,\n",
      "          7.3021e-03,  2.0178e-03, -1.0826e-02,  5.9150e-03, -6.9888e-03,\n",
      "          7.8747e-03,  6.7112e-03, -1.1735e-02, -7.1489e-03, -1.3776e-02,\n",
      "          1.7337e-02,  7.2386e-03,  8.2841e-03,  1.6194e-02,  1.2009e-02,\n",
      "          5.3144e-03,  4.2873e-03, -1.7048e-03, -1.8961e-03, -2.6258e-03,\n",
      "         -5.3675e-03, -0.0000e+00, -6.4305e-03,  3.4807e-03, -0.0000e+00,\n",
      "          1.0248e-02,  1.1382e-02, -6.9560e-03, -3.7739e-03,  1.2268e-03,\n",
      "         -1.7962e-03,  8.3318e-03,  4.2970e-03,  1.0275e-02, -1.9345e-03,\n",
      "         -9.0415e-03,  1.8785e-03, -4.6938e-03, -0.0000e+00,  3.9890e-03,\n",
      "         -0.0000e+00, -7.4551e-04, -1.7375e-03,  5.3828e-03, -3.1090e-03,\n",
      "         -4.7895e-03, -1.6624e-02,  6.8890e-03, -1.4037e-03, -8.0347e-04,\n",
      "         -3.7452e-03,  6.2926e-03, -1.1380e-02, -2.0226e-03, -9.3648e-03,\n",
      "         -2.7932e-03,  8.5714e-03,  0.0000e+00, -1.5664e-03,  8.6588e-03,\n",
      "          8.7693e-03,  0.0000e+00, -6.7492e-03, -1.3148e-02,  8.4814e-03,\n",
      "         -3.7272e-03, -0.0000e+00, -8.5984e-04, -1.8526e-02,  0.0000e+00,\n",
      "          3.7469e-03, -3.6390e-03,  8.7418e-03, -6.9529e-03, -2.9557e-03,\n",
      "          1.2674e-02,  1.3224e-03,  6.8414e-03, -0.0000e+00,  5.7196e-04,\n",
      "          8.8635e-03, -1.1768e-03, -1.0765e-02,  5.6501e-03,  5.0053e-03,\n",
      "         -8.4224e-03,  2.4528e-03,  1.3623e-03,  2.2443e-03, -2.5712e-03,\n",
      "         -1.2968e-03,  4.1128e-03,  2.7977e-03, -1.3782e-02, -1.7651e-02,\n",
      "         -8.6426e-04, -3.4884e-03, -1.2814e-03, -5.1138e-04, -1.1179e-02,\n",
      "          5.9648e-03, -9.3009e-03, -7.1664e-03, -7.8930e-04,  1.1461e-02,\n",
      "         -6.2403e-03,  5.0344e-03, -3.6276e-05,  5.5484e-03, -1.2646e-02,\n",
      "         -0.0000e+00, -1.6025e-02,  4.9251e-03,  4.7974e-03,  6.5686e-03,\n",
      "         -9.3499e-03, -1.0989e-02,  0.0000e+00, -0.0000e+00, -9.9647e-03,\n",
      "         -2.6002e-03,  1.0666e-02, -7.1618e-03, -7.4625e-03, -1.4016e-03,\n",
      "         -1.4907e-03, -1.0864e-02,  8.8828e-03, -1.1249e-02, -6.2201e-03,\n",
      "         -1.4143e-02,  1.1698e-02,  2.7078e-03, -2.5345e-03,  5.0621e-03,\n",
      "          7.6748e-03,  2.8923e-03,  3.3207e-03,  7.0879e-03,  2.2046e-04,\n",
      "         -8.5880e-04, -4.2213e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 9 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0009, -0.0034,  0.0070,  ...,  0.0025, -0.0031,  0.0066]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0009, -0.0034,  0.0070,  ...,  0.0025, -0.0031,  0.0066]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-5.6458e-03,  9.7251e-03, -2.3428e-04, -2.2329e-03, -3.3895e-03,\n",
      "          0.0000e+00, -0.0000e+00, -7.1839e-03,  3.2381e-03, -1.0012e-02,\n",
      "         -1.0960e-02, -4.2985e-03, -6.4803e-04,  5.4481e-03,  0.0000e+00,\n",
      "         -1.5465e-02,  1.1320e-03, -0.0000e+00,  3.1430e-03,  5.9135e-03,\n",
      "          2.0645e-03,  1.1712e-02, -9.1072e-03, -4.0242e-03, -0.0000e+00,\n",
      "         -5.0544e-03,  9.2075e-03,  1.9128e-03, -4.6949e-03, -8.2777e-03,\n",
      "         -1.6117e-02, -2.7808e-03, -0.0000e+00, -1.2979e-03, -1.8538e-03,\n",
      "         -2.6215e-04,  1.2255e-02,  3.3498e-03,  3.3542e-03,  1.8667e-03,\n",
      "          2.0263e-03, -1.1440e-02, -5.6486e-04,  4.6258e-03, -3.2058e-03,\n",
      "          4.2878e-03, -6.7392e-03,  3.3064e-03, -3.1596e-03,  4.5971e-03,\n",
      "          1.1495e-03, -4.5743e-03, -0.0000e+00, -1.1796e-02,  1.0986e-02,\n",
      "          9.4343e-03,  2.6731e-03,  1.9913e-03,  1.3016e-02, -1.2750e-03,\n",
      "         -7.8453e-03,  6.9377e-03, -2.2406e-02,  0.0000e+00, -4.2143e-03,\n",
      "          8.2852e-03,  2.2290e-03,  2.5042e-03, -2.4580e-03, -8.0188e-03,\n",
      "          3.0206e-03,  1.6194e-02, -2.4821e-04,  9.4785e-03,  1.0887e-02,\n",
      "         -1.5229e-03,  3.9486e-03, -7.8548e-03, -1.5792e-03, -1.4596e-03,\n",
      "         -4.1295e-03,  1.0914e-02, -4.5077e-03, -1.2260e-02, -4.1827e-03,\n",
      "          1.1052e-03, -7.2170e-03,  2.4336e-03, -1.0130e-02,  0.0000e+00,\n",
      "          0.0000e+00, -6.2565e-03, -7.2510e-04,  5.6130e-03, -0.0000e+00,\n",
      "         -7.3516e-03, -1.7708e-03, -6.0505e-03, -1.2338e-02, -3.3142e-03,\n",
      "          8.6359e-03, -2.1284e-03, -3.1582e-03,  4.8211e-03, -0.0000e+00,\n",
      "          0.0000e+00,  1.3870e-04,  3.4250e-03,  3.9527e-03, -1.1843e-03,\n",
      "         -1.0122e-03, -1.5577e-03, -9.1942e-03,  4.2271e-03, -1.4884e-02,\n",
      "          2.9342e-03,  7.0869e-03,  1.4259e-04,  1.5041e-03, -5.7076e-03,\n",
      "         -3.7524e-03, -1.2361e-02, -1.2592e-03,  7.3963e-03,  5.8280e-03,\n",
      "          4.3458e-03,  2.3753e-03,  1.0494e-02,  2.1961e-04, -1.6296e-02,\n",
      "         -1.6563e-03,  9.4373e-04, -2.7494e-03, -5.9169e-03,  4.5599e-03,\n",
      "          3.1420e-03,  1.0250e-02, -2.2342e-04, -5.0214e-03, -0.0000e+00,\n",
      "          6.9828e-04, -8.2258e-03,  4.6511e-03, -8.7086e-03,  9.4952e-04,\n",
      "         -6.8145e-03, -4.5858e-03,  6.0558e-03, -3.9076e-03,  8.6439e-03,\n",
      "         -4.1626e-03, -2.9043e-03,  1.5520e-03,  2.9250e-03, -1.9916e-04,\n",
      "          9.7782e-03, -6.6764e-03,  4.7446e-03,  5.4544e-03,  3.0334e-03,\n",
      "          1.0173e-03,  2.3515e-03, -1.0628e-03, -9.2429e-04, -1.7929e-03,\n",
      "         -3.0268e-03, -4.6667e-03,  2.6148e-03,  3.7405e-03, -1.5891e-02,\n",
      "          1.0830e-02,  1.5156e-03,  5.0539e-04, -1.7132e-03,  1.7501e-03,\n",
      "          1.2697e-02,  0.0000e+00,  2.6831e-03,  2.0799e-04, -8.2296e-03,\n",
      "         -6.0035e-03,  8.8245e-04, -1.0344e-02, -7.8801e-03, -6.2087e-03,\n",
      "          0.0000e+00,  2.0723e-03, -1.1079e-03, -1.4989e-03,  8.2360e-04,\n",
      "          2.1277e-03,  7.0031e-04,  4.6094e-04,  7.1711e-04,  7.3138e-03,\n",
      "         -0.0000e+00,  1.1445e-02,  8.4918e-03, -0.0000e+00,  5.3654e-03,\n",
      "         -1.1110e-03, -3.8470e-03,  3.0464e-03,  1.8841e-03,  5.5531e-03,\n",
      "         -7.1560e-03, -0.0000e+00, -8.3177e-04,  1.7984e-03,  0.0000e+00,\n",
      "         -8.2820e-03,  3.2096e-03, -4.3453e-03,  1.0187e-03,  6.5171e-03,\n",
      "         -3.0916e-03,  1.2934e-02, -2.0131e-03, -1.0182e-02,  4.7943e-03,\n",
      "         -4.1780e-03,  4.3060e-03, -1.0530e-02, -4.9940e-03, -1.2549e-02,\n",
      "         -3.3393e-03,  5.4653e-03, -3.4152e-03,  7.3045e-03,  8.9187e-03,\n",
      "          3.2682e-03,  9.1652e-04,  5.2812e-03,  5.8783e-03,  2.2090e-05,\n",
      "          5.3259e-03, -6.8135e-03, -1.2157e-03, -1.6102e-02,  1.2611e-03,\n",
      "         -2.2066e-03,  1.2717e-03, -3.9233e-04, -3.1136e-03,  4.4463e-04,\n",
      "          2.7144e-03, -2.9890e-03, -4.7351e-03, -5.6761e-03,  3.1865e-03,\n",
      "          2.6653e-03, -2.1984e-03, -2.8970e-03, -0.0000e+00, -6.6059e-03,\n",
      "          1.8610e-03, -4.1552e-03, -0.0000e+00, -8.2418e-03, -1.1044e-02,\n",
      "          1.5867e-03, -3.9876e-03, -5.7825e-03,  3.8089e-03, -8.5412e-03,\n",
      "         -1.0627e-02, -7.6386e-03, -9.8071e-03,  5.6758e-03, -4.2085e-03,\n",
      "          1.0167e-02, -3.0083e-04, -1.0253e-02, -1.5964e-02,  2.8969e-03,\n",
      "         -8.6666e-03, -6.2376e-03, -1.1823e-02,  1.5180e-02, -1.0332e-02,\n",
      "          4.3979e-03,  2.5450e-03,  2.2575e-03,  1.0603e-02, -1.2929e-02,\n",
      "         -9.2177e-03,  1.3882e-02,  1.4476e-03,  7.4561e-03, -9.2931e-03,\n",
      "          6.5882e-04, -9.7515e-04,  9.2797e-03, -1.1860e-03, -2.7826e-03,\n",
      "         -9.1987e-03, -1.0753e-02, -1.0134e-03, -6.8600e-03, -4.7707e-03,\n",
      "          5.8593e-03,  1.4146e-03, -1.1864e-02, -3.4598e-04, -4.5857e-03,\n",
      "          3.5906e-03, -8.8687e-03, -7.0087e-03, -1.6638e-03,  6.9336e-03,\n",
      "          4.6662e-03, -4.0196e-03,  1.0399e-02,  4.3632e-03,  0.0000e+00,\n",
      "          1.1544e-03, -2.7448e-03,  8.1843e-03, -3.1613e-03,  5.9357e-03,\n",
      "          9.2952e-03, -5.2749e-03,  5.4273e-03, -3.2086e-03, -5.7381e-03,\n",
      "          6.8458e-03,  0.0000e+00,  3.2177e-03,  1.3910e-03, -0.0000e+00,\n",
      "         -9.8990e-05,  8.9165e-03, -7.2981e-03,  1.3191e-03,  4.0041e-03,\n",
      "          1.1545e-04,  9.4775e-03,  3.9975e-03,  1.3218e-02,  5.9834e-03,\n",
      "         -2.2929e-03, -1.5424e-02,  1.3801e-02, -2.3072e-03, -7.9436e-03,\n",
      "          2.6743e-03, -2.8933e-03, -3.6889e-03, -6.7652e-04,  9.8359e-03,\n",
      "          6.6551e-03, -5.2945e-03, -0.0000e+00,  0.0000e+00, -5.1642e-04,\n",
      "          1.6634e-03,  6.7472e-03,  2.5120e-03,  1.3574e-03, -7.8609e-03,\n",
      "          8.6196e-03, -1.7765e-03, -0.0000e+00, -1.0140e-02,  1.4872e-02,\n",
      "         -6.0462e-03,  5.7378e-03, -9.6922e-03,  5.5585e-03,  5.9265e-03,\n",
      "          6.8797e-03,  1.9918e-03, -1.0494e-02,  7.4983e-03, -7.6262e-03,\n",
      "          7.7142e-03,  8.1408e-03, -1.1302e-02, -6.4241e-03, -1.3134e-02,\n",
      "          1.7000e-02,  6.8619e-03,  6.7178e-03,  1.6602e-02,  1.3571e-02,\n",
      "          3.9747e-03,  4.1944e-03, -2.0358e-03, -2.3040e-03, -3.5266e-03,\n",
      "         -4.5629e-03,  0.0000e+00, -5.2585e-03,  3.5037e-03, -0.0000e+00,\n",
      "          1.1227e-02,  1.1805e-02, -5.9125e-03, -2.8719e-03,  1.1209e-03,\n",
      "         -3.1250e-03,  7.6049e-03,  4.2635e-03,  1.0865e-02, -2.2030e-03,\n",
      "         -8.6837e-03,  9.8490e-04, -3.6273e-03, -0.0000e+00,  4.1553e-03,\n",
      "         -0.0000e+00, -3.5189e-03, -1.8272e-03,  5.6569e-03, -2.9847e-03,\n",
      "         -7.2240e-03, -1.5674e-02,  5.2798e-03, -1.3647e-03, -7.4635e-04,\n",
      "         -2.6291e-03,  6.6450e-03, -1.2818e-02, -1.7985e-03, -1.0670e-02,\n",
      "         -9.9513e-04,  8.2470e-03,  0.0000e+00, -2.3875e-03,  9.2086e-03,\n",
      "          7.0461e-03,  0.0000e+00, -5.5204e-03, -1.3110e-02,  7.9332e-03,\n",
      "         -4.4707e-03,  0.0000e+00, -9.3208e-04, -1.8809e-02,  0.0000e+00,\n",
      "          3.7542e-03, -4.8687e-03,  7.5399e-03, -6.7691e-03, -1.6200e-03,\n",
      "          1.1765e-02, -4.2142e-04,  7.9995e-03, -0.0000e+00, -1.1398e-03,\n",
      "          7.6677e-03,  9.4796e-05, -1.0689e-02,  5.8045e-03,  4.7138e-03,\n",
      "         -7.9786e-03,  2.6807e-03,  5.2183e-04,  2.2940e-03, -2.9778e-03,\n",
      "         -4.0140e-03,  6.3977e-03,  3.3769e-03, -1.3548e-02, -1.7113e-02,\n",
      "         -4.1955e-04, -3.3533e-03, -6.6079e-04,  1.7652e-03, -1.1468e-02,\n",
      "          4.5211e-03, -8.1211e-03, -3.6119e-03,  5.5217e-04,  1.0554e-02,\n",
      "         -6.5533e-03,  4.8281e-03, -8.0553e-04,  4.4657e-03, -1.3366e-02,\n",
      "         -0.0000e+00, -1.4373e-02,  5.5990e-03,  4.0367e-03,  5.4516e-03,\n",
      "         -9.6369e-03, -1.0604e-02,  0.0000e+00, -0.0000e+00, -9.3401e-03,\n",
      "         -2.0638e-03,  1.0819e-02, -6.1623e-03, -7.5337e-03, -1.2898e-03,\n",
      "         -1.1657e-03, -9.2493e-03,  8.0842e-03, -1.0370e-02, -5.4779e-03,\n",
      "         -1.3791e-02,  9.8927e-03,  1.9014e-03, -2.0884e-03,  6.1091e-03,\n",
      "          7.6394e-03,  3.4402e-03,  2.7866e-03,  7.2887e-03,  9.8300e-04,\n",
      "          1.1546e-03, -5.0667e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[ 0.0024,  0.0000,  0.0021,  ..., -0.0012,  0.0000,  0.0000],\n",
      "         [ 0.0024,  0.0000,  0.0011,  ..., -0.0010,  0.0005,  0.0013],\n",
      "         [ 0.0000,  0.0032,  0.0010,  ..., -0.0010, -0.0001,  0.0008]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>), tensor([[[ 0.0009, -0.0036,  0.0076,  ...,  0.0025, -0.0031,  0.0070],\n",
      "         [-0.0008, -0.0030,  0.0058,  ...,  0.0020, -0.0014,  0.0074],\n",
      "         [-0.0007, -0.0047,  0.0050,  ...,  0.0032, -0.0003,  0.0054]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tRUN backward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? True\n",
      "\t\t\t\tOh, then create full_batch_previous memory and state by (3, 4096) tensor filling 0.\n",
      "\t\t\t\tSet current_length_index... is it forward?? False\n",
      "\t\t\t\tOops, backward!! current_length_index = 0\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "binary_mask = tensor([[ True, False,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [False,  True,  True,  ...,  True,  True,  True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 0.0000],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 0.0000, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 9 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8089e-03,  1.4019e-03, -8.0173e-04,  2.7266e-03,  1.5510e-03,\n",
      "          1.0097e-03,  1.2684e-03, -1.2003e-03,  6.1846e-04, -2.6253e-03,\n",
      "         -2.4565e-03,  0.0000e+00,  0.0000e+00, -3.2279e-03, -5.3840e-04,\n",
      "         -1.9412e-03,  2.6801e-03, -4.8148e-04, -4.7152e-04,  2.0605e-03,\n",
      "          7.7011e-04,  1.0532e-04,  3.3687e-04, -7.5475e-04, -1.7253e-03,\n",
      "          2.1085e-03, -1.9696e-04,  9.1037e-04,  1.5788e-03,  7.7868e-04,\n",
      "         -2.8094e-04, -1.7669e-03,  1.6910e-03, -2.4980e-03,  3.0779e-03,\n",
      "         -2.5934e-03,  3.4986e-03,  1.3299e-04,  5.8401e-04, -4.2464e-04,\n",
      "          1.5119e-03,  2.6603e-03, -2.3133e-03,  3.1538e-03, -2.3455e-03,\n",
      "          2.1538e-03, -2.4360e-03,  2.4764e-04, -1.4955e-03,  1.1835e-03,\n",
      "          2.0131e-04, -3.1189e-03, -1.7889e-03, -2.1976e-03,  2.4341e-03,\n",
      "          3.7992e-04, -3.6490e-03, -0.0000e+00,  2.4893e-06,  0.0000e+00,\n",
      "          3.2313e-03,  0.0000e+00,  4.1508e-04,  1.4951e-03, -3.6981e-04,\n",
      "          3.5716e-04,  8.7022e-04,  2.7436e-03, -1.2999e-03, -7.0038e-04,\n",
      "          2.9265e-03,  1.1821e-03, -2.4228e-03,  1.5308e-03, -6.9321e-04,\n",
      "         -5.2278e-04,  2.5720e-03,  3.6110e-04,  0.0000e+00, -4.7300e-04,\n",
      "         -5.9509e-04, -1.7536e-03,  1.2184e-03, -1.9169e-03, -4.6583e-04,\n",
      "         -1.8606e-04,  3.8775e-03, -9.1938e-05, -1.6085e-03, -2.2571e-03,\n",
      "         -2.4431e-03, -2.7019e-03,  2.7328e-03,  9.2501e-04, -2.6396e-03,\n",
      "         -1.7846e-03,  1.7082e-03, -8.7228e-04,  1.6185e-03,  2.4206e-03,\n",
      "         -1.1063e-03,  1.6250e-03,  5.6982e-04, -5.9228e-04, -9.9176e-04,\n",
      "          2.8044e-03,  1.5525e-03,  2.1959e-03,  3.1293e-03, -1.2159e-03,\n",
      "         -0.0000e+00,  1.0598e-03,  8.3255e-04, -1.0740e-03, -2.2109e-05,\n",
      "          1.6470e-03, -3.6516e-04,  6.8405e-04,  1.0398e-03, -2.9918e-03,\n",
      "          6.6924e-04, -1.8695e-03, -6.3160e-05, -9.3377e-04,  2.1630e-03,\n",
      "         -3.0646e-03, -3.4122e-03, -4.3135e-03,  0.0000e+00, -4.8187e-05,\n",
      "         -1.6949e-05, -1.4218e-03,  0.0000e+00,  2.9061e-04,  0.0000e+00,\n",
      "          3.8596e-04,  0.0000e+00, -2.1168e-03, -6.4768e-04, -3.6407e-03,\n",
      "         -5.0454e-05,  1.8025e-04, -1.5067e-03, -8.2637e-04,  0.0000e+00,\n",
      "          2.6797e-03,  3.0673e-03,  4.3587e-04,  3.4029e-03, -1.0858e-03,\n",
      "         -2.9692e-04, -1.2189e-03, -1.7116e-03,  1.6008e-03, -6.4251e-04,\n",
      "          7.9896e-04,  1.5649e-03,  0.0000e+00, -2.7200e-04,  2.4476e-03,\n",
      "          3.5091e-03, -1.5237e-04, -0.0000e+00, -1.0561e-03, -6.9955e-04,\n",
      "          2.2715e-03,  3.0573e-04,  2.1982e-03,  3.6681e-06,  1.9864e-03,\n",
      "          0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0558e-03,  1.4825e-03,\n",
      "         -6.4835e-04,  2.2592e-03, -1.5025e-03, -1.3965e-03, -1.7487e-03,\n",
      "         -7.1609e-04,  2.1146e-03,  0.0000e+00,  2.7135e-04,  2.2237e-03,\n",
      "         -7.0137e-04,  1.7030e-03,  3.1018e-03,  2.1956e-03,  3.7587e-03,\n",
      "         -1.9337e-03,  2.2420e-03,  8.4923e-04,  2.5780e-03, -0.0000e+00,\n",
      "         -3.5560e-03, -1.1937e-03,  4.3272e-03, -3.8363e-03,  2.3785e-03,\n",
      "          2.5283e-03, -1.2193e-03,  3.3945e-03, -5.5683e-05,  1.2327e-03,\n",
      "          0.0000e+00, -1.2785e-03,  7.9805e-04, -2.0927e-03,  3.3845e-03,\n",
      "         -2.4449e-03,  1.6760e-03, -2.6507e-04, -3.8973e-03,  6.4325e-04,\n",
      "         -1.1662e-03, -2.2691e-03,  1.5328e-03,  0.0000e+00, -0.0000e+00,\n",
      "         -1.8260e-03,  9.6019e-04,  2.6517e-03,  2.2655e-03,  1.3165e-03,\n",
      "         -9.2632e-04,  2.1769e-03, -2.4334e-03, -8.8990e-04,  7.7342e-04,\n",
      "          1.0196e-03,  2.9333e-03,  0.0000e+00, -5.6236e-04, -2.9069e-03,\n",
      "          4.2839e-03,  2.0507e-03,  4.0906e-04,  1.9554e-03,  1.6814e-03,\n",
      "          2.4035e-03,  2.6603e-03, -2.5698e-03, -2.1220e-03,  3.4899e-03,\n",
      "          1.1747e-03,  3.3259e-03, -2.3232e-03, -7.4980e-04, -2.1344e-03,\n",
      "         -5.9998e-04,  7.0203e-04,  3.1406e-03,  3.2843e-03, -4.2250e-04,\n",
      "         -0.0000e+00, -8.2217e-04,  1.5942e-03, -1.1981e-03, -8.3177e-04,\n",
      "         -1.5550e-03, -1.8351e-03,  1.0038e-03, -0.0000e+00, -1.4314e-03,\n",
      "         -2.7797e-03, -2.3265e-04, -1.7227e-03,  8.1699e-04,  2.6287e-03,\n",
      "         -1.9305e-03,  1.4143e-04, -1.0704e-03, -5.9300e-04, -1.5005e-05,\n",
      "          1.7101e-03,  0.0000e+00,  2.8231e-03,  1.1533e-03,  1.4166e-03,\n",
      "         -3.8992e-03,  3.6974e-03, -7.4100e-04,  1.6632e-03,  0.0000e+00,\n",
      "          0.0000e+00,  6.8533e-04, -3.3102e-03, -2.1029e-03, -3.7678e-03,\n",
      "         -1.6125e-03,  6.6228e-04, -0.0000e+00,  4.1977e-04,  3.7368e-03,\n",
      "         -1.2764e-03,  2.1160e-03,  2.0384e-03, -1.7287e-03,  5.7435e-05,\n",
      "          1.7989e-04, -3.0059e-03, -1.4092e-03,  2.3904e-03, -6.2837e-04,\n",
      "         -7.2384e-04, -9.1883e-04,  0.0000e+00,  2.2839e-03, -9.9236e-04,\n",
      "          2.0519e-04, -3.8508e-03,  0.0000e+00, -2.4349e-04,  1.9803e-03,\n",
      "          5.1689e-03, -3.9892e-04,  2.8910e-03, -4.4482e-04,  2.4336e-03,\n",
      "          0.0000e+00,  1.2829e-03,  1.2385e-05, -8.8132e-04, -1.4645e-04,\n",
      "         -1.2715e-03, -1.3586e-03,  2.4161e-04,  3.5044e-04, -1.6925e-04,\n",
      "         -3.1994e-03,  1.4094e-03, -4.8109e-05,  3.9457e-03,  1.6389e-03,\n",
      "         -7.5314e-04, -1.4717e-03, -6.3988e-04,  1.2324e-03, -0.0000e+00,\n",
      "          1.1718e-03,  2.9758e-03, -1.8705e-04,  1.1021e-03,  1.7511e-04,\n",
      "          1.9640e-03, -9.9228e-04,  8.6471e-04, -4.0847e-03,  2.1239e-03,\n",
      "         -3.4624e-03,  1.4166e-03,  1.2983e-03, -1.7589e-03, -0.0000e+00,\n",
      "          3.1874e-03,  2.1961e-05,  0.0000e+00, -1.9443e-03,  8.5624e-04,\n",
      "         -1.1280e-03, -1.2910e-03, -0.0000e+00,  2.5378e-03, -2.0772e-03,\n",
      "         -2.4703e-03,  5.5517e-04, -1.3823e-03, -1.2010e-04,  1.7635e-03,\n",
      "         -3.1314e-03,  1.0206e-03,  3.3971e-03,  2.4566e-03,  6.6966e-04,\n",
      "         -2.5002e-03, -3.7697e-03,  9.0568e-04,  4.5773e-03,  6.6651e-04,\n",
      "          1.4429e-03,  1.6019e-03, -2.8862e-03,  4.3136e-03, -9.6398e-04,\n",
      "          7.8290e-04,  0.0000e+00, -2.2113e-03, -2.6769e-03,  1.0615e-03,\n",
      "          1.1263e-03, -6.6342e-04,  1.7031e-03,  3.5773e-03,  0.0000e+00,\n",
      "         -1.0072e-03, -8.8969e-04, -3.5124e-03,  1.2008e-04, -1.2942e-03,\n",
      "          0.0000e+00, -1.1652e-03,  2.3627e-03, -2.4246e-03, -3.5061e-04,\n",
      "         -2.5693e-03,  2.8017e-03, -3.2896e-03,  3.0830e-03,  1.4162e-03,\n",
      "         -0.0000e+00, -5.6670e-04, -1.2623e-03,  1.0424e-03, -9.0155e-04,\n",
      "         -0.0000e+00,  1.3790e-03, -1.6748e-03,  6.6462e-06,  8.7834e-05,\n",
      "          3.5239e-04, -1.9717e-04,  2.2519e-03, -1.3744e-03, -1.6842e-03,\n",
      "          1.8806e-03, -4.0396e-03, -9.6879e-04, -2.1434e-03,  1.0910e-03,\n",
      "         -2.9029e-03,  1.4649e-03,  1.7373e-03,  5.4096e-04,  1.1468e-03,\n",
      "         -1.1792e-03,  0.0000e+00, -1.2302e-03,  6.2412e-04, -1.7896e-03,\n",
      "         -9.3885e-04,  2.2878e-03,  1.1876e-03,  4.3840e-04,  2.8792e-03,\n",
      "         -9.2970e-04,  2.5772e-03,  1.0969e-03,  1.3925e-03,  3.1684e-03,\n",
      "         -1.8721e-03,  2.7464e-03, -0.0000e+00, -1.6574e-03,  3.1420e-04,\n",
      "         -2.8170e-03,  1.1817e-03,  4.7097e-04, -4.2737e-03,  2.3916e-03,\n",
      "         -1.4874e-03,  6.0504e-04, -4.9325e-03, -3.4452e-03,  2.5025e-03,\n",
      "          7.6101e-04,  4.6090e-04, -1.0070e-03, -6.3376e-04, -6.9539e-04,\n",
      "         -2.5678e-03, -1.3161e-03, -9.2342e-04,  2.3402e-03, -1.5837e-03,\n",
      "         -1.7699e-03,  7.8163e-04,  5.1958e-04,  3.5155e-03, -4.5818e-03,\n",
      "         -2.1487e-03,  2.7249e-04,  9.5904e-04,  2.6976e-03, -3.2253e-03,\n",
      "          0.0000e+00, -2.0204e-03, -4.6607e-03,  1.4995e-03, -1.1002e-05,\n",
      "         -7.9393e-04,  3.2548e-03, -7.5633e-04, -9.5717e-04, -1.2296e-03,\n",
      "          4.9517e-04, -6.5807e-04, -6.0886e-04, -2.3721e-03,  3.0096e-03,\n",
      "          3.7176e-03, -3.2180e-04,  1.5546e-04, -2.9297e-03, -1.3887e-03,\n",
      "          1.1290e-03,  3.1119e-03,  1.0205e-03,  1.2509e-03, -0.0000e+00,\n",
      "          3.2764e-03, -6.6004e-04]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 3.0993e-04, -5.0172e-04,  3.4022e-05,  ...,  3.9394e-05,\n",
      "         -1.4825e-04, -7.3581e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 3.0993e-04, -5.0172e-04,  3.4022e-05,  ...,  3.9394e-05,\n",
      "         -1.4825e-04, -7.3581e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 5.6145e-03,  3.7102e-03, -1.5466e-03,  5.4125e-03,  4.6873e-03,\n",
      "          2.5497e-03,  2.3377e-03, -1.1794e-03,  1.5880e-03, -6.6010e-03,\n",
      "         -3.2658e-03,  0.0000e+00,  0.0000e+00, -6.6019e-03,  2.2277e-03,\n",
      "         -3.8928e-03,  3.2497e-03, -1.9949e-03, -1.8617e-04,  2.9629e-03,\n",
      "         -1.5871e-03,  1.9267e-03,  2.4291e-03, -9.5646e-04, -2.0967e-03,\n",
      "          5.6954e-03,  9.2246e-04,  2.6822e-03,  1.0378e-03,  2.7076e-03,\n",
      "         -1.1853e-03, -3.3962e-03,  3.8879e-03, -4.5006e-03,  5.5440e-03,\n",
      "         -6.3296e-03,  3.6015e-03,  1.8973e-03,  1.9334e-03, -1.2306e-03,\n",
      "          2.3010e-03,  7.1953e-03, -6.0655e-03,  3.4957e-03, -6.0697e-03,\n",
      "          5.9398e-03, -1.9094e-03, -4.5113e-04,  2.5816e-05,  2.7862e-03,\n",
      "          6.5562e-04, -5.8904e-03, -2.2811e-03, -1.8201e-03,  3.2271e-03,\n",
      "          1.9466e-03, -5.0127e-03, -0.0000e+00,  6.1463e-05,  0.0000e+00,\n",
      "          7.8829e-03, -0.0000e+00,  1.8370e-04,  2.2694e-03, -1.2342e-04,\n",
      "          4.5619e-04, -1.4746e-03,  2.9788e-03, -7.8751e-04, -1.2059e-03,\n",
      "          6.5053e-03,  3.3896e-03, -2.0778e-03,  1.5028e-03, -1.5030e-03,\n",
      "          2.0886e-03,  7.4608e-03,  2.4709e-04,  0.0000e+00,  6.2762e-04,\n",
      "         -3.1543e-03, -2.7453e-03,  2.6952e-03, -5.1558e-03,  2.1890e-04,\n",
      "          1.4570e-03,  6.7165e-03,  4.8209e-04, -1.7293e-03, -5.1056e-03,\n",
      "         -5.2884e-03, -3.6853e-03,  4.0147e-03,  1.9872e-03, -3.5715e-03,\n",
      "         -8.5616e-03,  2.2217e-03,  3.1897e-04,  3.3312e-03,  1.7497e-03,\n",
      "         -1.6922e-03,  3.2081e-03,  4.8553e-05,  1.1485e-03, -9.0776e-04,\n",
      "          6.1262e-03,  2.5010e-03,  3.1744e-03,  2.2100e-03,  2.5304e-03,\n",
      "          0.0000e+00,  3.7201e-03,  1.4096e-03, -1.4686e-03,  1.4567e-03,\n",
      "          3.0884e-03,  8.4993e-04, -8.2392e-04,  7.7249e-04, -3.1958e-03,\n",
      "         -7.2660e-04, -3.0232e-03, -1.9751e-03, -6.4230e-04,  4.8933e-03,\n",
      "         -3.2019e-03, -4.9601e-03, -6.9823e-03,  0.0000e+00,  1.2056e-03,\n",
      "          2.0716e-03, -1.3264e-03, -0.0000e+00, -3.6152e-04,  0.0000e+00,\n",
      "          1.3468e-04,  0.0000e+00, -4.1089e-03, -1.0380e-03, -7.3235e-03,\n",
      "          3.2828e-03,  9.7271e-04, -2.3781e-03, -3.4543e-03,  0.0000e+00,\n",
      "          4.6157e-03,  3.4196e-03,  4.0464e-03,  6.6869e-03, -2.1002e-03,\n",
      "         -9.2864e-04, -2.9230e-03, -2.8433e-03,  3.8009e-03, -9.4571e-04,\n",
      "         -8.6413e-04,  2.5424e-03,  0.0000e+00,  2.3966e-03,  4.0992e-03,\n",
      "          7.0407e-03,  8.2541e-04, -0.0000e+00,  1.0982e-03, -1.2842e-03,\n",
      "          3.4984e-03,  1.4478e-03,  3.6884e-03, -1.7087e-03,  2.9753e-03,\n",
      "          0.0000e+00, -0.0000e+00, -0.0000e+00, -3.5449e-03,  7.5155e-04,\n",
      "          1.2363e-03,  4.8504e-03, -2.1546e-03, -2.3255e-03, -2.4296e-03,\n",
      "          2.1987e-05,  5.0862e-03,  0.0000e+00, -9.0825e-04,  3.0895e-03,\n",
      "         -2.4461e-03,  8.5325e-04,  6.3336e-03,  3.4690e-03,  7.7699e-03,\n",
      "         -1.4936e-03,  3.6079e-03, -3.8986e-04,  4.8818e-03, -0.0000e+00,\n",
      "         -7.5058e-03, -1.9127e-03,  8.6772e-03, -6.7789e-03,  7.6636e-03,\n",
      "          4.7372e-03,  1.0909e-03,  4.4020e-03, -7.6759e-04,  2.0701e-03,\n",
      "          0.0000e+00, -2.1006e-03,  1.3935e-03, -4.1781e-03,  8.6778e-03,\n",
      "         -6.4290e-03,  2.6617e-03, -1.3909e-03, -6.4625e-03,  1.3807e-05,\n",
      "         -2.8915e-03, -1.5197e-03,  5.2341e-03,  0.0000e+00, -0.0000e+00,\n",
      "         -3.2010e-03, -1.7491e-05,  5.0285e-03,  8.0016e-03,  2.8962e-03,\n",
      "         -1.9602e-03,  6.5825e-03, -4.2173e-03, -1.5932e-03,  6.9724e-04,\n",
      "         -1.4872e-03,  5.1363e-03,  0.0000e+00, -2.4188e-03, -3.9688e-03,\n",
      "          7.9746e-03,  4.0460e-03, -5.1624e-04,  3.0620e-03,  4.0097e-03,\n",
      "          3.9649e-03,  6.3803e-03, -3.7654e-03, -3.8691e-03,  6.2430e-03,\n",
      "          1.0751e-03,  6.4202e-03, -5.7290e-03, -1.3018e-03, -2.8064e-03,\n",
      "         -1.4703e-04,  1.6336e-03,  5.4501e-03,  5.1273e-03, -1.0634e-03,\n",
      "         -0.0000e+00, -8.2417e-04,  2.3736e-03, -2.2876e-03, -2.4324e-03,\n",
      "         -2.3975e-03, -3.5086e-03,  1.6643e-03, -0.0000e+00, -1.9348e-03,\n",
      "         -4.8475e-03, -1.8452e-03, -8.3179e-04, -3.6739e-04,  6.3500e-03,\n",
      "         -4.5380e-03, -1.5215e-03, -2.8020e-03,  4.3337e-04,  9.6001e-04,\n",
      "          4.5022e-03, -0.0000e+00,  5.8813e-03,  3.5682e-03,  2.5918e-03,\n",
      "         -7.1105e-03,  5.4973e-03,  1.0394e-03,  8.2729e-04,  0.0000e+00,\n",
      "         -0.0000e+00,  2.8289e-04, -7.6090e-03, -3.4469e-03, -7.1062e-03,\n",
      "         -1.6930e-03,  1.3043e-03, -0.0000e+00, -2.2501e-04,  5.3851e-03,\n",
      "         -3.6381e-03,  1.9617e-03,  3.0157e-03, -2.6041e-03, -2.8207e-03,\n",
      "          9.7789e-04, -9.4984e-03, -2.0709e-03,  2.5490e-03, -3.8181e-03,\n",
      "         -1.0270e-03, -2.6162e-03,  0.0000e+00,  2.1636e-03,  4.2157e-04,\n",
      "         -3.7025e-05, -7.6404e-03,  0.0000e+00, -2.2497e-03,  1.9685e-03,\n",
      "          7.7939e-03, -2.8875e-03,  6.2423e-03, -2.8631e-03,  6.2085e-03,\n",
      "          0.0000e+00,  2.1567e-03,  2.4753e-04, -6.5495e-04, -1.4860e-03,\n",
      "         -2.0926e-03, -1.8127e-03,  9.1804e-04, -1.4580e-03, -2.4117e-03,\n",
      "         -5.4168e-03,  2.8130e-03, -6.5499e-04,  7.2892e-03,  1.9893e-03,\n",
      "          5.6235e-04, -2.4241e-03, -1.3407e-03, -1.0075e-03, -0.0000e+00,\n",
      "          9.0850e-04,  3.8712e-03, -4.5154e-04,  3.1761e-04, -6.7801e-04,\n",
      "          4.7620e-03, -9.5227e-04,  2.3833e-03, -6.0878e-03,  4.2993e-03,\n",
      "         -5.0429e-03,  3.4945e-03,  8.2160e-04, -3.9668e-03, -0.0000e+00,\n",
      "          8.1059e-03,  2.9728e-04,  0.0000e+00, -2.3050e-03,  2.8340e-03,\n",
      "         -2.8540e-03, -3.3655e-03, -0.0000e+00,  4.6210e-03, -4.3793e-03,\n",
      "         -1.5837e-03, -2.3368e-03, -2.3859e-03,  1.8611e-03,  3.5882e-03,\n",
      "         -3.8807e-03,  1.9643e-03,  7.2312e-03,  3.7268e-03, -4.0519e-04,\n",
      "         -5.6364e-03, -6.3756e-03, -1.5649e-03,  7.3859e-03,  2.0803e-03,\n",
      "          1.9564e-03,  2.2322e-03, -5.3406e-03,  5.4728e-03, -2.0828e-03,\n",
      "          8.1966e-04,  0.0000e+00, -4.1416e-03, -3.6589e-03,  3.3630e-03,\n",
      "          2.1675e-03,  1.8026e-04,  3.1725e-03,  4.5001e-03,  0.0000e+00,\n",
      "         -5.3107e-03, -4.3238e-05, -5.1426e-03,  3.9597e-04, -4.3718e-03,\n",
      "          0.0000e+00,  9.4859e-04,  2.8593e-03, -2.1922e-03, -8.0303e-04,\n",
      "         -4.3476e-03,  4.0169e-03, -7.0201e-03,  4.2098e-03,  4.4500e-03,\n",
      "          0.0000e+00, -4.1621e-03, -4.4655e-03,  4.8219e-04, -2.2464e-03,\n",
      "         -0.0000e+00,  2.5049e-03, -4.6780e-03, -1.7006e-03,  1.0904e-03,\n",
      "          9.3693e-04,  1.0165e-03,  3.7937e-03, -5.5267e-04, -1.9151e-03,\n",
      "          4.1897e-03, -7.1941e-03, -1.0581e-03, -7.0674e-04,  8.2082e-04,\n",
      "         -3.6163e-03,  2.8704e-03,  2.0304e-03,  6.0493e-05,  1.1174e-04,\n",
      "         -3.7305e-03,  0.0000e+00, -3.2148e-03,  2.0870e-03, -1.8572e-03,\n",
      "         -3.7009e-03,  6.1229e-03,  3.3831e-03,  6.3681e-04,  3.1228e-03,\n",
      "         -2.2021e-03,  3.2818e-03,  5.2425e-03,  3.1143e-03,  3.8255e-03,\n",
      "         -5.8162e-03,  6.3929e-03,  0.0000e+00, -1.8021e-03, -2.1977e-04,\n",
      "         -5.5751e-03, -3.2068e-05, -1.9832e-03, -7.1481e-03,  4.2097e-03,\n",
      "         -2.6468e-03,  3.0677e-03, -9.1386e-03, -3.7047e-03,  4.2745e-03,\n",
      "          1.0160e-03,  3.8438e-04, -3.4076e-03,  1.3378e-03, -2.3700e-03,\n",
      "         -7.5833e-03, -1.8665e-03, -2.4851e-03,  4.0606e-03, -2.7531e-03,\n",
      "         -4.6527e-03,  1.1401e-03,  2.2428e-04,  5.6717e-03, -8.1371e-03,\n",
      "         -4.7614e-03,  1.1780e-03,  3.4558e-03,  2.8999e-03, -3.5857e-03,\n",
      "          0.0000e+00, -4.4731e-03, -8.5121e-03,  3.7389e-03,  7.4157e-04,\n",
      "         -4.6160e-04,  5.2898e-03, -1.4796e-03, -3.4846e-03, -1.5525e-03,\n",
      "          2.3251e-03, -1.2252e-03, -2.3520e-03, -5.2691e-03,  4.4132e-03,\n",
      "          7.3381e-03, -2.7765e-04, -1.8850e-03, -3.6398e-03, -3.3324e-03,\n",
      "          4.3671e-04,  4.7175e-03,  1.9768e-03,  2.4104e-04, -0.0000e+00,\n",
      "          6.9139e-03, -1.8381e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 7.7725e-04, -1.2482e-03,  8.6866e-05,  ...,  2.9132e-04,\n",
      "         -5.4026e-04, -9.5909e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 7.7725e-04, -1.2482e-03,  8.6866e-05,  ...,  2.9132e-04,\n",
      "         -5.4026e-04, -9.5909e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 4.7437e-03,  3.3522e-03, -2.3507e-03,  6.8425e-03,  6.4116e-03,\n",
      "          2.4310e-03,  4.6166e-03, -1.7184e-03,  4.0774e-04, -7.4956e-03,\n",
      "         -4.8975e-03,  0.0000e+00,  0.0000e+00, -8.7650e-03,  2.7933e-03,\n",
      "         -5.2810e-03,  3.8277e-03, -3.1521e-03, -2.4845e-04,  5.3074e-03,\n",
      "         -2.5767e-03,  3.6418e-03,  1.9439e-03, -1.7089e-03, -3.1477e-03,\n",
      "          6.0499e-03, -4.1515e-04,  3.8962e-03,  9.5723e-04,  3.7685e-03,\n",
      "         -3.0298e-03, -4.2526e-03,  4.2009e-03, -6.9410e-03,  6.7562e-03,\n",
      "         -7.6245e-03,  2.4683e-03,  1.8476e-03,  3.1634e-03, -1.8167e-03,\n",
      "          3.1919e-03,  8.3052e-03, -7.6532e-03,  4.2411e-03, -7.9454e-03,\n",
      "          8.2025e-03, -3.2389e-03, -2.4526e-03, -8.1379e-04,  4.9993e-03,\n",
      "          5.9062e-04, -4.5024e-03, -2.5915e-03, -1.3613e-03,  3.9438e-03,\n",
      "          3.0952e-03, -5.1944e-03, -0.0000e+00, -6.1269e-04,  0.0000e+00,\n",
      "          1.0318e-02,  0.0000e+00,  1.6358e-03,  2.2686e-03, -1.9826e-04,\n",
      "          8.4588e-04, -1.5394e-03,  3.7882e-03, -1.4975e-04, -2.3251e-03,\n",
      "          6.4616e-03,  4.1683e-03,  1.1861e-03,  1.4454e-04, -3.8807e-03,\n",
      "          4.6037e-03,  7.0985e-03,  2.2788e-03,  0.0000e+00,  1.5475e-03,\n",
      "         -2.7702e-03, -2.1202e-03,  2.4711e-03, -7.4102e-03,  8.4800e-04,\n",
      "          2.8702e-03,  8.1967e-03,  2.2325e-03, -3.1149e-03, -6.6484e-03,\n",
      "         -7.5927e-03, -6.4491e-03,  5.6768e-03,  2.0037e-03, -4.7598e-03,\n",
      "         -1.0073e-02,  2.6512e-03,  8.1762e-04,  4.1716e-03,  1.5880e-03,\n",
      "         -1.9662e-03,  4.7268e-03,  8.4117e-04,  2.3213e-03, -2.0877e-04,\n",
      "          7.5960e-03,  2.9430e-03,  4.7141e-03,  7.0587e-04,  2.8149e-03,\n",
      "          0.0000e+00,  4.0317e-03,  2.1464e-03, -1.9067e-03,  3.3281e-03,\n",
      "          4.2535e-03,  4.1937e-04, -3.1859e-04, -1.0955e-04, -6.9019e-03,\n",
      "         -2.4755e-04, -4.7870e-03, -2.6077e-03, -9.2266e-04,  6.8278e-03,\n",
      "         -5.0029e-03, -6.4194e-03, -7.3338e-03,  0.0000e+00,  3.5416e-03,\n",
      "          2.2257e-03, -5.8656e-04,  0.0000e+00, -1.0228e-03,  0.0000e+00,\n",
      "         -2.0007e-04,  0.0000e+00, -1.9210e-03, -3.1273e-03, -7.1685e-03,\n",
      "          3.3053e-03,  2.5542e-03, -4.2307e-03, -4.1734e-03,  0.0000e+00,\n",
      "          5.9072e-03,  4.1225e-03,  7.0063e-03,  6.5281e-03, -1.3597e-03,\n",
      "          9.0376e-05, -4.3308e-03, -2.3345e-03,  5.5719e-03,  1.7213e-03,\n",
      "         -7.9003e-04,  4.4707e-03,  0.0000e+00,  1.4739e-03,  5.4854e-03,\n",
      "          7.3623e-03,  8.2512e-04, -0.0000e+00,  6.3416e-04, -2.7122e-03,\n",
      "          4.1767e-03,  9.5291e-04,  4.6695e-03, -2.1385e-03,  5.0117e-03,\n",
      "          0.0000e+00, -0.0000e+00, -0.0000e+00, -3.2444e-03,  2.7635e-04,\n",
      "          1.2782e-03,  7.1767e-03, -4.1768e-03, -1.6015e-03, -2.6368e-03,\n",
      "          9.2857e-04,  7.4058e-03,  0.0000e+00, -1.4008e-03,  4.3578e-03,\n",
      "         -2.9535e-03,  5.8604e-04,  5.2532e-03,  3.9561e-03,  8.3124e-03,\n",
      "         -1.0297e-03,  4.3891e-03, -2.5475e-03,  5.3907e-03, -0.0000e+00,\n",
      "         -8.6715e-03, -4.1425e-03,  1.0990e-02, -8.8053e-03,  9.3184e-03,\n",
      "          7.4196e-03,  2.1252e-03,  4.6344e-03,  1.0675e-03,  2.8072e-03,\n",
      "          0.0000e+00, -2.9138e-03,  3.7968e-03, -3.9240e-03,  1.1365e-02,\n",
      "         -7.3545e-03,  2.3059e-03, -1.4175e-03, -6.4646e-03, -6.9065e-04,\n",
      "         -3.7455e-03, -1.5353e-03,  5.9930e-03,  0.0000e+00, -0.0000e+00,\n",
      "         -4.4003e-03,  5.8296e-04,  5.3450e-03,  1.0416e-02,  2.3950e-03,\n",
      "         -3.5738e-03,  9.5212e-03, -4.3839e-03, -2.4327e-03, -7.6445e-04,\n",
      "         -8.6341e-04,  7.3660e-03,  0.0000e+00, -2.4047e-03, -5.7151e-03,\n",
      "          8.9339e-03,  4.7954e-03, -7.8311e-04,  3.6096e-03,  5.3926e-03,\n",
      "          6.1158e-03,  7.6080e-03, -6.1178e-03, -3.8083e-03,  7.5473e-03,\n",
      "          3.7153e-03,  6.1714e-03, -6.3122e-03, -4.6831e-03, -4.8052e-03,\n",
      "         -8.3263e-04,  2.8607e-03,  6.4465e-03,  5.5976e-03, -1.4072e-03,\n",
      "         -0.0000e+00,  2.6107e-04,  4.0591e-03, -4.1733e-03, -2.2319e-03,\n",
      "         -2.8371e-03, -4.8147e-03, -2.4768e-04, -0.0000e+00, -2.8198e-03,\n",
      "         -4.8271e-03, -1.6657e-03, -5.4236e-04,  1.3831e-03,  8.7539e-03,\n",
      "         -5.8644e-03, -2.7388e-03, -3.4430e-03, -8.2397e-04,  1.8253e-03,\n",
      "          4.7606e-03,  0.0000e+00,  7.7876e-03,  3.7768e-03,  2.7481e-03,\n",
      "         -8.3513e-03,  7.3550e-03,  1.8598e-03,  6.3466e-04,  0.0000e+00,\n",
      "         -0.0000e+00, -3.0436e-04, -1.1438e-02, -4.6559e-03, -8.5645e-03,\n",
      "         -1.8082e-03,  1.0303e-03, -0.0000e+00, -1.8055e-03,  8.0068e-03,\n",
      "         -3.4451e-03,  9.4004e-04,  3.9395e-03, -4.9148e-03, -3.1177e-03,\n",
      "          2.2730e-03, -8.2232e-03, -3.6293e-03,  1.8770e-03, -4.2294e-03,\n",
      "         -5.5665e-04, -1.4544e-03,  0.0000e+00,  2.7483e-03, -1.7432e-04,\n",
      "          4.8492e-04, -9.8609e-03, -0.0000e+00, -4.4052e-03,  3.9328e-03,\n",
      "          8.7657e-03, -3.0180e-03,  7.0602e-03, -4.4524e-03,  7.4226e-03,\n",
      "          0.0000e+00,  4.8530e-03, -1.1426e-03, -3.8982e-04, -2.1496e-03,\n",
      "         -1.3491e-03, -1.0009e-03,  1.0038e-04,  6.4724e-05, -2.7965e-03,\n",
      "         -6.5729e-03,  2.7510e-03, -8.7818e-04,  8.2821e-03,  1.9314e-03,\n",
      "         -5.0524e-04, -3.0160e-03, -2.0286e-03, -1.9007e-03, -0.0000e+00,\n",
      "          6.1643e-04,  3.5226e-03, -9.4686e-04,  1.8730e-03,  1.9293e-04,\n",
      "          3.8766e-03, -1.7271e-03,  4.6367e-03, -6.8393e-03,  5.1377e-03,\n",
      "         -7.5065e-03,  2.5279e-03,  9.2178e-04, -6.6745e-03, -0.0000e+00,\n",
      "          1.1070e-02,  1.1437e-04, -0.0000e+00, -2.6460e-03,  4.2037e-03,\n",
      "         -3.2203e-03, -5.4442e-03, -0.0000e+00,  3.4766e-03, -5.4051e-03,\n",
      "         -2.4882e-03, -1.9559e-03, -2.9577e-03,  8.4453e-04,  4.6743e-03,\n",
      "         -4.0860e-03,  1.0946e-03,  8.8222e-03,  3.5245e-03, -3.5121e-04,\n",
      "         -7.1000e-03, -7.9368e-03, -6.5054e-04,  8.3028e-03,  3.3237e-03,\n",
      "          3.2223e-03,  3.7181e-03, -5.8403e-03,  6.0542e-03, -6.6113e-04,\n",
      "          4.1827e-04,  0.0000e+00, -3.7798e-03, -4.0282e-03,  3.9860e-03,\n",
      "          2.5831e-03,  2.6942e-04,  2.8951e-03,  2.8918e-03, -0.0000e+00,\n",
      "         -5.2041e-03, -1.7724e-03, -6.4003e-03,  1.5483e-03, -4.0812e-03,\n",
      "          0.0000e+00, -6.4171e-04,  3.0093e-03, -4.5177e-03, -1.0215e-03,\n",
      "         -5.2633e-03,  1.8190e-03, -8.9478e-03,  3.9290e-03,  7.2441e-03,\n",
      "          0.0000e+00, -3.0690e-03, -4.4049e-03,  1.6000e-03, -2.2372e-03,\n",
      "         -0.0000e+00,  4.3926e-03, -4.5713e-03, -1.4422e-03,  1.7469e-03,\n",
      "          1.5546e-04, -5.1545e-04,  4.2725e-03, -1.0112e-04, -3.3810e-03,\n",
      "          5.6811e-03, -8.8223e-03, -2.5519e-03, -7.7982e-04,  1.5197e-04,\n",
      "         -5.3416e-03,  3.6458e-03,  1.9169e-03, -2.8245e-04,  6.2610e-05,\n",
      "         -5.4411e-03,  0.0000e+00, -3.9157e-03,  2.8421e-03, -2.7994e-03,\n",
      "         -4.0234e-03,  6.4661e-03,  3.0095e-03,  7.1242e-04,  4.3497e-03,\n",
      "         -1.6432e-03,  2.7575e-03,  7.0365e-03,  4.9558e-03,  3.2812e-03,\n",
      "         -7.6416e-03,  8.3372e-03,  0.0000e+00, -2.1122e-03,  8.3223e-04,\n",
      "         -6.9245e-03,  1.5397e-04, -3.5423e-03, -6.5697e-03,  3.5501e-03,\n",
      "         -4.5848e-03,  5.4561e-03, -1.2229e-02, -2.9372e-03,  4.0107e-03,\n",
      "          1.7263e-03,  3.0553e-04, -4.7052e-03,  2.6509e-03, -3.0506e-03,\n",
      "         -7.9652e-03, -2.3345e-03, -4.0911e-03,  4.4120e-03, -3.8926e-03,\n",
      "         -7.0591e-03,  1.2840e-03, -5.7790e-04,  7.3037e-03, -9.1072e-03,\n",
      "         -7.0526e-03,  1.5060e-03,  4.1574e-03,  4.1113e-03, -4.5737e-03,\n",
      "          0.0000e+00, -4.4581e-03, -1.0308e-02,  5.5350e-03,  1.4026e-03,\n",
      "         -1.3981e-03,  6.7244e-03, -2.2484e-03, -6.6891e-03, -3.6219e-03,\n",
      "          3.0581e-03, -1.3165e-03, -2.3421e-03, -9.0291e-03,  5.9826e-03,\n",
      "          1.1114e-02, -2.0321e-03, -6.1304e-04, -4.1941e-03, -4.9790e-03,\n",
      "          8.5862e-05,  6.9036e-03,  2.2403e-03, -1.0585e-03, -0.0000e+00,\n",
      "          6.0723e-03, -5.5345e-04]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 1.5279e-03, -1.8104e-03,  4.3533e-05,  ...,  4.7020e-04,\n",
      "         -1.0207e-03, -1.3235e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 1.5279e-03, -1.8104e-03,  4.3533e-05,  ...,  4.7020e-04,\n",
      "         -1.0207e-03, -1.3235e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0053,  0.0042, -0.0012,  ..., -0.0000,  0.0062, -0.0006],\n",
      "        [ 0.0028,  0.0000, -0.0008,  ..., -0.0012,  0.0000, -0.0007]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 1.9959e-03, -2.3442e-03,  3.5440e-05,  ...,  6.7354e-04,\n",
      "         -1.5347e-03, -2.0832e-04],\n",
      "        [ 2.6783e-04, -4.7124e-04,  5.1080e-05,  ...,  2.5446e-05,\n",
      "          4.7902e-05, -2.1988e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 1.9959e-03, -2.3442e-03,  3.5440e-05,  ...,  6.7354e-04,\n",
      "         -1.5347e-03, -2.0832e-04],\n",
      "        [ 2.6783e-04, -4.7124e-04,  5.1080e-05,  ...,  2.5446e-05,\n",
      "          4.7902e-05, -2.1988e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0062,  0.0048, -0.0018,  ..., -0.0000,  0.0081, -0.0006],\n",
      "        [ 0.0048,  0.0000, -0.0004,  ...,  0.0009,  0.0000, -0.0008],\n",
      "        [ 0.0028,  0.0014, -0.0008,  ..., -0.0012,  0.0033, -0.0007]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 2.3402e-03, -2.7580e-03,  1.6042e-04,  ...,  8.4845e-04,\n",
      "         -2.0483e-03, -5.1211e-04],\n",
      "        [ 8.0774e-04, -9.0195e-04,  3.3320e-04,  ...,  2.0887e-04,\n",
      "         -2.1567e-04, -5.0595e-04],\n",
      "        [ 1.7930e-04, -4.6989e-04, -6.3288e-05,  ...,  6.9423e-06,\n",
      "          1.0502e-04, -1.1559e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 2.3402e-03, -2.7580e-03,  1.6042e-04,  ...,  8.4845e-04,\n",
      "         -2.0483e-03, -5.1211e-04],\n",
      "        [ 8.0774e-04, -9.0195e-04,  3.3320e-04,  ...,  2.0887e-04,\n",
      "         -2.1567e-04, -5.0595e-04],\n",
      "        [ 1.7930e-04, -4.6989e-04, -6.3288e-05,  ...,  6.9423e-06,\n",
      "          1.0502e-04, -1.1559e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0062,  0.0049, -0.0019,  ..., -0.0000,  0.0079, -0.0008],\n",
      "        [ 0.0046,  0.0000, -0.0006,  ...,  0.0006,  0.0000, -0.0011],\n",
      "        [ 0.0047,  0.0034, -0.0011,  ..., -0.0011,  0.0050, -0.0003]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0027, -0.0031,  0.0003,  ...,  0.0010, -0.0025, -0.0007],\n",
      "        [ 0.0011, -0.0015,  0.0004,  ...,  0.0005, -0.0002, -0.0009],\n",
      "        [ 0.0004, -0.0008, -0.0001,  ..., -0.0001,  0.0002, -0.0001]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0027, -0.0031,  0.0003,  ...,  0.0010, -0.0025, -0.0007],\n",
      "        [ 0.0011, -0.0015,  0.0004,  ...,  0.0005, -0.0002, -0.0009],\n",
      "        [ 0.0004, -0.0008, -0.0001,  ..., -0.0001,  0.0002, -0.0001]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0085,  0.0064, -0.0021,  ..., -0.0000,  0.0096, -0.0016],\n",
      "        [ 0.0074,  0.0000, -0.0006,  ...,  0.0001,  0.0000,  0.0006],\n",
      "        [ 0.0054,  0.0046, -0.0017,  ..., -0.0011,  0.0056,  0.0012]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0029, -0.0033,  0.0004,  ...,  0.0013, -0.0028, -0.0010],\n",
      "        [ 0.0012, -0.0023,  0.0004,  ...,  0.0005, -0.0003, -0.0013],\n",
      "        [ 0.0005, -0.0014, -0.0003,  ..., -0.0002,  0.0003, -0.0002]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0029, -0.0033,  0.0004,  ...,  0.0013, -0.0028, -0.0010],\n",
      "        [ 0.0012, -0.0023,  0.0004,  ...,  0.0005, -0.0003, -0.0013],\n",
      "        [ 0.0005, -0.0014, -0.0003,  ..., -0.0002,  0.0003, -0.0002]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 6.9808e-03,  6.1869e-03, -1.2474e-03,  ..., -0.0000e+00,\n",
      "          8.1038e-03, -3.5403e-03],\n",
      "        [ 8.5911e-03,  0.0000e+00,  6.0740e-05,  ...,  1.3158e-03,\n",
      "          0.0000e+00,  4.5774e-04],\n",
      "        [ 6.4377e-03,  4.7569e-03, -3.8948e-04,  ..., -2.0943e-04,\n",
      "          5.4095e-03,  1.8878e-04]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 2.7193e-03, -3.6205e-03,  3.3395e-04,  ...,  1.5383e-03,\n",
      "         -3.1962e-03, -1.1268e-03],\n",
      "        [ 1.1651e-03, -2.6599e-03,  3.6390e-04,  ...,  5.2208e-04,\n",
      "         -1.9328e-04, -1.7175e-03],\n",
      "        [ 3.9546e-04, -2.1668e-03, -4.6889e-04,  ...,  3.1293e-04,\n",
      "         -8.6180e-05,  4.3383e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 2.7193e-03, -3.6205e-03,  3.3395e-04,  ...,  1.5383e-03,\n",
      "         -3.1962e-03, -1.1268e-03],\n",
      "        [ 1.1651e-03, -2.6599e-03,  3.6390e-04,  ...,  5.2208e-04,\n",
      "         -1.9328e-04, -1.7175e-03],\n",
      "        [ 3.9546e-04, -2.1668e-03, -4.6889e-04,  ...,  3.1293e-04,\n",
      "         -8.6180e-05,  4.3383e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 6.0352e-03,  5.4789e-03, -6.9219e-05,  ..., -0.0000e+00,\n",
      "          8.1498e-03, -4.5524e-03],\n",
      "        [ 8.4825e-03,  0.0000e+00, -7.7061e-04,  ..., -5.4001e-04,\n",
      "          0.0000e+00,  9.0527e-04],\n",
      "        [ 6.8579e-03,  5.4174e-03, -5.9865e-04,  ...,  6.8416e-05,\n",
      "          5.2789e-03,  4.7986e-04]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 0 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 2.5507e-03, -4.1890e-03,  1.1653e-04,  ...,  1.8033e-03,\n",
      "         -3.3940e-03, -1.1982e-03],\n",
      "        [ 1.4233e-03, -2.7195e-03,  1.8651e-04,  ...,  7.7403e-04,\n",
      "         -4.3797e-05, -1.9745e-03],\n",
      "        [ 2.3307e-04, -2.8212e-03, -2.0759e-04,  ...,  4.8184e-04,\n",
      "         -5.3186e-04,  1.9959e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 2.5507e-03, -4.1890e-03,  1.1653e-04,  ...,  1.8033e-03,\n",
      "         -3.3940e-03, -1.1982e-03],\n",
      "        [ 1.4233e-03, -2.7195e-03,  1.8651e-04,  ...,  7.7403e-04,\n",
      "         -4.3797e-05, -1.9745e-03],\n",
      "        [ 2.3307e-04, -2.8212e-03, -2.0759e-04,  ...,  4.8184e-04,\n",
      "         -5.3186e-04,  1.9959e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0071,  0.0052, -0.0002,  ..., -0.0000,  0.0089, -0.0032],\n",
      "        [ 0.0088,  0.0000, -0.0008,  ..., -0.0022,  0.0000,  0.0008],\n",
      "        [ 0.0075,  0.0055, -0.0006,  ..., -0.0018,  0.0069,  0.0007]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[-1.5158e-03, -5.0278e-04, -7.7537e-04,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00],\n",
      "         [-1.1270e-03, -3.8583e-04, -3.5395e-04,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [-6.2726e-04, -0.0000e+00, -6.4494e-04,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[ 2.6878e-03, -4.7545e-03, -3.0655e-05,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03],\n",
      "         [ 1.7130e-03, -3.0089e-03,  9.5765e-05,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [ 2.4044e-04, -3.7126e-03, -1.3427e-04,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tsince layer_index != 0, adding cache to output sequence\n",
      "stacked_sequence_output.shape = torch.Size([2, 3, 10, 1024])\n",
      "final_states = (tensor([[[-5.6458e-03,  9.7251e-03, -2.3428e-04,  ..., -0.0000e+00,\n",
      "           8.9434e-03, -3.2452e-03],\n",
      "         [-5.9120e-03,  8.5314e-03, -1.4924e-03,  ..., -2.1824e-03,\n",
      "           0.0000e+00,  7.6433e-04],\n",
      "         [-7.4715e-03,  8.3881e-03,  2.9731e-04,  ..., -1.7795e-03,\n",
      "           6.9465e-03,  6.8041e-04]],\n",
      "\n",
      "        [[ 2.4202e-03,  0.0000e+00,  2.0514e-03,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00],\n",
      "         [ 2.3629e-03,  0.0000e+00,  1.1207e-03,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [ 0.0000e+00,  3.1858e-03,  1.0247e-03,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04]]], device='cuda:0', grad_fn=<CatBackward>), tensor([[[-1.7503e-02, -1.8750e-02,  4.6151e-03,  ..., -1.0900e-02,\n",
      "          -2.0248e-02,  1.6588e-02],\n",
      "         [-1.4481e-02, -1.6137e-02,  3.5963e-03,  ..., -4.0869e-03,\n",
      "          -1.0450e-02,  1.6065e-02],\n",
      "         [-1.6703e-02, -1.6557e-02,  3.4767e-03,  ..., -4.2885e-03,\n",
      "          -2.1409e-02,  1.4609e-02]],\n",
      "\n",
      "        [[ 9.0830e-04, -3.6197e-03,  7.5843e-03,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03],\n",
      "         [-8.0450e-04, -3.0386e-03,  5.7585e-03,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [-7.2586e-04, -4.7068e-03,  5.0241e-03,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04]]], device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "stacked\n",
      "num_layers = 2\n",
      "num_valid = 3\n",
      "returned_timesteps = 10\n",
      "encoder_dim = 1024\n",
      "num_valid < batch_size -> False\n",
      "sequence_length_difference = total_sequence_length - returned_timesteps\n",
      "sequence_length_difference = 0\n",
      "sequence_length_difference is larger than 0? : False\n",
      "UPDATE STATES... inputs: final_states, restoration_indices\n",
      "_EncoderBase의 `_update_states` 메서드 실행\n",
      "inputs:\n",
      "final_states = (tensor([[[-5.6458e-03,  9.7251e-03, -2.3428e-04,  ..., -0.0000e+00,\n",
      "           8.9434e-03, -3.2452e-03],\n",
      "         [-5.9120e-03,  8.5314e-03, -1.4924e-03,  ..., -2.1824e-03,\n",
      "           0.0000e+00,  7.6433e-04],\n",
      "         [-7.4715e-03,  8.3881e-03,  2.9731e-04,  ..., -1.7795e-03,\n",
      "           6.9465e-03,  6.8041e-04]],\n",
      "\n",
      "        [[ 2.4202e-03,  0.0000e+00,  2.0514e-03,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00],\n",
      "         [ 2.3629e-03,  0.0000e+00,  1.1207e-03,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [ 0.0000e+00,  3.1858e-03,  1.0247e-03,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04]]], device='cuda:0', grad_fn=<CatBackward>), tensor([[[-1.7503e-02, -1.8750e-02,  4.6151e-03,  ..., -1.0900e-02,\n",
      "          -2.0248e-02,  1.6588e-02],\n",
      "         [-1.4481e-02, -1.6137e-02,  3.5963e-03,  ..., -4.0869e-03,\n",
      "          -1.0450e-02,  1.6065e-02],\n",
      "         [-1.6703e-02, -1.6557e-02,  3.4767e-03,  ..., -4.2885e-03,\n",
      "          -2.1409e-02,  1.4609e-02]],\n",
      "\n",
      "        [[ 9.0830e-04, -3.6197e-03,  7.5843e-03,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03],\n",
      "         [-8.0450e-04, -3.0386e-03,  5.7585e-03,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [-7.2586e-04, -4.7068e-03,  5.0241e-03,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04]]], device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "new_unsorted_states = [tensor([[[-5.9120e-03,  8.5314e-03, -1.4924e-03,  ..., -2.1824e-03,\n",
      "           0.0000e+00,  7.6433e-04],\n",
      "         [-7.4715e-03,  8.3881e-03,  2.9731e-04,  ..., -1.7795e-03,\n",
      "           6.9465e-03,  6.8041e-04],\n",
      "         [-5.6458e-03,  9.7251e-03, -2.3428e-04,  ..., -0.0000e+00,\n",
      "           8.9434e-03, -3.2452e-03]],\n",
      "\n",
      "        [[ 2.3629e-03,  0.0000e+00,  1.1207e-03,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [ 0.0000e+00,  3.1858e-03,  1.0247e-03,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04],\n",
      "         [ 2.4202e-03,  0.0000e+00,  2.0514e-03,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00]]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>), tensor([[[-1.4481e-02, -1.6137e-02,  3.5963e-03,  ..., -4.0869e-03,\n",
      "          -1.0450e-02,  1.6065e-02],\n",
      "         [-1.6703e-02, -1.6557e-02,  3.4767e-03,  ..., -4.2885e-03,\n",
      "          -2.1409e-02,  1.4609e-02],\n",
      "         [-1.7503e-02, -1.8750e-02,  4.6151e-03,  ..., -1.0900e-02,\n",
      "          -2.0248e-02,  1.6588e-02]],\n",
      "\n",
      "        [[-8.0450e-04, -3.0386e-03,  5.7585e-03,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [-7.2586e-04, -4.7068e-03,  5.0241e-03,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04],\n",
      "         [ 9.0830e-04, -3.6197e-03,  7.5843e-03,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03]]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>)]\n",
      "self._states is None = True\n",
      "이전 상태가 존재하지 않습니다. new_unsorted_states로 새롭게 만들어 줍니다.\n",
      "STATES: (tensor([[[-5.9120e-03,  8.5314e-03, -1.4924e-03,  ..., -2.1824e-03,\n",
      "           0.0000e+00,  7.6433e-04],\n",
      "         [-7.4715e-03,  8.3881e-03,  2.9731e-04,  ..., -1.7795e-03,\n",
      "           6.9465e-03,  6.8041e-04],\n",
      "         [-5.6458e-03,  9.7251e-03, -2.3428e-04,  ..., -0.0000e+00,\n",
      "           8.9434e-03, -3.2452e-03]],\n",
      "\n",
      "        [[ 2.3629e-03,  0.0000e+00,  1.1207e-03,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [ 0.0000e+00,  3.1858e-03,  1.0247e-03,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04],\n",
      "         [ 2.4202e-03,  0.0000e+00,  2.0514e-03,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00]]], device='cuda:0'), tensor([[[-1.4481e-02, -1.6137e-02,  3.5963e-03,  ..., -4.0869e-03,\n",
      "          -1.0450e-02,  1.6065e-02],\n",
      "         [-1.6703e-02, -1.6557e-02,  3.4767e-03,  ..., -4.2885e-03,\n",
      "          -2.1409e-02,  1.4609e-02],\n",
      "         [-1.7503e-02, -1.8750e-02,  4.6151e-03,  ..., -1.0900e-02,\n",
      "          -2.0248e-02,  1.6588e-02]],\n",
      "\n",
      "        [[-8.0450e-04, -3.0386e-03,  5.7585e-03,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [-7.2586e-04, -4.7068e-03,  5.0241e-03,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04],\n",
      "         [ 9.0830e-04, -3.6197e-03,  7.5843e-03,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(token_embedding, Variable(masks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD!!!!**************\n",
      "batch_size = 3\n",
      "total_sequence_length = 10\n",
      "_EncoderBase.sort_and_run_forward 메서드 실시...\n",
      "\tbatch_size = 3, num_valid = 3\n",
      "\tsequence_lengths = tensor([ 7,  6, 10], device='cuda:0')\n",
      "\t1. sorted_inputs.shape = torch.Size([3, 10, 512])\n",
      "\t2. sorted_sequence_lengths = tensor([10,  7,  6], device='cuda:0')\n",
      "\t3. restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "\t4. sorting_indices = tensor([2, 0, 1], device='cuda:0')\n",
      "\t             sorted_inputs.shape  = torch.Size([3, 10, 512])\n",
      "\tpacked_sequence_input.data.shape  = torch.Size([23, 512])\n",
      "\tpacked_sequence_input.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n",
      "\tself.stateful is True\n",
      "\tstateful is True,\n",
      "\t\tConduct `_get_initial_states`\n",
      "\tRUN `_lstm_forward`... by initial_states\n",
      "\t\tinitial_state is None? False\n",
      "\t\tinitial is not None and it's size equal to forward_layers' length,\n",
      "\t\tthen hidden_states is\n",
      "\t\t A = initial_state[0].split(1, 0) = (tensor([[[-0.0056,  0.0097, -0.0002,  ..., -0.0000,  0.0089, -0.0032],\n",
      "         [-0.0059,  0.0085, -0.0015,  ..., -0.0022,  0.0000,  0.0008],\n",
      "         [-0.0075,  0.0084,  0.0003,  ..., -0.0018,  0.0069,  0.0007]]],\n",
      "       device='cuda:0'), tensor([[[ 2.4202e-03,  0.0000e+00,  2.0514e-03,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00],\n",
      "         [ 2.3629e-03,  0.0000e+00,  1.1207e-03,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [ 0.0000e+00,  3.1858e-03,  1.0247e-03,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04]]], device='cuda:0'))\n",
      "\t\t B = initial_state[1].split(1, 0) = (tensor([[[-0.0175, -0.0187,  0.0046,  ..., -0.0109, -0.0202,  0.0166],\n",
      "         [-0.0145, -0.0161,  0.0036,  ..., -0.0041, -0.0104,  0.0161],\n",
      "         [-0.0167, -0.0166,  0.0035,  ..., -0.0043, -0.0214,  0.0146]]],\n",
      "       device='cuda:0'), tensor([[[ 9.0830e-04, -3.6197e-03,  7.5843e-03,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03],\n",
      "         [-8.0450e-04, -3.0386e-03,  5.7585e-03,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [-7.2586e-04, -4.7068e-03,  5.0241e-03,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04]]], device='cuda:0'))\n",
      "\t\t hidden_states = list(zip(A, B))\n",
      "\t\t               = [(tensor([[[-0.0056,  0.0097, -0.0002,  ..., -0.0000,  0.0089, -0.0032],\n",
      "         [-0.0059,  0.0085, -0.0015,  ..., -0.0022,  0.0000,  0.0008],\n",
      "         [-0.0075,  0.0084,  0.0003,  ..., -0.0018,  0.0069,  0.0007]]],\n",
      "       device='cuda:0'), tensor([[[-0.0175, -0.0187,  0.0046,  ..., -0.0109, -0.0202,  0.0166],\n",
      "         [-0.0145, -0.0161,  0.0036,  ..., -0.0041, -0.0104,  0.0161],\n",
      "         [-0.0167, -0.0166,  0.0035,  ..., -0.0043, -0.0214,  0.0146]]],\n",
      "       device='cuda:0')), (tensor([[[ 2.4202e-03,  0.0000e+00,  2.0514e-03,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00],\n",
      "         [ 2.3629e-03,  0.0000e+00,  1.1207e-03,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [ 0.0000e+00,  3.1858e-03,  1.0247e-03,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04]]], device='cuda:0'), tensor([[[ 9.0830e-04, -3.6197e-03,  7.5843e-03,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03],\n",
      "         [-8.0450e-04, -3.0386e-03,  5.7585e-03,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [-7.2586e-04, -4.7068e-03,  5.0241e-03,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04]]], device='cuda:0'))]\n",
      "\t\tinputs is `PackedSequence`\n",
      "\t\ttype(inputs) = <class 'torch.nn.utils.rnn.PackedSequence'>\n",
      "\t\t\tinputs.data.shape = torch.Size([23, 512])\n",
      "\t\t\tinputs.batch_sizes = tensor([3, 3, 3, 3, 3, 3, 2, 1, 1, 1])\n",
      "\t\t\tinputs.sorted_indices = None\n",
      "\t\t\tinputs.unsorted_indices = None\n",
      "\t\tRestore PAD_char to inputs...\n",
      "\t\t바뀐 inputs의 정보 출력\n",
      "\t\ttype(inputs) = <class 'torch.Tensor'>\n",
      "\t\t\tinputs.shape = torch.Size([3, 10, 512])\n",
      "\t\tbatch_lengths = tensor([10,  7,  6])\n",
      "\t\tAssign forward_output_sequence = backward_output_sequence = inputs\n",
      "\t\tSet final_states, sequqnce_outputs as empty list, []\n",
      "\t\tGet a forward layer and backward layer at layer 1\n",
      "\t\tCaching...: output_sequence to cache both forward and backward\n",
      "\t\tstate is None? False\n",
      "\t\t\tAlright, Set hidden_state/memory_state for both forward and backward\n",
      "\t\t\tstate[0](hidden_state) = tensor([[[-0.0056,  0.0097, -0.0002,  ..., -0.0000,  0.0089, -0.0032],\n",
      "         [-0.0059,  0.0085, -0.0015,  ..., -0.0022,  0.0000,  0.0008],\n",
      "         [-0.0075,  0.0084,  0.0003,  ..., -0.0018,  0.0069,  0.0007]]],\n",
      "       device='cuda:0')\n",
      "\t\t\tstate[1](memory_state) = tensor([[[-0.0175, -0.0187,  0.0046,  ..., -0.0109, -0.0202,  0.0166],\n",
      "         [-0.0145, -0.0161,  0.0036,  ..., -0.0041, -0.0104,  0.0161],\n",
      "         [-0.0167, -0.0166,  0.0035,  ..., -0.0043, -0.0214,  0.0146]]],\n",
      "       device='cuda:0')\n",
      "\t\tRUN forward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? False\n",
      "\t\t\t\tOk, Using `initial_state`, create full_batch_previous memory and state.\n",
      "\t\t\t\t(previous_state) = initial_state[0] = tensor([[[-0.0056,  0.0097, -0.0002,  ...,  0.0010,  0.0012, -0.0051],\n",
      "         [-0.0059,  0.0085, -0.0015,  ...,  0.0023,  0.0019, -0.0056],\n",
      "         [-0.0075,  0.0084,  0.0003,  ...,  0.0040,  0.0011, -0.0061]]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tfull_batch_previous_state = initial_state[0].squeeze(0) = tensor([[-0.0056,  0.0097, -0.0002,  ...,  0.0010,  0.0012, -0.0051],\n",
      "        [-0.0059,  0.0085, -0.0015,  ...,  0.0023,  0.0019, -0.0056],\n",
      "        [-0.0075,  0.0084,  0.0003,  ...,  0.0040,  0.0011, -0.0061]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\t(previous_memory) = initial_state[1] = tensor([[[-0.0175, -0.0187,  0.0046,  ..., -0.0043, -0.0085, -0.0041],\n",
      "         [-0.0145, -0.0161,  0.0036,  ...,  0.0083,  0.0021, -0.0064],\n",
      "         [-0.0167, -0.0166,  0.0035,  ...,  0.0076,  0.0003, -0.0039]]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tfull_batch_previous_memory = initial_state[1].squeeze(0) = tensor([[-0.0175, -0.0187,  0.0046,  ..., -0.0043, -0.0085, -0.0041],\n",
      "        [-0.0145, -0.0161,  0.0036,  ...,  0.0083,  0.0021, -0.0064],\n",
      "        [-0.0167, -0.0166,  0.0035,  ...,  0.0076,  0.0003, -0.0039]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tSet current_length_index... is it forward?? True\n",
      "\t\t\t\tOk, forward!! current_length_index = batch_size - 1 = 2\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[-0.0056,  0.0097, -0.0002,  ...,  0.0010,  0.0012, -0.0051],\n",
      "        [-0.0059,  0.0085, -0.0015,  ...,  0.0023,  0.0019, -0.0056],\n",
      "        [-0.0075,  0.0084,  0.0003,  ...,  0.0040,  0.0011, -0.0061]],\n",
      "       device='cuda:0')\n",
      "binary_mask = tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True, False,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 0 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0175, -0.0187,  0.0046,  ..., -0.0043, -0.0085, -0.0041],\n",
      "        [-0.0145, -0.0161,  0.0036,  ...,  0.0083,  0.0021, -0.0064],\n",
      "        [-0.0167, -0.0166,  0.0035,  ...,  0.0076,  0.0003, -0.0039]],\n",
      "       device='cuda:0')\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0175, -0.0187,  0.0046,  ..., -0.0043, -0.0085, -0.0041],\n",
      "        [-0.0145, -0.0161,  0.0036,  ...,  0.0083,  0.0021, -0.0064],\n",
      "        [-0.0167, -0.0166,  0.0035,  ...,  0.0076,  0.0003, -0.0039]],\n",
      "       device='cuda:0')\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0180, -0.0255,  0.0068,  ..., -0.0016, -0.0069, -0.0037],\n",
      "        [-0.0164, -0.0231,  0.0064,  ...,  0.0079,  0.0008, -0.0055],\n",
      "        [-0.0176, -0.0234,  0.0055,  ...,  0.0073, -0.0011, -0.0035]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0180, -0.0255,  0.0068,  ..., -0.0016, -0.0069, -0.0037],\n",
      "        [-0.0164, -0.0231,  0.0064,  ...,  0.0079,  0.0008, -0.0055],\n",
      "        [-0.0176, -0.0234,  0.0055,  ...,  0.0073, -0.0011, -0.0035]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0078, -0.0260, -0.0205,  ...,  0.0145,  0.0510, -0.0102],\n",
      "        [ 0.0290,  0.0047, -0.0227,  ...,  0.0162,  0.0292, -0.0161],\n",
      "        [ 0.0056, -0.0269, -0.0216,  ..., -0.0044,  0.0573, -0.0453]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0197, -0.0266,  0.0111,  ...,  0.0004, -0.0047, -0.0022],\n",
      "        [-0.0140, -0.0242,  0.0033,  ...,  0.0108,  0.0010, -0.0068],\n",
      "        [-0.0144, -0.0276,  0.0076,  ...,  0.0050,  0.0032, -0.0074]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0197, -0.0266,  0.0111,  ...,  0.0004, -0.0047, -0.0022],\n",
      "        [-0.0140, -0.0242,  0.0033,  ...,  0.0108,  0.0010, -0.0068],\n",
      "        [-0.0144, -0.0276,  0.0076,  ...,  0.0050,  0.0032, -0.0074]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0439, -0.0200, -0.0223,  ...,  0.0114,  0.0445, -0.0566],\n",
      "        [-0.0055, -0.0275, -0.0182,  ...,  0.0095,  0.0294, -0.0096],\n",
      "        [ 0.0261, -0.0046, -0.0160,  ...,  0.0313,  0.0107, -0.0573]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0169, -0.0189,  0.0132,  ..., -0.0051, -0.0059, -0.0007],\n",
      "        [-0.0148, -0.0206,  0.0071,  ...,  0.0102,  0.0061, -0.0057],\n",
      "        [-0.0166, -0.0285,  0.0013,  ...,  0.0040, -0.0042, -0.0140]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0169, -0.0189,  0.0132,  ..., -0.0051, -0.0059, -0.0007],\n",
      "        [-0.0148, -0.0206,  0.0071,  ...,  0.0102,  0.0061, -0.0057],\n",
      "        [-0.0166, -0.0285,  0.0013,  ...,  0.0040, -0.0042, -0.0140]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0195,  0.0024, -0.0233,  ...,  0.0120,  0.0189, -0.0226],\n",
      "        [ 0.0259, -0.0076, -0.0144,  ...,  0.0053,  0.0274, -0.0328],\n",
      "        [ 0.0088, -0.0010, -0.0169,  ...,  0.0276,  0.0261, -0.0068]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0179, -0.0193,  0.0091,  ..., -0.0083, -0.0121, -0.0028],\n",
      "        [-0.0160, -0.0159,  0.0083,  ...,  0.0103,  0.0024, -0.0130],\n",
      "        [-0.0131, -0.0271,  0.0053,  ...,  0.0051, -0.0027, -0.0142]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0179, -0.0193,  0.0091,  ..., -0.0083, -0.0121, -0.0028],\n",
      "        [-0.0160, -0.0159,  0.0083,  ...,  0.0103,  0.0024, -0.0130],\n",
      "        [-0.0131, -0.0271,  0.0053,  ...,  0.0051, -0.0027, -0.0142]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0267,  0.0010, -0.0196,  ...,  0.0005,  0.0283, -0.0203],\n",
      "        [ 0.0260, -0.0238,  0.0250,  ...,  0.0017,  0.0297, -0.0235],\n",
      "        [-0.0157,  0.0032, -0.0359,  ...,  0.0232,  0.0049, -0.0217]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0151, -0.0198,  0.0098,  ..., -0.0047, -0.0071, -0.0034],\n",
      "        [-0.0083, -0.0178,  0.0069,  ...,  0.0130,  0.0030, -0.0149],\n",
      "        [-0.0160, -0.0206,  0.0061,  ...,  0.0131,  0.0012, -0.0121]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0151, -0.0198,  0.0098,  ..., -0.0047, -0.0071, -0.0034],\n",
      "        [-0.0083, -0.0178,  0.0069,  ...,  0.0130,  0.0030, -0.0149],\n",
      "        [-0.0160, -0.0206,  0.0061,  ...,  0.0131,  0.0012, -0.0121]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-6.8748e-03, -1.3605e-02,  7.1700e-03,  ...,  1.2522e-02,\n",
      "          2.0355e-02, -2.4865e-02],\n",
      "        [-2.9532e-03, -1.3722e-02, -2.7846e-02,  ..., -3.4922e-03,\n",
      "          1.9039e-03, -3.1346e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0154, -0.0181,  0.0098,  ...,  0.0024, -0.0035, -0.0062],\n",
      "        [-0.0089, -0.0179,  0.0042,  ...,  0.0155,  0.0035, -0.0158]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0154, -0.0181,  0.0098,  ...,  0.0024, -0.0035, -0.0062],\n",
      "        [-0.0089, -0.0179,  0.0042,  ...,  0.0155,  0.0035, -0.0158]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 7.8088e-03, -2.3229e-02, -1.9114e-02,  ...,  2.5953e-02,\n",
      "          5.1098e-02, -5.5057e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0119, -0.0176,  0.0099,  ...,  0.0035, -0.0032, -0.0007]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0119, -0.0176,  0.0099,  ...,  0.0035, -0.0032, -0.0007]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 3.9943e-02, -6.3473e-04, -2.4333e-02, -2.6465e-02,  3.6282e-02,\n",
      "         -8.1245e-02,  2.0234e-02, -4.7533e-02, -1.4706e-02,  1.8934e-02,\n",
      "          4.9761e-04, -6.6569e-02,  3.0510e-02,  8.3242e-03,  3.4185e-03,\n",
      "          4.2514e-02,  3.6875e-02,  1.1155e-02, -6.4569e-02, -2.6975e-02,\n",
      "          1.4409e-02, -8.3901e-03,  1.4958e-02,  1.4861e-02,  1.6609e-02,\n",
      "          4.6618e-02,  8.1440e-02,  6.3781e-02,  4.3447e-02,  5.9358e-03,\n",
      "         -5.9265e-04,  3.1957e-02, -3.8824e-03,  6.6324e-02,  1.1476e-03,\n",
      "          3.0135e-02,  2.8017e-02, -2.6136e-03, -8.7879e-02,  8.0971e-03,\n",
      "          4.2865e-02,  2.4699e-02, -1.6333e-03,  4.5484e-02,  4.8548e-02,\n",
      "         -2.7261e-02,  4.9371e-02, -1.1094e-02, -6.2017e-02,  6.3433e-02,\n",
      "          1.1362e-02, -3.7070e-02, -2.8188e-02, -3.8195e-02, -5.8539e-02,\n",
      "          1.2449e-02, -2.7576e-03,  3.2095e-02,  7.0868e-03,  7.0801e-03,\n",
      "         -2.9167e-02, -6.1884e-02,  3.3293e-02,  7.4093e-03,  8.6460e-03,\n",
      "          2.1394e-02,  2.8104e-02, -2.3261e-02, -4.2063e-02,  2.1559e-02,\n",
      "         -7.4343e-03, -4.5396e-02, -1.7797e-02, -8.0800e-03, -5.4489e-02,\n",
      "         -7.8084e-03,  5.2327e-03, -1.9146e-02, -4.8832e-02,  2.1798e-02,\n",
      "          9.2141e-03, -3.6777e-02,  6.6118e-03,  3.2340e-02,  1.9615e-02,\n",
      "          2.1029e-02, -2.2645e-02, -2.0520e-02, -8.7323e-02, -4.5272e-02,\n",
      "         -2.7091e-02, -2.3780e-02, -2.8575e-02,  7.5322e-02,  2.6836e-02,\n",
      "         -1.4969e-02,  1.4021e-02,  3.6706e-03, -6.0353e-02, -9.9927e-03,\n",
      "         -7.4608e-02, -6.0827e-03,  4.9744e-02, -1.1646e-02, -1.5283e-02,\n",
      "          3.9851e-02,  1.9263e-02, -2.9827e-02,  3.6977e-02,  4.5320e-02,\n",
      "         -4.4115e-02, -5.5769e-02,  1.1291e-02,  3.4049e-02, -4.0742e-02,\n",
      "         -7.8004e-02, -3.9623e-02, -6.5474e-02, -5.7309e-02,  3.2225e-03,\n",
      "          4.3883e-03,  2.9382e-02,  7.0717e-02,  6.7704e-02, -2.4883e-02,\n",
      "          2.6200e-02, -1.4513e-02,  2.5271e-02,  3.6185e-02, -6.2445e-03,\n",
      "         -1.5884e-02, -5.3081e-03, -5.1873e-03,  4.8760e-02,  4.0506e-02,\n",
      "          4.3785e-02,  1.0459e-02,  5.6056e-02, -4.2227e-03,  4.8874e-02,\n",
      "         -2.1245e-02, -3.1359e-03,  1.8048e-02, -2.3473e-02,  5.5935e-02,\n",
      "         -3.8959e-02,  1.0918e-02, -8.7178e-02,  3.0119e-03, -2.1534e-02,\n",
      "          3.2873e-03,  7.5276e-02, -3.9145e-02, -2.3037e-02,  1.6807e-02,\n",
      "          5.1875e-02, -6.6744e-05, -2.1719e-02,  3.4947e-02, -1.9906e-02,\n",
      "         -4.2533e-02, -1.4287e-02, -1.7697e-02, -3.5453e-02, -2.6380e-02,\n",
      "          3.0872e-03, -3.2254e-02, -5.6165e-02, -1.4399e-02,  6.4918e-04,\n",
      "         -4.0651e-02,  5.3044e-02,  5.6464e-02, -7.1317e-02,  2.5132e-02,\n",
      "         -2.7497e-02, -1.5007e-02, -4.0442e-02,  3.2662e-02, -3.9329e-02,\n",
      "          1.1346e-02,  5.2525e-02,  2.5804e-02,  2.6124e-02, -8.0331e-02,\n",
      "          1.4954e-02,  2.4630e-02,  1.5274e-02, -4.8547e-02,  3.7759e-02,\n",
      "         -2.4851e-02, -6.2086e-02,  1.5069e-02,  5.3204e-02,  1.5821e-02,\n",
      "         -2.3344e-02, -1.3930e-03,  1.4708e-02,  1.1591e-02, -7.3345e-02,\n",
      "         -3.6105e-02, -2.8722e-02,  7.6852e-04, -2.2699e-02, -8.3008e-02,\n",
      "          7.0183e-02, -3.0014e-02, -9.3609e-03, -2.8232e-02,  6.1581e-02,\n",
      "         -2.4002e-02,  1.5176e-02,  5.0153e-02, -9.7399e-04, -5.9783e-02,\n",
      "          3.4617e-02, -2.9192e-03,  2.7485e-02,  6.9439e-02, -1.1364e-02,\n",
      "         -1.3452e-01,  2.2476e-02, -1.4151e-02, -7.1442e-03, -1.2852e-02,\n",
      "         -2.2974e-02,  2.5305e-02,  9.9605e-03,  2.9039e-03, -3.2136e-02,\n",
      "          1.0917e-02,  3.9640e-02, -1.0201e-02, -1.9363e-02, -1.1269e-02,\n",
      "          9.5938e-03,  5.2384e-02, -5.6701e-02, -8.5196e-03,  1.2629e-02,\n",
      "         -4.5141e-02, -2.6792e-02, -1.0696e-02,  2.9739e-02, -5.2543e-02,\n",
      "          4.4625e-03,  3.8448e-02, -1.4611e-03,  7.8076e-02, -1.0361e-02,\n",
      "         -1.6192e-02, -7.5027e-03,  5.8933e-02, -8.2104e-03, -1.0303e-02,\n",
      "         -1.0491e-02, -6.1503e-02,  3.2754e-02, -1.5981e-02, -2.7549e-02,\n",
      "         -5.1394e-02, -3.0974e-02,  4.4842e-02,  1.3567e-02, -4.8374e-03,\n",
      "          4.6250e-02,  4.9839e-02, -1.7988e-02,  3.7572e-02, -4.8327e-02,\n",
      "         -1.2389e-01,  9.6038e-03,  2.2187e-02,  1.4403e-02,  4.4363e-03,\n",
      "         -7.8948e-03, -3.4436e-02, -5.8862e-02, -3.2297e-02,  2.4687e-03,\n",
      "          1.3516e-02, -1.9046e-02,  6.2428e-02, -2.4486e-02,  2.5915e-02,\n",
      "         -4.7463e-02,  4.1579e-02, -1.3121e-02, -2.5279e-02, -1.6883e-02,\n",
      "         -1.2437e-02, -6.8095e-02,  3.2792e-02, -3.9081e-02,  2.3718e-02,\n",
      "         -7.6230e-03,  2.5899e-02, -9.0227e-03, -4.4538e-02,  1.8885e-02,\n",
      "          6.7445e-03,  2.3967e-02,  3.6491e-02,  6.1667e-02,  2.9088e-02,\n",
      "         -4.6740e-02,  4.2268e-02,  5.3146e-03,  3.4716e-03,  1.3579e-02,\n",
      "         -3.4293e-02, -7.6553e-03, -3.0861e-02,  2.1517e-02, -1.5814e-02,\n",
      "          1.6505e-02,  2.4900e-02,  3.6272e-02, -1.1502e-02,  2.1404e-02,\n",
      "         -1.9012e-02,  4.1999e-02, -2.3130e-02, -1.2259e-03, -3.5337e-02,\n",
      "         -6.3823e-02, -1.7515e-02, -4.7187e-02, -2.1176e-02,  2.9516e-02,\n",
      "         -5.6702e-02, -3.0155e-02, -4.3931e-02,  5.9581e-02, -4.8076e-02,\n",
      "          3.2992e-02,  5.8642e-02,  4.0638e-02, -4.9440e-03, -2.6222e-02,\n",
      "         -1.7771e-02, -4.0846e-02,  2.8135e-02, -4.8641e-02, -3.6589e-02,\n",
      "         -4.3364e-03, -4.0931e-02,  1.2174e-02, -2.9099e-02, -3.0528e-02,\n",
      "          1.0307e-02,  6.1238e-02, -5.8316e-02, -1.3105e-02,  9.0719e-03,\n",
      "         -1.7113e-02, -1.6299e-02, -4.1770e-02,  5.2599e-02, -2.1581e-02,\n",
      "          1.3731e-02,  3.1022e-02, -9.9482e-03,  2.3787e-02, -7.2789e-02,\n",
      "          3.9918e-03, -1.1121e-01,  6.5336e-02, -3.4317e-02,  6.3555e-02,\n",
      "          4.7264e-02, -2.1690e-02,  3.2510e-02, -2.9224e-03,  8.1941e-03,\n",
      "          2.2115e-03,  4.4054e-02, -2.8647e-02, -1.7062e-02,  3.1700e-02,\n",
      "          1.3082e-02, -7.8945e-03,  5.9706e-02, -3.6144e-03,  1.2505e-02,\n",
      "          3.5541e-02,  5.5393e-02,  7.4438e-03,  3.9664e-02, -2.1253e-02,\n",
      "         -6.8311e-03, -4.3957e-02,  3.7919e-02, -2.8258e-02, -8.0768e-03,\n",
      "         -4.2647e-02,  3.2717e-02, -2.9899e-02, -9.3153e-03,  1.5475e-02,\n",
      "          3.0740e-02,  1.8109e-02,  2.8607e-02, -3.8829e-02, -4.7982e-02,\n",
      "         -7.1243e-03, -2.4547e-02,  8.3619e-04, -1.8965e-02,  1.3804e-02,\n",
      "          1.0712e-02, -3.4088e-02, -7.9899e-02, -7.0846e-03,  6.9593e-03,\n",
      "         -2.1685e-02,  5.5655e-02, -1.2060e-02,  2.3431e-03,  7.2531e-02,\n",
      "         -6.0682e-02, -7.4575e-02, -1.7317e-02,  5.8419e-03, -2.3470e-02,\n",
      "          9.9523e-03, -6.2580e-02,  7.6685e-02,  2.9135e-02, -3.8954e-02,\n",
      "          1.4604e-02,  3.1968e-03,  3.2509e-02, -2.8317e-03,  2.2350e-02,\n",
      "         -4.7101e-02, -5.0960e-02,  3.1412e-03, -1.5378e-02, -2.6794e-02,\n",
      "         -1.9578e-02,  1.0552e-02, -6.9772e-02,  2.0636e-02, -2.9148e-03,\n",
      "         -4.0716e-02,  2.5176e-03,  7.4695e-03, -3.4024e-04,  3.2252e-02,\n",
      "         -2.8218e-02, -1.9674e-02, -2.1514e-02, -9.1328e-03,  2.0247e-02,\n",
      "          3.0113e-02, -7.1527e-02,  4.6293e-02,  2.1005e-02, -2.0541e-02,\n",
      "          4.0748e-02,  4.6423e-03, -2.4288e-02, -1.5478e-02, -3.5585e-02,\n",
      "          2.1604e-02,  3.8710e-02,  1.4737e-02, -3.1023e-02, -5.9019e-02,\n",
      "          3.0318e-02,  3.5039e-02, -1.1337e-03,  3.8222e-02,  1.5961e-02,\n",
      "          2.4988e-02,  1.0752e-02,  7.9247e-02,  3.4722e-02, -7.4136e-02,\n",
      "          3.9050e-02, -1.8820e-02,  6.0536e-02,  6.0405e-03, -1.8136e-02,\n",
      "          4.7733e-02,  4.5106e-03, -5.9822e-03, -1.5836e-02,  1.4583e-03,\n",
      "         -2.1234e-02, -2.3092e-02, -6.0511e-03,  7.6471e-02, -1.0707e-02,\n",
      "         -3.3524e-02,  3.2486e-02, -7.2459e-03, -6.4545e-02, -4.4839e-02,\n",
      "          3.7363e-02, -2.0615e-02, -1.1821e-01, -2.0830e-02,  6.9055e-02,\n",
      "         -1.0918e-02,  5.4751e-02,  2.7437e-02,  3.1381e-02,  1.9435e-02,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          5.1162e-02, -3.3509e-02]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0100, -0.0158,  0.0079,  ...,  0.0016, -0.0057, -0.0063]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0100, -0.0158,  0.0079,  ...,  0.0016, -0.0057, -0.0063]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0225,  0.0021, -0.0073, -0.0453,  0.0067, -0.1035,  0.0007, -0.0384,\n",
      "         -0.0241, -0.0236, -0.0056, -0.0469,  0.0058, -0.0028,  0.0082,  0.0394,\n",
      "          0.0411,  0.0036, -0.0933, -0.0109,  0.0350, -0.0014,  0.0195,  0.0090,\n",
      "          0.0102,  0.0144,  0.0910,  0.0898,  0.0680, -0.0267,  0.0296,  0.0123,\n",
      "         -0.0376,  0.0462,  0.0312, -0.0027,  0.0219, -0.0156, -0.0906, -0.0108,\n",
      "          0.0491, -0.0022, -0.0330,  0.0454,  0.0668, -0.0107,  0.0695, -0.0285,\n",
      "         -0.1164,  0.0746,  0.0207,  0.0046, -0.0736, -0.0499, -0.0580, -0.0040,\n",
      "          0.0032,  0.0278,  0.0149, -0.0144,  0.0139, -0.0536,  0.0244,  0.0268,\n",
      "         -0.0166, -0.0146, -0.0104, -0.0225, -0.0341,  0.0614, -0.0234, -0.0311,\n",
      "         -0.0392,  0.0299, -0.0474, -0.0211, -0.0228, -0.0282, -0.0288, -0.0021,\n",
      "         -0.0283, -0.0399,  0.0053,  0.0269, -0.0246,  0.0394, -0.0482, -0.0131,\n",
      "         -0.0672, -0.0286, -0.0382, -0.0050, -0.0136,  0.0675,  0.0515,  0.0383,\n",
      "          0.0347, -0.0290, -0.0956,  0.0055, -0.0542,  0.0374,  0.0811, -0.0625,\n",
      "          0.0048,  0.0439,  0.0274, -0.0130,  0.0037,  0.0702, -0.0189, -0.0653,\n",
      "          0.0338,  0.0484, -0.0758, -0.1118, -0.0982, -0.0586, -0.1017,  0.0323,\n",
      "         -0.0187,  0.0306,  0.0579,  0.0252, -0.0435,  0.0251, -0.0279,  0.0555,\n",
      "          0.0822,  0.0015,  0.0172,  0.0274, -0.0081,  0.0368,  0.0218,  0.0329,\n",
      "          0.0185,  0.0088, -0.0144,  0.0301,  0.0175, -0.0075,  0.0198,  0.0053,\n",
      "          0.0708,  0.0166, -0.0065, -0.0641, -0.0199, -0.0439,  0.0324,  0.0846,\n",
      "         -0.0156, -0.0623,  0.0368,  0.0376, -0.0219, -0.0321,  0.0116, -0.0214,\n",
      "         -0.0275, -0.0293, -0.0180, -0.0346, -0.0491,  0.0416, -0.0381, -0.0820,\n",
      "         -0.0298,  0.0151, -0.0111,  0.0488,  0.0482, -0.0697,  0.0230, -0.0346,\n",
      "         -0.0385, -0.0320,  0.0022, -0.0195,  0.0382,  0.0669,  0.0117, -0.0143,\n",
      "         -0.0986,  0.0230, -0.0114,  0.0136, -0.0391,  0.0525, -0.0245, -0.0297,\n",
      "         -0.0252,  0.0744,  0.0637,  0.0055,  0.0261,  0.0363, -0.0025, -0.0683,\n",
      "         -0.0255, -0.0456,  0.0402, -0.0391, -0.0926,  0.0581, -0.0183, -0.0362,\n",
      "         -0.0535,  0.0409, -0.0179,  0.0289,  0.0655,  0.0294, -0.0775,  0.0396,\n",
      "          0.0239,  0.0403,  0.0801, -0.0544, -0.1100, -0.0068, -0.0387, -0.0217,\n",
      "          0.0214, -0.0616, -0.0089, -0.0123,  0.0534, -0.0370,  0.0211,  0.0167,\n",
      "         -0.0111, -0.0011,  0.0025,  0.0404,  0.0642, -0.0452, -0.0113,  0.0538,\n",
      "         -0.0520, -0.0056,  0.0366,  0.0759, -0.0529, -0.0374,  0.0588,  0.0106,\n",
      "          0.1057, -0.0457, -0.0078,  0.0133,  0.0720, -0.0095, -0.0401,  0.0218,\n",
      "         -0.0456,  0.0502,  0.0087, -0.0067, -0.0664, -0.0569,  0.0319,  0.0456,\n",
      "          0.0117,  0.0440,  0.0148, -0.0104,  0.0672, -0.0583, -0.1598,  0.0023,\n",
      "          0.0538, -0.0143,  0.0224,  0.0161, -0.0428, -0.0923, -0.0291, -0.0308,\n",
      "         -0.0071, -0.0245,  0.0439, -0.0270,  0.0261, -0.0623,  0.0007, -0.0537,\n",
      "         -0.0007, -0.0388,  0.0227, -0.0308,  0.0428, -0.0017,  0.0260, -0.0640,\n",
      "         -0.0064, -0.0349, -0.0219,  0.0544,  0.0239,  0.0471,  0.0232,  0.0261,\n",
      "          0.0282, -0.0696,  0.0527,  0.0118,  0.0263, -0.0006, -0.0661,  0.0209,\n",
      "          0.0058,  0.0030, -0.0387,  0.0064,  0.0166,  0.0501, -0.0120,  0.0489,\n",
      "          0.0016,  0.0325, -0.0278,  0.0218,  0.0103, -0.0845, -0.0082, -0.0481,\n",
      "         -0.0449,  0.0369, -0.0878, -0.0075, -0.0770,  0.0889, -0.0595,  0.0261,\n",
      "          0.0722,  0.0245, -0.0125, -0.0249, -0.0267, -0.0498, -0.0126, -0.0515,\n",
      "         -0.0232, -0.0248, -0.0696,  0.0107, -0.0039,  0.0008,  0.0189,  0.0703,\n",
      "         -0.0633,  0.0129,  0.0091, -0.0232,  0.0251, -0.0279,  0.1155, -0.0159,\n",
      "         -0.0343,  0.0638, -0.0056,  0.0369, -0.0636, -0.0179, -0.1186,  0.0721,\n",
      "         -0.0307,  0.0792,  0.0270,  0.0300,  0.0747,  0.0376,  0.0251,  0.0128,\n",
      "          0.0466, -0.0187, -0.0412,  0.0287,  0.0269, -0.0115,  0.0786,  0.0066,\n",
      "         -0.0067,  0.0407,  0.0497,  0.0495,  0.0253, -0.0395, -0.0038, -0.0613,\n",
      "          0.0059, -0.0225,  0.0075,  0.0116,  0.0301, -0.0510,  0.0483, -0.0205,\n",
      "          0.0458,  0.0350,  0.0831, -0.0208, -0.0651,  0.0033, -0.0349,  0.0098,\n",
      "         -0.0437,  0.0113, -0.0011, -0.0237, -0.0102,  0.0026,  0.0296, -0.0294,\n",
      "          0.0504,  0.0353, -0.0013,  0.0868, -0.0338, -0.1060, -0.0469, -0.0076,\n",
      "         -0.0594, -0.0139, -0.1025,  0.0495,  0.0576, -0.0711,  0.0156, -0.0270,\n",
      "          0.0126,  0.0289,  0.0422, -0.0147, -0.0343,  0.0242, -0.0238,  0.0042,\n",
      "         -0.0163, -0.0254, -0.0318, -0.0033, -0.0148, -0.0554, -0.0152,  0.0177,\n",
      "         -0.0006,  0.0078,  0.0026, -0.0279, -0.0466, -0.0133, -0.0026,  0.0167,\n",
      "         -0.0890,  0.0875, -0.0195, -0.0232,  0.0426, -0.0253, -0.0016, -0.0328,\n",
      "          0.0101,  0.0372,  0.0486, -0.0026, -0.0226, -0.0536,  0.0217,  0.0102,\n",
      "         -0.0430,  0.0425,  0.0180, -0.0253,  0.0030,  0.0580,  0.0355, -0.0872,\n",
      "          0.0397, -0.0145,  0.0678,  0.0079, -0.0342,  0.0471,  0.0262,  0.0146,\n",
      "         -0.0287, -0.0009, -0.0416,  0.0084, -0.0227,  0.0526, -0.0619, -0.0201,\n",
      "          0.0613,  0.0342, -0.0878, -0.0522,  0.0243, -0.0385, -0.0821, -0.0479,\n",
      "          0.0745, -0.0005,  0.0558,  0.0333, -0.0184,  0.0317,  0.0303, -0.0447]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 9 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0142, -0.0201,  0.0066,  ..., -0.0049, -0.0103, -0.0146]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0142, -0.0201,  0.0066,  ..., -0.0049, -0.0103, -0.0146]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 4.0999e-03, -6.1420e-03, -4.0220e-03, -4.2812e-02,  2.2545e-03,\n",
      "         -5.5843e-02, -3.3867e-03, -2.4588e-02, -5.0965e-02,  5.2278e-03,\n",
      "          3.2208e-03, -6.6673e-02, -4.4035e-03, -3.9993e-02, -1.1946e-02,\n",
      "          5.1611e-02,  9.1874e-03, -3.4062e-03, -6.2161e-02, -1.2382e-02,\n",
      "          1.7805e-02,  3.6729e-03,  4.2529e-03,  1.7206e-02, -1.2278e-02,\n",
      "          1.4176e-02,  6.9620e-02,  5.3863e-02,  2.9146e-02,  1.3687e-04,\n",
      "          4.2660e-03,  2.6004e-02, -2.9728e-04,  2.5874e-02,  5.3049e-04,\n",
      "         -9.1188e-03, -2.0923e-03, -3.1675e-02, -6.1669e-02,  2.7857e-02,\n",
      "          3.7919e-02,  4.7040e-02, -2.9004e-02,  2.8061e-02,  2.2440e-02,\n",
      "         -1.7007e-02,  4.6762e-02, -9.9050e-03, -6.0391e-02,  4.4008e-02,\n",
      "          2.7127e-02, -1.0071e-02, -4.4653e-02, -3.4113e-02, -5.4936e-02,\n",
      "         -2.7576e-02, -1.1772e-02,  1.6111e-02, -2.4791e-02, -1.2392e-02,\n",
      "         -4.5851e-02, -7.0682e-02,  6.8990e-02,  1.5804e-02, -1.8523e-02,\n",
      "          1.7020e-02,  3.7107e-03, -3.7832e-02, -9.7512e-03,  2.5285e-02,\n",
      "         -1.8639e-02, -1.1800e-02, -3.4856e-02,  2.3652e-03, -4.5387e-02,\n",
      "         -2.9169e-02, -7.3532e-03, -5.6582e-02, -4.0548e-02,  2.6095e-02,\n",
      "         -4.1448e-03, -7.7175e-03, -4.9508e-03,  5.8557e-02,  7.4085e-03,\n",
      "          2.0493e-02, -2.6433e-02, -5.9150e-02, -8.5825e-02, -3.6332e-02,\n",
      "         -6.7253e-02, -1.8667e-02,  1.9783e-02,  4.1228e-02,  3.5272e-02,\n",
      "         -1.5310e-02,  2.6563e-02,  9.0099e-03, -5.8591e-02, -1.2875e-02,\n",
      "         -4.5589e-02,  3.7654e-02,  5.1700e-02, -9.7946e-03, -2.8365e-02,\n",
      "          3.8642e-02,  3.2772e-02, -7.0115e-03,  1.7229e-02,  4.8634e-02,\n",
      "         -4.3900e-02, -5.7408e-02,  4.0625e-02,  4.4493e-02, -5.2082e-02,\n",
      "         -7.4520e-02, -5.9377e-02, -6.7939e-02, -7.3847e-02,  1.3183e-02,\n",
      "         -4.9373e-03,  1.8830e-02,  9.7497e-02,  1.3256e-02,  1.6883e-03,\n",
      "          1.4859e-02, -2.3568e-03,  5.6624e-02,  5.3152e-02, -1.2295e-02,\n",
      "          1.2090e-02, -1.3728e-05,  1.0096e-02,  3.3864e-02,  4.2704e-02,\n",
      "          5.4648e-02,  4.1818e-03,  3.7166e-03, -1.5428e-02,  4.5995e-02,\n",
      "          2.1937e-02, -2.8221e-02,  1.6410e-02, -4.7979e-04,  4.1428e-02,\n",
      "         -2.7716e-02,  1.6255e-03, -6.3316e-02, -1.8258e-02, -4.5491e-02,\n",
      "          1.1397e-02,  7.0264e-02,  1.8543e-02, -4.3941e-02,  1.5809e-02,\n",
      "          1.0576e-02, -1.1876e-02,  1.1684e-02, -1.4857e-03, -1.3847e-02,\n",
      "         -3.5052e-03, -2.9399e-02, -3.4140e-02, -3.0366e-02, -4.2807e-02,\n",
      "          2.2627e-02,  9.7901e-06, -3.6235e-02, -5.8573e-03,  2.0378e-02,\n",
      "         -6.4172e-02,  3.1886e-02,  1.5496e-02, -2.8181e-02,  3.5096e-02,\n",
      "         -6.8952e-03, -2.5324e-02, -3.1501e-02,  7.9062e-03, -2.8681e-02,\n",
      "         -7.9037e-03,  3.5255e-02,  2.4252e-02,  2.9042e-02, -6.2465e-02,\n",
      "          1.4517e-02, -1.9319e-02, -2.7564e-02, -2.5307e-02,  3.7366e-02,\n",
      "         -3.2373e-02, -6.7099e-02,  1.8414e-02,  2.4672e-02,  1.2600e-02,\n",
      "         -5.6666e-03,  2.5549e-03,  4.0709e-03,  1.9000e-02, -6.1543e-02,\n",
      "          1.2990e-02, -2.5736e-02,  1.1150e-02, -2.5157e-02, -8.1259e-02,\n",
      "          2.5197e-02,  1.6732e-03,  2.1115e-02, -4.1658e-02,  6.6255e-02,\n",
      "         -2.5587e-02, -3.2157e-04,  1.2388e-02,  4.0023e-02, -6.1048e-02,\n",
      "          6.0717e-02,  2.1476e-02,  5.8257e-03,  4.3190e-02, -5.3267e-03,\n",
      "         -1.1518e-01,  5.6670e-03, -2.2334e-02, -2.0324e-02,  1.9542e-03,\n",
      "         -3.7446e-02,  1.5559e-02, -7.3694e-03,  2.4515e-02, -4.5967e-02,\n",
      "         -3.7894e-03,  3.2286e-02,  1.0811e-02,  4.4440e-03,  4.8463e-03,\n",
      "          2.5042e-02,  4.7359e-02, -6.8103e-02, -6.5093e-03,  7.0785e-02,\n",
      "         -3.5528e-02,  4.7189e-03, -1.0189e-02,  4.2352e-02, -2.0553e-02,\n",
      "         -1.7234e-02,  1.3394e-02,  1.8244e-02,  6.2473e-02, -4.8055e-02,\n",
      "          1.2645e-02, -7.0589e-03,  5.6752e-02, -3.4373e-02, -3.1496e-02,\n",
      "         -5.9777e-03, -2.4881e-02,  7.9201e-02,  1.3747e-02, -2.5393e-03,\n",
      "         -3.2503e-02, -3.1110e-02,  5.7813e-02, -4.5621e-03, -1.0531e-02,\n",
      "          4.2530e-02,  3.0264e-02, -6.1265e-03,  4.5427e-02, -5.1061e-02,\n",
      "         -1.5275e-01,  1.0616e-02,  2.6280e-02,  1.8947e-02,  3.2359e-02,\n",
      "         -1.4984e-02,  3.7450e-03, -6.5668e-02,  2.4765e-03, -3.2025e-02,\n",
      "          2.9947e-02,  4.5328e-03,  3.0595e-02, -5.2844e-03,  7.7454e-02,\n",
      "         -1.9671e-02,  2.9783e-02, -3.6750e-02, -1.6449e-02,  6.7979e-03,\n",
      "          6.7633e-03, -3.4163e-02,  2.9027e-02, -1.8871e-02,  3.0973e-02,\n",
      "         -3.5600e-02, -2.3323e-02, -2.5930e-02, -4.0542e-02,  2.5499e-02,\n",
      "          3.3001e-02,  1.2359e-02,  2.5706e-02,  3.0320e-02,  4.6401e-02,\n",
      "         -2.3879e-02,  2.0202e-02, -2.2375e-03,  8.2037e-03, -2.0483e-02,\n",
      "         -7.8328e-02, -1.1248e-02, -9.1449e-03,  7.5573e-03, -2.2487e-02,\n",
      "         -1.6329e-02,  1.8263e-02,  3.7522e-02,  1.9167e-02,  1.4377e-02,\n",
      "         -1.4204e-02,  3.2154e-02, -4.2945e-02,  5.7657e-03, -8.0327e-04,\n",
      "         -6.6083e-02, -2.8194e-04, -1.8378e-02, -4.8475e-02,  2.3617e-02,\n",
      "         -3.3750e-02,  1.1834e-02, -1.0890e-02,  2.5747e-02, -3.7289e-02,\n",
      "          5.2489e-02,  3.3673e-02,  2.8601e-02,  4.2754e-03, -4.7038e-02,\n",
      "         -3.0422e-02, -1.7425e-02,  1.3590e-02, -5.1289e-02, -2.6229e-02,\n",
      "         -1.0611e-02, -7.7437e-03,  2.7641e-02, -4.1801e-03,  2.6006e-02,\n",
      "         -3.3880e-03,  3.3067e-03, -5.1195e-02,  1.0351e-02, -5.3868e-03,\n",
      "         -3.0990e-02,  2.9896e-02, -2.5370e-02,  7.7419e-02, -1.1452e-02,\n",
      "         -2.1355e-02,  2.0652e-02,  5.2692e-02,  3.8496e-02, -5.2882e-02,\n",
      "          2.0792e-03, -8.6128e-02,  3.2290e-02, -1.0719e-02,  1.2896e-02,\n",
      "          3.0134e-02,  1.5601e-02,  3.8173e-02,  3.0548e-02,  4.3263e-02,\n",
      "          3.2726e-02,  1.9825e-02, -1.0034e-02, -2.1873e-02,  1.3308e-02,\n",
      "          3.9475e-02, -1.2140e-02,  8.5712e-02,  4.2187e-02, -2.3477e-02,\n",
      "         -1.9578e-02,  3.7402e-02,  1.4542e-03,  3.4245e-02, -1.5425e-02,\n",
      "         -2.8997e-03, -3.4361e-02,  3.4428e-02, -3.5852e-02,  1.1834e-02,\n",
      "         -3.3657e-03, -3.2083e-04, -5.2636e-02,  3.2873e-02, -6.1226e-04,\n",
      "          2.0334e-02,  3.3753e-02,  5.1543e-02,  4.3593e-03, -4.9230e-02,\n",
      "         -1.4788e-02, -2.5702e-02, -9.4918e-04, -9.5153e-03,  9.0001e-03,\n",
      "          2.0273e-02, -2.8655e-02, -2.4118e-02,  2.3451e-02,  2.7414e-02,\n",
      "         -3.3920e-02,  5.2001e-02,  3.6443e-02, -6.0504e-03,  7.4200e-02,\n",
      "         -5.9112e-02, -5.3881e-02, -1.7870e-02,  7.4283e-03, -2.7031e-02,\n",
      "         -1.5785e-02, -6.3814e-02,  4.7019e-02,  1.4489e-02, -3.2238e-02,\n",
      "          5.6374e-03, -6.0349e-03,  6.9934e-03,  2.6128e-02,  1.4643e-02,\n",
      "          1.8811e-03,  1.8494e-02, -9.8517e-04, -2.6079e-02, -2.9486e-02,\n",
      "         -7.7439e-02, -1.8391e-02, -4.0349e-02,  8.9021e-03,  2.3187e-02,\n",
      "         -5.5234e-02, -2.4531e-03,  7.9741e-03,  8.4657e-03,  4.5370e-03,\n",
      "         -2.5644e-02,  2.0494e-03, -3.9706e-02, -5.1241e-02,  1.2272e-02,\n",
      "         -8.8885e-03, -6.7159e-02,  5.1499e-02,  1.8548e-02, -5.7593e-02,\n",
      "          4.5964e-02,  2.3069e-02, -1.5816e-02, -1.7899e-02, -2.6860e-02,\n",
      "          3.9363e-02,  4.0149e-02,  3.6070e-03,  9.7411e-03, -8.0640e-02,\n",
      "          5.5356e-02,  1.6593e-02, -3.2203e-03,  7.8833e-02,  4.0068e-02,\n",
      "          4.2472e-02, -1.1705e-02,  7.8580e-02,  3.7629e-02, -6.7491e-02,\n",
      "          1.6742e-02, -1.4671e-02,  4.7917e-02, -2.8045e-02, -2.8992e-02,\n",
      "          1.3161e-03, -6.2237e-03, -1.4460e-02, -1.6555e-03,  1.5189e-02,\n",
      "          1.8578e-02, -3.5313e-02, -9.5090e-05,  7.6177e-02,  2.5247e-03,\n",
      "         -3.2401e-02,  2.0006e-02,  1.6701e-02, -4.7967e-02, -4.5037e-02,\n",
      "          2.0186e-02, -3.6078e-03, -8.5892e-02, -3.5775e-02,  4.8689e-02,\n",
      "          4.3603e-03,  6.5434e-02,  1.7247e-02, -1.3526e-02,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[-0.0056,  0.0105,  0.0005,  ...,  0.0014,  0.0006, -0.0060],\n",
      "         [-0.0061,  0.0091, -0.0014,  ...,  0.0037,  0.0020, -0.0077],\n",
      "         [-0.0079,  0.0090,  0.0009,  ...,  0.0057,  0.0018, -0.0071]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>), tensor([[[-0.0169, -0.0180,  0.0054,  ..., -0.0026, -0.0084, -0.0070],\n",
      "         [-0.0129, -0.0160,  0.0032,  ...,  0.0134,  0.0013, -0.0074],\n",
      "         [-0.0185, -0.0187,  0.0045,  ...,  0.0110, -0.0010, -0.0041]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tRUN backward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? False\n",
      "\t\t\t\tOk, Using `initial_state`, create full_batch_previous memory and state.\n",
      "\t\t\t\t(previous_state) = initial_state[0] = tensor([[[ 0.0071,  0.0052, -0.0002,  ..., -0.0000,  0.0089, -0.0032],\n",
      "         [ 0.0088,  0.0000, -0.0008,  ..., -0.0022,  0.0000,  0.0008],\n",
      "         [ 0.0075,  0.0055, -0.0006,  ..., -0.0018,  0.0069,  0.0007]]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tfull_batch_previous_state = initial_state[0].squeeze(0) = tensor([[ 0.0071,  0.0052, -0.0002,  ..., -0.0000,  0.0089, -0.0032],\n",
      "        [ 0.0088,  0.0000, -0.0008,  ..., -0.0022,  0.0000,  0.0008],\n",
      "        [ 0.0075,  0.0055, -0.0006,  ..., -0.0018,  0.0069,  0.0007]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\t(previous_memory) = initial_state[1] = tensor([[[-0.0288, -0.0241,  0.0114,  ..., -0.0109, -0.0202,  0.0166],\n",
      "         [-0.0257, -0.0202,  0.0130,  ..., -0.0041, -0.0104,  0.0161],\n",
      "         [-0.0304, -0.0200,  0.0124,  ..., -0.0043, -0.0214,  0.0146]]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tfull_batch_previous_memory = initial_state[1].squeeze(0) = tensor([[-0.0288, -0.0241,  0.0114,  ..., -0.0109, -0.0202,  0.0166],\n",
      "        [-0.0257, -0.0202,  0.0130,  ..., -0.0041, -0.0104,  0.0161],\n",
      "        [-0.0304, -0.0200,  0.0124,  ..., -0.0043, -0.0214,  0.0146]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tSet current_length_index... is it forward?? False\n",
      "\t\t\t\tOops, backward!! current_length_index = 0\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[ 0.0071,  0.0052, -0.0002,  ..., -0.0000,  0.0089, -0.0032],\n",
      "        [ 0.0088,  0.0000, -0.0008,  ..., -0.0022,  0.0000,  0.0008],\n",
      "        [ 0.0075,  0.0055, -0.0006,  ..., -0.0018,  0.0069,  0.0007]],\n",
      "       device='cuda:0')\n",
      "binary_mask = tensor([[ True,  True,  True,  ...,  True, False,  True],\n",
      "        [ True,  True,  True,  ..., False, False,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 0.0000, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 9 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0288, -0.0241,  0.0114,  ..., -0.0109, -0.0202,  0.0166]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0288, -0.0241,  0.0114,  ..., -0.0109, -0.0202,  0.0166]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 4.0999e-03, -6.1420e-03, -4.0220e-03, -4.2812e-02,  2.2545e-03,\n",
      "         -5.5843e-02, -3.3867e-03, -2.4588e-02, -5.0965e-02,  5.2278e-03,\n",
      "          3.2208e-03, -6.6673e-02, -4.4035e-03, -3.9993e-02, -1.1946e-02,\n",
      "          5.1611e-02,  9.1874e-03, -3.4062e-03, -6.2161e-02, -1.2382e-02,\n",
      "          1.7805e-02,  3.6729e-03,  4.2529e-03,  1.7206e-02, -1.2278e-02,\n",
      "          1.4176e-02,  6.9620e-02,  5.3863e-02,  2.9146e-02,  1.3687e-04,\n",
      "          4.2660e-03,  2.6004e-02, -2.9728e-04,  2.5874e-02,  5.3049e-04,\n",
      "         -9.1188e-03, -2.0923e-03, -3.1675e-02, -6.1669e-02,  2.7857e-02,\n",
      "          3.7919e-02,  4.7040e-02, -2.9004e-02,  2.8061e-02,  2.2440e-02,\n",
      "         -1.7007e-02,  4.6762e-02, -9.9050e-03, -6.0391e-02,  4.4008e-02,\n",
      "          2.7127e-02, -1.0071e-02, -4.4653e-02, -3.4113e-02, -5.4936e-02,\n",
      "         -2.7576e-02, -1.1772e-02,  1.6111e-02, -2.4791e-02, -1.2392e-02,\n",
      "         -4.5851e-02, -7.0682e-02,  6.8990e-02,  1.5804e-02, -1.8523e-02,\n",
      "          1.7020e-02,  3.7107e-03, -3.7832e-02, -9.7512e-03,  2.5285e-02,\n",
      "         -1.8639e-02, -1.1800e-02, -3.4856e-02,  2.3652e-03, -4.5387e-02,\n",
      "         -2.9169e-02, -7.3532e-03, -5.6582e-02, -4.0548e-02,  2.6095e-02,\n",
      "         -4.1448e-03, -7.7175e-03, -4.9508e-03,  5.8557e-02,  7.4085e-03,\n",
      "          2.0493e-02, -2.6433e-02, -5.9150e-02, -8.5825e-02, -3.6332e-02,\n",
      "         -6.7253e-02, -1.8667e-02,  1.9783e-02,  4.1228e-02,  3.5272e-02,\n",
      "         -1.5310e-02,  2.6563e-02,  9.0099e-03, -5.8591e-02, -1.2875e-02,\n",
      "         -4.5589e-02,  3.7654e-02,  5.1700e-02, -9.7946e-03, -2.8365e-02,\n",
      "          3.8642e-02,  3.2772e-02, -7.0115e-03,  1.7229e-02,  4.8634e-02,\n",
      "         -4.3900e-02, -5.7408e-02,  4.0625e-02,  4.4493e-02, -5.2082e-02,\n",
      "         -7.4520e-02, -5.9377e-02, -6.7939e-02, -7.3847e-02,  1.3183e-02,\n",
      "         -4.9373e-03,  1.8830e-02,  9.7497e-02,  1.3256e-02,  1.6883e-03,\n",
      "          1.4859e-02, -2.3568e-03,  5.6624e-02,  5.3152e-02, -1.2295e-02,\n",
      "          1.2090e-02, -1.3728e-05,  1.0096e-02,  3.3864e-02,  4.2704e-02,\n",
      "          5.4648e-02,  4.1818e-03,  3.7166e-03, -1.5428e-02,  4.5995e-02,\n",
      "          2.1937e-02, -2.8221e-02,  1.6410e-02, -4.7979e-04,  4.1428e-02,\n",
      "         -2.7716e-02,  1.6255e-03, -6.3316e-02, -1.8258e-02, -4.5491e-02,\n",
      "          1.1397e-02,  7.0264e-02,  1.8543e-02, -4.3941e-02,  1.5809e-02,\n",
      "          1.0576e-02, -1.1876e-02,  1.1684e-02, -1.4857e-03, -1.3847e-02,\n",
      "         -3.5052e-03, -2.9399e-02, -3.4140e-02, -3.0366e-02, -4.2807e-02,\n",
      "          2.2627e-02,  9.7901e-06, -3.6235e-02, -5.8573e-03,  2.0378e-02,\n",
      "         -6.4172e-02,  3.1886e-02,  1.5496e-02, -2.8181e-02,  3.5096e-02,\n",
      "         -6.8952e-03, -2.5324e-02, -3.1501e-02,  7.9062e-03, -2.8681e-02,\n",
      "         -7.9037e-03,  3.5255e-02,  2.4252e-02,  2.9042e-02, -6.2465e-02,\n",
      "          1.4517e-02, -1.9319e-02, -2.7564e-02, -2.5307e-02,  3.7366e-02,\n",
      "         -3.2373e-02, -6.7099e-02,  1.8414e-02,  2.4672e-02,  1.2600e-02,\n",
      "         -5.6666e-03,  2.5549e-03,  4.0709e-03,  1.9000e-02, -6.1543e-02,\n",
      "          1.2990e-02, -2.5736e-02,  1.1150e-02, -2.5157e-02, -8.1259e-02,\n",
      "          2.5197e-02,  1.6732e-03,  2.1115e-02, -4.1658e-02,  6.6255e-02,\n",
      "         -2.5587e-02, -3.2157e-04,  1.2388e-02,  4.0023e-02, -6.1048e-02,\n",
      "          6.0717e-02,  2.1476e-02,  5.8257e-03,  4.3190e-02, -5.3267e-03,\n",
      "         -1.1518e-01,  5.6670e-03, -2.2334e-02, -2.0324e-02,  1.9542e-03,\n",
      "         -3.7446e-02,  1.5559e-02, -7.3694e-03,  2.4515e-02, -4.5967e-02,\n",
      "         -3.7894e-03,  3.2286e-02,  1.0811e-02,  4.4440e-03,  4.8463e-03,\n",
      "          2.5042e-02,  4.7359e-02, -6.8103e-02, -6.5093e-03,  7.0785e-02,\n",
      "         -3.5528e-02,  4.7189e-03, -1.0189e-02,  4.2352e-02, -2.0553e-02,\n",
      "         -1.7234e-02,  1.3394e-02,  1.8244e-02,  6.2473e-02, -4.8055e-02,\n",
      "          1.2645e-02, -7.0589e-03,  5.6752e-02, -3.4373e-02, -3.1496e-02,\n",
      "         -5.9777e-03, -2.4881e-02,  7.9201e-02,  1.3747e-02, -2.5393e-03,\n",
      "         -3.2503e-02, -3.1110e-02,  5.7813e-02, -4.5621e-03, -1.0531e-02,\n",
      "          4.2530e-02,  3.0264e-02, -6.1265e-03,  4.5427e-02, -5.1061e-02,\n",
      "         -1.5275e-01,  1.0616e-02,  2.6280e-02,  1.8947e-02,  3.2359e-02,\n",
      "         -1.4984e-02,  3.7450e-03, -6.5668e-02,  2.4765e-03, -3.2025e-02,\n",
      "          2.9947e-02,  4.5328e-03,  3.0595e-02, -5.2844e-03,  7.7454e-02,\n",
      "         -1.9671e-02,  2.9783e-02, -3.6750e-02, -1.6449e-02,  6.7979e-03,\n",
      "          6.7633e-03, -3.4163e-02,  2.9027e-02, -1.8871e-02,  3.0973e-02,\n",
      "         -3.5600e-02, -2.3323e-02, -2.5930e-02, -4.0542e-02,  2.5499e-02,\n",
      "          3.3001e-02,  1.2359e-02,  2.5706e-02,  3.0320e-02,  4.6401e-02,\n",
      "         -2.3879e-02,  2.0202e-02, -2.2375e-03,  8.2037e-03, -2.0483e-02,\n",
      "         -7.8328e-02, -1.1248e-02, -9.1449e-03,  7.5573e-03, -2.2487e-02,\n",
      "         -1.6329e-02,  1.8263e-02,  3.7522e-02,  1.9167e-02,  1.4377e-02,\n",
      "         -1.4204e-02,  3.2154e-02, -4.2945e-02,  5.7657e-03, -8.0327e-04,\n",
      "         -6.6083e-02, -2.8194e-04, -1.8378e-02, -4.8475e-02,  2.3617e-02,\n",
      "         -3.3750e-02,  1.1834e-02, -1.0890e-02,  2.5747e-02, -3.7289e-02,\n",
      "          5.2489e-02,  3.3673e-02,  2.8601e-02,  4.2754e-03, -4.7038e-02,\n",
      "         -3.0422e-02, -1.7425e-02,  1.3590e-02, -5.1289e-02, -2.6229e-02,\n",
      "         -1.0611e-02, -7.7437e-03,  2.7641e-02, -4.1801e-03,  2.6006e-02,\n",
      "         -3.3880e-03,  3.3067e-03, -5.1195e-02,  1.0351e-02, -5.3868e-03,\n",
      "         -3.0990e-02,  2.9896e-02, -2.5370e-02,  7.7419e-02, -1.1452e-02,\n",
      "         -2.1355e-02,  2.0652e-02,  5.2692e-02,  3.8496e-02, -5.2882e-02,\n",
      "          2.0792e-03, -8.6128e-02,  3.2290e-02, -1.0719e-02,  1.2896e-02,\n",
      "          3.0134e-02,  1.5601e-02,  3.8173e-02,  3.0548e-02,  4.3263e-02,\n",
      "          3.2726e-02,  1.9825e-02, -1.0034e-02, -2.1873e-02,  1.3308e-02,\n",
      "          3.9475e-02, -1.2140e-02,  8.5712e-02,  4.2187e-02, -2.3477e-02,\n",
      "         -1.9578e-02,  3.7402e-02,  1.4542e-03,  3.4245e-02, -1.5425e-02,\n",
      "         -2.8997e-03, -3.4361e-02,  3.4428e-02, -3.5852e-02,  1.1834e-02,\n",
      "         -3.3657e-03, -3.2083e-04, -5.2636e-02,  3.2873e-02, -6.1226e-04,\n",
      "          2.0334e-02,  3.3753e-02,  5.1543e-02,  4.3593e-03, -4.9230e-02,\n",
      "         -1.4788e-02, -2.5702e-02, -9.4918e-04, -9.5153e-03,  9.0001e-03,\n",
      "          2.0273e-02, -2.8655e-02, -2.4118e-02,  2.3451e-02,  2.7414e-02,\n",
      "         -3.3920e-02,  5.2001e-02,  3.6443e-02, -6.0504e-03,  7.4200e-02,\n",
      "         -5.9112e-02, -5.3881e-02, -1.7870e-02,  7.4283e-03, -2.7031e-02,\n",
      "         -1.5785e-02, -6.3814e-02,  4.7019e-02,  1.4489e-02, -3.2238e-02,\n",
      "          5.6374e-03, -6.0349e-03,  6.9934e-03,  2.6128e-02,  1.4643e-02,\n",
      "          1.8811e-03,  1.8494e-02, -9.8517e-04, -2.6079e-02, -2.9486e-02,\n",
      "         -7.7439e-02, -1.8391e-02, -4.0349e-02,  8.9021e-03,  2.3187e-02,\n",
      "         -5.5234e-02, -2.4531e-03,  7.9741e-03,  8.4657e-03,  4.5370e-03,\n",
      "         -2.5644e-02,  2.0494e-03, -3.9706e-02, -5.1241e-02,  1.2272e-02,\n",
      "         -8.8885e-03, -6.7159e-02,  5.1499e-02,  1.8548e-02, -5.7593e-02,\n",
      "          4.5964e-02,  2.3069e-02, -1.5816e-02, -1.7899e-02, -2.6860e-02,\n",
      "          3.9363e-02,  4.0149e-02,  3.6070e-03,  9.7411e-03, -8.0640e-02,\n",
      "          5.5356e-02,  1.6593e-02, -3.2203e-03,  7.8833e-02,  4.0068e-02,\n",
      "          4.2472e-02, -1.1705e-02,  7.8580e-02,  3.7629e-02, -6.7491e-02,\n",
      "          1.6742e-02, -1.4671e-02,  4.7917e-02, -2.8045e-02, -2.8992e-02,\n",
      "          1.3161e-03, -6.2237e-03, -1.4460e-02, -1.6555e-03,  1.5189e-02,\n",
      "          1.8578e-02, -3.5313e-02, -9.5090e-05,  7.6177e-02,  2.5247e-03,\n",
      "         -3.2401e-02,  2.0006e-02,  1.6701e-02, -4.7967e-02, -4.5037e-02,\n",
      "          2.0186e-02, -3.6078e-03, -8.5892e-02, -3.5775e-02,  4.8689e-02,\n",
      "          4.3603e-03,  6.5434e-02,  1.7247e-02, -1.3526e-02,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0275, -0.0272,  0.0180,  ..., -0.0090, -0.0159,  0.0157]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0275, -0.0272,  0.0180,  ..., -0.0090, -0.0159,  0.0157]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0225,  0.0021, -0.0073, -0.0453,  0.0067, -0.1035,  0.0007, -0.0384,\n",
      "         -0.0241, -0.0236, -0.0056, -0.0469,  0.0058, -0.0028,  0.0082,  0.0394,\n",
      "          0.0411,  0.0036, -0.0933, -0.0109,  0.0350, -0.0014,  0.0195,  0.0090,\n",
      "          0.0102,  0.0144,  0.0910,  0.0898,  0.0680, -0.0267,  0.0296,  0.0123,\n",
      "         -0.0376,  0.0462,  0.0312, -0.0027,  0.0219, -0.0156, -0.0906, -0.0108,\n",
      "          0.0491, -0.0022, -0.0330,  0.0454,  0.0668, -0.0107,  0.0695, -0.0285,\n",
      "         -0.1164,  0.0746,  0.0207,  0.0046, -0.0736, -0.0499, -0.0580, -0.0040,\n",
      "          0.0032,  0.0278,  0.0149, -0.0144,  0.0139, -0.0536,  0.0244,  0.0268,\n",
      "         -0.0166, -0.0146, -0.0104, -0.0225, -0.0341,  0.0614, -0.0234, -0.0311,\n",
      "         -0.0392,  0.0299, -0.0474, -0.0211, -0.0228, -0.0282, -0.0288, -0.0021,\n",
      "         -0.0283, -0.0399,  0.0053,  0.0269, -0.0246,  0.0394, -0.0482, -0.0131,\n",
      "         -0.0672, -0.0286, -0.0382, -0.0050, -0.0136,  0.0675,  0.0515,  0.0383,\n",
      "          0.0347, -0.0290, -0.0956,  0.0055, -0.0542,  0.0374,  0.0811, -0.0625,\n",
      "          0.0048,  0.0439,  0.0274, -0.0130,  0.0037,  0.0702, -0.0189, -0.0653,\n",
      "          0.0338,  0.0484, -0.0758, -0.1118, -0.0982, -0.0586, -0.1017,  0.0323,\n",
      "         -0.0187,  0.0306,  0.0579,  0.0252, -0.0435,  0.0251, -0.0279,  0.0555,\n",
      "          0.0822,  0.0015,  0.0172,  0.0274, -0.0081,  0.0368,  0.0218,  0.0329,\n",
      "          0.0185,  0.0088, -0.0144,  0.0301,  0.0175, -0.0075,  0.0198,  0.0053,\n",
      "          0.0708,  0.0166, -0.0065, -0.0641, -0.0199, -0.0439,  0.0324,  0.0846,\n",
      "         -0.0156, -0.0623,  0.0368,  0.0376, -0.0219, -0.0321,  0.0116, -0.0214,\n",
      "         -0.0275, -0.0293, -0.0180, -0.0346, -0.0491,  0.0416, -0.0381, -0.0820,\n",
      "         -0.0298,  0.0151, -0.0111,  0.0488,  0.0482, -0.0697,  0.0230, -0.0346,\n",
      "         -0.0385, -0.0320,  0.0022, -0.0195,  0.0382,  0.0669,  0.0117, -0.0143,\n",
      "         -0.0986,  0.0230, -0.0114,  0.0136, -0.0391,  0.0525, -0.0245, -0.0297,\n",
      "         -0.0252,  0.0744,  0.0637,  0.0055,  0.0261,  0.0363, -0.0025, -0.0683,\n",
      "         -0.0255, -0.0456,  0.0402, -0.0391, -0.0926,  0.0581, -0.0183, -0.0362,\n",
      "         -0.0535,  0.0409, -0.0179,  0.0289,  0.0655,  0.0294, -0.0775,  0.0396,\n",
      "          0.0239,  0.0403,  0.0801, -0.0544, -0.1100, -0.0068, -0.0387, -0.0217,\n",
      "          0.0214, -0.0616, -0.0089, -0.0123,  0.0534, -0.0370,  0.0211,  0.0167,\n",
      "         -0.0111, -0.0011,  0.0025,  0.0404,  0.0642, -0.0452, -0.0113,  0.0538,\n",
      "         -0.0520, -0.0056,  0.0366,  0.0759, -0.0529, -0.0374,  0.0588,  0.0106,\n",
      "          0.1057, -0.0457, -0.0078,  0.0133,  0.0720, -0.0095, -0.0401,  0.0218,\n",
      "         -0.0456,  0.0502,  0.0087, -0.0067, -0.0664, -0.0569,  0.0319,  0.0456,\n",
      "          0.0117,  0.0440,  0.0148, -0.0104,  0.0672, -0.0583, -0.1598,  0.0023,\n",
      "          0.0538, -0.0143,  0.0224,  0.0161, -0.0428, -0.0923, -0.0291, -0.0308,\n",
      "         -0.0071, -0.0245,  0.0439, -0.0270,  0.0261, -0.0623,  0.0007, -0.0537,\n",
      "         -0.0007, -0.0388,  0.0227, -0.0308,  0.0428, -0.0017,  0.0260, -0.0640,\n",
      "         -0.0064, -0.0349, -0.0219,  0.0544,  0.0239,  0.0471,  0.0232,  0.0261,\n",
      "          0.0282, -0.0696,  0.0527,  0.0118,  0.0263, -0.0006, -0.0661,  0.0209,\n",
      "          0.0058,  0.0030, -0.0387,  0.0064,  0.0166,  0.0501, -0.0120,  0.0489,\n",
      "          0.0016,  0.0325, -0.0278,  0.0218,  0.0103, -0.0845, -0.0082, -0.0481,\n",
      "         -0.0449,  0.0369, -0.0878, -0.0075, -0.0770,  0.0889, -0.0595,  0.0261,\n",
      "          0.0722,  0.0245, -0.0125, -0.0249, -0.0267, -0.0498, -0.0126, -0.0515,\n",
      "         -0.0232, -0.0248, -0.0696,  0.0107, -0.0039,  0.0008,  0.0189,  0.0703,\n",
      "         -0.0633,  0.0129,  0.0091, -0.0232,  0.0251, -0.0279,  0.1155, -0.0159,\n",
      "         -0.0343,  0.0638, -0.0056,  0.0369, -0.0636, -0.0179, -0.1186,  0.0721,\n",
      "         -0.0307,  0.0792,  0.0270,  0.0300,  0.0747,  0.0376,  0.0251,  0.0128,\n",
      "          0.0466, -0.0187, -0.0412,  0.0287,  0.0269, -0.0115,  0.0786,  0.0066,\n",
      "         -0.0067,  0.0407,  0.0497,  0.0495,  0.0253, -0.0395, -0.0038, -0.0613,\n",
      "          0.0059, -0.0225,  0.0075,  0.0116,  0.0301, -0.0510,  0.0483, -0.0205,\n",
      "          0.0458,  0.0350,  0.0831, -0.0208, -0.0651,  0.0033, -0.0349,  0.0098,\n",
      "         -0.0437,  0.0113, -0.0011, -0.0237, -0.0102,  0.0026,  0.0296, -0.0294,\n",
      "          0.0504,  0.0353, -0.0013,  0.0868, -0.0338, -0.1060, -0.0469, -0.0076,\n",
      "         -0.0594, -0.0139, -0.1025,  0.0495,  0.0576, -0.0711,  0.0156, -0.0270,\n",
      "          0.0126,  0.0289,  0.0422, -0.0147, -0.0343,  0.0242, -0.0238,  0.0042,\n",
      "         -0.0163, -0.0254, -0.0318, -0.0033, -0.0148, -0.0554, -0.0152,  0.0177,\n",
      "         -0.0006,  0.0078,  0.0026, -0.0279, -0.0466, -0.0133, -0.0026,  0.0167,\n",
      "         -0.0890,  0.0875, -0.0195, -0.0232,  0.0426, -0.0253, -0.0016, -0.0328,\n",
      "          0.0101,  0.0372,  0.0486, -0.0026, -0.0226, -0.0536,  0.0217,  0.0102,\n",
      "         -0.0430,  0.0425,  0.0180, -0.0253,  0.0030,  0.0580,  0.0355, -0.0872,\n",
      "          0.0397, -0.0145,  0.0678,  0.0079, -0.0342,  0.0471,  0.0262,  0.0146,\n",
      "         -0.0287, -0.0009, -0.0416,  0.0084, -0.0227,  0.0526, -0.0619, -0.0201,\n",
      "          0.0613,  0.0342, -0.0878, -0.0522,  0.0243, -0.0385, -0.0821, -0.0479,\n",
      "          0.0745, -0.0005,  0.0558,  0.0333, -0.0184,  0.0317,  0.0303, -0.0447]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0236, -0.0291,  0.0184,  ..., -0.0125, -0.0174,  0.0184]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0236, -0.0291,  0.0184,  ..., -0.0125, -0.0174,  0.0184]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 3.9943e-02, -6.3473e-04, -2.4333e-02, -2.6465e-02,  3.6282e-02,\n",
      "         -8.1245e-02,  2.0234e-02, -4.7533e-02, -1.4706e-02,  1.8934e-02,\n",
      "          4.9761e-04, -6.6569e-02,  3.0510e-02,  8.3242e-03,  3.4185e-03,\n",
      "          4.2514e-02,  3.6875e-02,  1.1155e-02, -6.4569e-02, -2.6975e-02,\n",
      "          1.4409e-02, -8.3901e-03,  1.4958e-02,  1.4861e-02,  1.6609e-02,\n",
      "          4.6618e-02,  8.1440e-02,  6.3781e-02,  4.3447e-02,  5.9358e-03,\n",
      "         -5.9265e-04,  3.1957e-02, -3.8824e-03,  6.6324e-02,  1.1476e-03,\n",
      "          3.0135e-02,  2.8017e-02, -2.6136e-03, -8.7879e-02,  8.0971e-03,\n",
      "          4.2865e-02,  2.4699e-02, -1.6333e-03,  4.5484e-02,  4.8548e-02,\n",
      "         -2.7261e-02,  4.9371e-02, -1.1094e-02, -6.2017e-02,  6.3433e-02,\n",
      "          1.1362e-02, -3.7070e-02, -2.8188e-02, -3.8195e-02, -5.8539e-02,\n",
      "          1.2449e-02, -2.7576e-03,  3.2095e-02,  7.0868e-03,  7.0801e-03,\n",
      "         -2.9167e-02, -6.1884e-02,  3.3293e-02,  7.4093e-03,  8.6460e-03,\n",
      "          2.1394e-02,  2.8104e-02, -2.3261e-02, -4.2063e-02,  2.1559e-02,\n",
      "         -7.4343e-03, -4.5396e-02, -1.7797e-02, -8.0800e-03, -5.4489e-02,\n",
      "         -7.8084e-03,  5.2327e-03, -1.9146e-02, -4.8832e-02,  2.1798e-02,\n",
      "          9.2141e-03, -3.6777e-02,  6.6118e-03,  3.2340e-02,  1.9615e-02,\n",
      "          2.1029e-02, -2.2645e-02, -2.0520e-02, -8.7323e-02, -4.5272e-02,\n",
      "         -2.7091e-02, -2.3780e-02, -2.8575e-02,  7.5322e-02,  2.6836e-02,\n",
      "         -1.4969e-02,  1.4021e-02,  3.6706e-03, -6.0353e-02, -9.9927e-03,\n",
      "         -7.4608e-02, -6.0827e-03,  4.9744e-02, -1.1646e-02, -1.5283e-02,\n",
      "          3.9851e-02,  1.9263e-02, -2.9827e-02,  3.6977e-02,  4.5320e-02,\n",
      "         -4.4115e-02, -5.5769e-02,  1.1291e-02,  3.4049e-02, -4.0742e-02,\n",
      "         -7.8004e-02, -3.9623e-02, -6.5474e-02, -5.7309e-02,  3.2225e-03,\n",
      "          4.3883e-03,  2.9382e-02,  7.0717e-02,  6.7704e-02, -2.4883e-02,\n",
      "          2.6200e-02, -1.4513e-02,  2.5271e-02,  3.6185e-02, -6.2445e-03,\n",
      "         -1.5884e-02, -5.3081e-03, -5.1873e-03,  4.8760e-02,  4.0506e-02,\n",
      "          4.3785e-02,  1.0459e-02,  5.6056e-02, -4.2227e-03,  4.8874e-02,\n",
      "         -2.1245e-02, -3.1359e-03,  1.8048e-02, -2.3473e-02,  5.5935e-02,\n",
      "         -3.8959e-02,  1.0918e-02, -8.7178e-02,  3.0119e-03, -2.1534e-02,\n",
      "          3.2873e-03,  7.5276e-02, -3.9145e-02, -2.3037e-02,  1.6807e-02,\n",
      "          5.1875e-02, -6.6744e-05, -2.1719e-02,  3.4947e-02, -1.9906e-02,\n",
      "         -4.2533e-02, -1.4287e-02, -1.7697e-02, -3.5453e-02, -2.6380e-02,\n",
      "          3.0872e-03, -3.2254e-02, -5.6165e-02, -1.4399e-02,  6.4918e-04,\n",
      "         -4.0651e-02,  5.3044e-02,  5.6464e-02, -7.1317e-02,  2.5132e-02,\n",
      "         -2.7497e-02, -1.5007e-02, -4.0442e-02,  3.2662e-02, -3.9329e-02,\n",
      "          1.1346e-02,  5.2525e-02,  2.5804e-02,  2.6124e-02, -8.0331e-02,\n",
      "          1.4954e-02,  2.4630e-02,  1.5274e-02, -4.8547e-02,  3.7759e-02,\n",
      "         -2.4851e-02, -6.2086e-02,  1.5069e-02,  5.3204e-02,  1.5821e-02,\n",
      "         -2.3344e-02, -1.3930e-03,  1.4708e-02,  1.1591e-02, -7.3345e-02,\n",
      "         -3.6105e-02, -2.8722e-02,  7.6852e-04, -2.2699e-02, -8.3008e-02,\n",
      "          7.0183e-02, -3.0014e-02, -9.3609e-03, -2.8232e-02,  6.1581e-02,\n",
      "         -2.4002e-02,  1.5176e-02,  5.0153e-02, -9.7399e-04, -5.9783e-02,\n",
      "          3.4617e-02, -2.9192e-03,  2.7485e-02,  6.9439e-02, -1.1364e-02,\n",
      "         -1.3452e-01,  2.2476e-02, -1.4151e-02, -7.1442e-03, -1.2852e-02,\n",
      "         -2.2974e-02,  2.5305e-02,  9.9605e-03,  2.9039e-03, -3.2136e-02,\n",
      "          1.0917e-02,  3.9640e-02, -1.0201e-02, -1.9363e-02, -1.1269e-02,\n",
      "          9.5938e-03,  5.2384e-02, -5.6701e-02, -8.5196e-03,  1.2629e-02,\n",
      "         -4.5141e-02, -2.6792e-02, -1.0696e-02,  2.9739e-02, -5.2543e-02,\n",
      "          4.4625e-03,  3.8448e-02, -1.4611e-03,  7.8076e-02, -1.0361e-02,\n",
      "         -1.6192e-02, -7.5027e-03,  5.8933e-02, -8.2104e-03, -1.0303e-02,\n",
      "         -1.0491e-02, -6.1503e-02,  3.2754e-02, -1.5981e-02, -2.7549e-02,\n",
      "         -5.1394e-02, -3.0974e-02,  4.4842e-02,  1.3567e-02, -4.8374e-03,\n",
      "          4.6250e-02,  4.9839e-02, -1.7988e-02,  3.7572e-02, -4.8327e-02,\n",
      "         -1.2389e-01,  9.6038e-03,  2.2187e-02,  1.4403e-02,  4.4363e-03,\n",
      "         -7.8948e-03, -3.4436e-02, -5.8862e-02, -3.2297e-02,  2.4687e-03,\n",
      "          1.3516e-02, -1.9046e-02,  6.2428e-02, -2.4486e-02,  2.5915e-02,\n",
      "         -4.7463e-02,  4.1579e-02, -1.3121e-02, -2.5279e-02, -1.6883e-02,\n",
      "         -1.2437e-02, -6.8095e-02,  3.2792e-02, -3.9081e-02,  2.3718e-02,\n",
      "         -7.6230e-03,  2.5899e-02, -9.0227e-03, -4.4538e-02,  1.8885e-02,\n",
      "          6.7445e-03,  2.3967e-02,  3.6491e-02,  6.1667e-02,  2.9088e-02,\n",
      "         -4.6740e-02,  4.2268e-02,  5.3146e-03,  3.4716e-03,  1.3579e-02,\n",
      "         -3.4293e-02, -7.6553e-03, -3.0861e-02,  2.1517e-02, -1.5814e-02,\n",
      "          1.6505e-02,  2.4900e-02,  3.6272e-02, -1.1502e-02,  2.1404e-02,\n",
      "         -1.9012e-02,  4.1999e-02, -2.3130e-02, -1.2259e-03, -3.5337e-02,\n",
      "         -6.3823e-02, -1.7515e-02, -4.7187e-02, -2.1176e-02,  2.9516e-02,\n",
      "         -5.6702e-02, -3.0155e-02, -4.3931e-02,  5.9581e-02, -4.8076e-02,\n",
      "          3.2992e-02,  5.8642e-02,  4.0638e-02, -4.9440e-03, -2.6222e-02,\n",
      "         -1.7771e-02, -4.0846e-02,  2.8135e-02, -4.8641e-02, -3.6589e-02,\n",
      "         -4.3364e-03, -4.0931e-02,  1.2174e-02, -2.9099e-02, -3.0528e-02,\n",
      "          1.0307e-02,  6.1238e-02, -5.8316e-02, -1.3105e-02,  9.0719e-03,\n",
      "         -1.7113e-02, -1.6299e-02, -4.1770e-02,  5.2599e-02, -2.1581e-02,\n",
      "          1.3731e-02,  3.1022e-02, -9.9482e-03,  2.3787e-02, -7.2789e-02,\n",
      "          3.9918e-03, -1.1121e-01,  6.5336e-02, -3.4317e-02,  6.3555e-02,\n",
      "          4.7264e-02, -2.1690e-02,  3.2510e-02, -2.9224e-03,  8.1941e-03,\n",
      "          2.2115e-03,  4.4054e-02, -2.8647e-02, -1.7062e-02,  3.1700e-02,\n",
      "          1.3082e-02, -7.8945e-03,  5.9706e-02, -3.6144e-03,  1.2505e-02,\n",
      "          3.5541e-02,  5.5393e-02,  7.4438e-03,  3.9664e-02, -2.1253e-02,\n",
      "         -6.8311e-03, -4.3957e-02,  3.7919e-02, -2.8258e-02, -8.0768e-03,\n",
      "         -4.2647e-02,  3.2717e-02, -2.9899e-02, -9.3153e-03,  1.5475e-02,\n",
      "          3.0740e-02,  1.8109e-02,  2.8607e-02, -3.8829e-02, -4.7982e-02,\n",
      "         -7.1243e-03, -2.4547e-02,  8.3619e-04, -1.8965e-02,  1.3804e-02,\n",
      "          1.0712e-02, -3.4088e-02, -7.9899e-02, -7.0846e-03,  6.9593e-03,\n",
      "         -2.1685e-02,  5.5655e-02, -1.2060e-02,  2.3431e-03,  7.2531e-02,\n",
      "         -6.0682e-02, -7.4575e-02, -1.7317e-02,  5.8419e-03, -2.3470e-02,\n",
      "          9.9523e-03, -6.2580e-02,  7.6685e-02,  2.9135e-02, -3.8954e-02,\n",
      "          1.4604e-02,  3.1968e-03,  3.2509e-02, -2.8317e-03,  2.2350e-02,\n",
      "         -4.7101e-02, -5.0960e-02,  3.1412e-03, -1.5378e-02, -2.6794e-02,\n",
      "         -1.9578e-02,  1.0552e-02, -6.9772e-02,  2.0636e-02, -2.9148e-03,\n",
      "         -4.0716e-02,  2.5176e-03,  7.4695e-03, -3.4024e-04,  3.2252e-02,\n",
      "         -2.8218e-02, -1.9674e-02, -2.1514e-02, -9.1328e-03,  2.0247e-02,\n",
      "          3.0113e-02, -7.1527e-02,  4.6293e-02,  2.1005e-02, -2.0541e-02,\n",
      "          4.0748e-02,  4.6423e-03, -2.4288e-02, -1.5478e-02, -3.5585e-02,\n",
      "          2.1604e-02,  3.8710e-02,  1.4737e-02, -3.1023e-02, -5.9019e-02,\n",
      "          3.0318e-02,  3.5039e-02, -1.1337e-03,  3.8222e-02,  1.5961e-02,\n",
      "          2.4988e-02,  1.0752e-02,  7.9247e-02,  3.4722e-02, -7.4136e-02,\n",
      "          3.9050e-02, -1.8820e-02,  6.0536e-02,  6.0405e-03, -1.8136e-02,\n",
      "          4.7733e-02,  4.5106e-03, -5.9822e-03, -1.5836e-02,  1.4583e-03,\n",
      "         -2.1234e-02, -2.3092e-02, -6.0511e-03,  7.6471e-02, -1.0707e-02,\n",
      "         -3.3524e-02,  3.2486e-02, -7.2459e-03, -6.4545e-02, -4.4839e-02,\n",
      "          3.7363e-02, -2.0615e-02, -1.1821e-01, -2.0830e-02,  6.9055e-02,\n",
      "         -1.0918e-02,  5.4751e-02,  2.7437e-02,  3.1381e-02,  1.9435e-02,\n",
      "          5.1162e-02, -3.3509e-02]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0264, -0.0235,  0.0189,  ..., -0.0144, -0.0234,  0.0177],\n",
      "        [-0.0257, -0.0202,  0.0130,  ..., -0.0041, -0.0104,  0.0161]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0264, -0.0235,  0.0189,  ..., -0.0144, -0.0234,  0.0177],\n",
      "        [-0.0257, -0.0202,  0.0130,  ..., -0.0041, -0.0104,  0.0161]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 7.8088e-03, -2.3229e-02, -1.9114e-02,  ...,  2.5953e-02,\n",
      "          5.1098e-02, -5.5057e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0322, -0.0230,  0.0190,  ..., -0.0128, -0.0218,  0.0221],\n",
      "        [-0.0250, -0.0242,  0.0197,  ..., -0.0031, -0.0082,  0.0160],\n",
      "        [-0.0304, -0.0200,  0.0124,  ..., -0.0043, -0.0214,  0.0146]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0322, -0.0230,  0.0190,  ..., -0.0128, -0.0218,  0.0221],\n",
      "        [-0.0250, -0.0242,  0.0197,  ..., -0.0031, -0.0082,  0.0160],\n",
      "        [-0.0304, -0.0200,  0.0124,  ..., -0.0043, -0.0214,  0.0146]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-6.8748e-03, -1.3605e-02,  7.1700e-03,  ...,  1.2522e-02,\n",
      "          2.0355e-02, -2.4865e-02],\n",
      "        [-2.9532e-03, -1.3722e-02, -2.7846e-02,  ..., -3.4922e-03,\n",
      "          1.9039e-03, -3.1346e-02],\n",
      "        [ 4.0999e-03, -6.1420e-03, -4.0220e-03,  ...,  3.6052e-02,\n",
      "          3.8271e-02,  8.0397e-05]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0334, -0.0238,  0.0190,  ..., -0.0077, -0.0226,  0.0237],\n",
      "        [-0.0240, -0.0269,  0.0206,  ..., -0.0059, -0.0154,  0.0115],\n",
      "        [-0.0288, -0.0240,  0.0192,  ..., -0.0040, -0.0164,  0.0145]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0334, -0.0238,  0.0190,  ..., -0.0077, -0.0226,  0.0237],\n",
      "        [-0.0240, -0.0269,  0.0206,  ..., -0.0059, -0.0154,  0.0115],\n",
      "        [-0.0288, -0.0240,  0.0192,  ..., -0.0040, -0.0164,  0.0145]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0267,  0.0010, -0.0196,  ...,  0.0005,  0.0283, -0.0203],\n",
      "        [ 0.0260, -0.0238,  0.0250,  ...,  0.0017,  0.0297, -0.0235],\n",
      "        [-0.0157,  0.0032, -0.0359,  ...,  0.0232,  0.0049, -0.0217]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0300, -0.0227,  0.0171,  ..., -0.0129, -0.0218,  0.0189],\n",
      "        [-0.0245, -0.0259,  0.0202,  ..., -0.0084, -0.0140,  0.0184],\n",
      "        [-0.0299, -0.0252,  0.0219,  ..., -0.0034, -0.0197,  0.0165]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0300, -0.0227,  0.0171,  ..., -0.0129, -0.0218,  0.0189],\n",
      "        [-0.0245, -0.0259,  0.0202,  ..., -0.0084, -0.0140,  0.0184],\n",
      "        [-0.0299, -0.0252,  0.0219,  ..., -0.0034, -0.0197,  0.0165]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0195,  0.0024, -0.0233,  ...,  0.0120,  0.0189, -0.0226],\n",
      "        [ 0.0259, -0.0076, -0.0144,  ...,  0.0053,  0.0274, -0.0328],\n",
      "        [ 0.0088, -0.0010, -0.0169,  ...,  0.0276,  0.0261, -0.0068]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0251, -0.0286,  0.0192,  ..., -0.0149, -0.0221,  0.0201],\n",
      "        [-0.0237, -0.0252,  0.0159,  ..., -0.0089, -0.0101,  0.0183],\n",
      "        [-0.0329, -0.0201,  0.0212,  ..., -0.0035, -0.0204,  0.0181]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0251, -0.0286,  0.0192,  ..., -0.0149, -0.0221,  0.0201],\n",
      "        [-0.0237, -0.0252,  0.0159,  ..., -0.0089, -0.0101,  0.0183],\n",
      "        [-0.0329, -0.0201,  0.0212,  ..., -0.0035, -0.0204,  0.0181]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0439, -0.0200, -0.0223,  ...,  0.0114,  0.0445, -0.0566],\n",
      "        [-0.0055, -0.0275, -0.0182,  ...,  0.0095,  0.0294, -0.0096],\n",
      "        [ 0.0261, -0.0046, -0.0160,  ...,  0.0313,  0.0107, -0.0573]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0251, -0.0263,  0.0198,  ..., -0.0145, -0.0178,  0.0185],\n",
      "        [-0.0260, -0.0306,  0.0189,  ..., -0.0080, -0.0070,  0.0192],\n",
      "        [-0.0332, -0.0253,  0.0200,  ..., -0.0080, -0.0235,  0.0164]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0251, -0.0263,  0.0198,  ..., -0.0145, -0.0178,  0.0185],\n",
      "        [-0.0260, -0.0306,  0.0189,  ..., -0.0080, -0.0070,  0.0192],\n",
      "        [-0.0332, -0.0253,  0.0200,  ..., -0.0080, -0.0235,  0.0164]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0078, -0.0260, -0.0205,  ...,  0.0145,  0.0510, -0.0102],\n",
      "        [ 0.0290,  0.0047, -0.0227,  ...,  0.0162,  0.0292, -0.0161],\n",
      "        [ 0.0056, -0.0269, -0.0216,  ..., -0.0044,  0.0573, -0.0453]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 0 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0275, -0.0290,  0.0175,  ..., -0.0190, -0.0173,  0.0171],\n",
      "        [-0.0256, -0.0283,  0.0193,  ..., -0.0101, -0.0094,  0.0163],\n",
      "        [-0.0363, -0.0270,  0.0166,  ..., -0.0085, -0.0268,  0.0153]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0275, -0.0290,  0.0175,  ..., -0.0190, -0.0173,  0.0171],\n",
      "        [-0.0256, -0.0283,  0.0193,  ..., -0.0101, -0.0094,  0.0163],\n",
      "        [-0.0363, -0.0270,  0.0166,  ..., -0.0085, -0.0268,  0.0153]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275],\n",
      "        [-0.0034,  0.0098,  0.0014,  ...,  0.0376,  0.0196, -0.0275]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[ 0.0078,  0.0054,  0.0002,  ..., -0.0031,  0.0096, -0.0045],\n",
      "         [ 0.0106,  0.0045, -0.0008,  ..., -0.0000,  0.0068,  0.0014],\n",
      "         [ 0.0092,  0.0057, -0.0007,  ..., -0.0022,  0.0068, -0.0002]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>), tensor([[[-0.0294, -0.0244,  0.0137,  ..., -0.0127, -0.0196,  0.0183],\n",
      "         [-0.0278, -0.0246,  0.0151,  ..., -0.0053, -0.0150,  0.0178],\n",
      "         [-0.0371, -0.0227,  0.0124,  ..., -0.0039, -0.0280,  0.0172]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tGet a forward layer and backward layer at layer 2\n",
      "\t\tCaching...: output_sequence to cache both forward and backward\n",
      "\t\tstate is None? False\n",
      "\t\t\tAlright, Set hidden_state/memory_state for both forward and backward\n",
      "\t\t\tstate[0](hidden_state) = tensor([[[ 2.4202e-03,  0.0000e+00,  2.0514e-03,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00],\n",
      "         [ 2.3629e-03,  0.0000e+00,  1.1207e-03,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [ 0.0000e+00,  3.1858e-03,  1.0247e-03,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04]]], device='cuda:0')\n",
      "\t\t\tstate[1](memory_state) = tensor([[[ 9.0830e-04, -3.6197e-03,  7.5843e-03,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03],\n",
      "         [-8.0450e-04, -3.0386e-03,  5.7585e-03,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [-7.2586e-04, -4.7068e-03,  5.0241e-03,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04]]], device='cuda:0')\n",
      "\t\tRUN forward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? False\n",
      "\t\t\t\tOk, Using `initial_state`, create full_batch_previous memory and state.\n",
      "\t\t\t\t(previous_state) = initial_state[0] = tensor([[[ 0.0024,  0.0000,  0.0021,  ..., -0.0012,  0.0000,  0.0000],\n",
      "         [ 0.0024,  0.0000,  0.0011,  ..., -0.0010,  0.0005,  0.0013],\n",
      "         [ 0.0000,  0.0032,  0.0010,  ..., -0.0010, -0.0001,  0.0008]]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tfull_batch_previous_state = initial_state[0].squeeze(0) = tensor([[ 0.0024,  0.0000,  0.0021,  ..., -0.0012,  0.0000,  0.0000],\n",
      "        [ 0.0024,  0.0000,  0.0011,  ..., -0.0010,  0.0005,  0.0013],\n",
      "        [ 0.0000,  0.0032,  0.0010,  ..., -0.0010, -0.0001,  0.0008]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\t(previous_memory) = initial_state[1] = tensor([[[ 0.0009, -0.0036,  0.0076,  ...,  0.0025, -0.0031,  0.0070],\n",
      "         [-0.0008, -0.0030,  0.0058,  ...,  0.0020, -0.0014,  0.0074],\n",
      "         [-0.0007, -0.0047,  0.0050,  ...,  0.0032, -0.0003,  0.0054]]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tfull_batch_previous_memory = initial_state[1].squeeze(0) = tensor([[ 0.0009, -0.0036,  0.0076,  ...,  0.0025, -0.0031,  0.0070],\n",
      "        [-0.0008, -0.0030,  0.0058,  ...,  0.0020, -0.0014,  0.0074],\n",
      "        [-0.0007, -0.0047,  0.0050,  ...,  0.0032, -0.0003,  0.0054]],\n",
      "       device='cuda:0')\n",
      "\t\t\t\tSet current_length_index... is it forward?? True\n",
      "\t\t\t\tOk, forward!! current_length_index = batch_size - 1 = 2\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[ 0.0024,  0.0000,  0.0021,  ..., -0.0012,  0.0000,  0.0000],\n",
      "        [ 0.0024,  0.0000,  0.0011,  ..., -0.0010,  0.0005,  0.0013],\n",
      "        [ 0.0000,  0.0032,  0.0010,  ..., -0.0010, -0.0001,  0.0008]],\n",
      "       device='cuda:0')\n",
      "binary_mask = tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True, False, False]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 1.1111, 0.0000, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [0.0000, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 0 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0009, -0.0036,  0.0076,  ...,  0.0025, -0.0031,  0.0070],\n",
      "        [-0.0008, -0.0030,  0.0058,  ...,  0.0020, -0.0014,  0.0074],\n",
      "        [-0.0007, -0.0047,  0.0050,  ...,  0.0032, -0.0003,  0.0054]],\n",
      "       device='cuda:0')\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0009, -0.0036,  0.0076,  ...,  0.0025, -0.0031,  0.0070],\n",
      "        [-0.0008, -0.0030,  0.0058,  ...,  0.0020, -0.0014,  0.0074],\n",
      "        [-0.0007, -0.0047,  0.0050,  ...,  0.0032, -0.0003,  0.0054]],\n",
      "       device='cuda:0')\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0063,  0.0096,  0.0008,  ...,  0.0005,  0.0013, -0.0064],\n",
      "        [-0.0066,  0.0088, -0.0003,  ...,  0.0015,  0.0017, -0.0066],\n",
      "        [-0.0076,  0.0087,  0.0013,  ...,  0.0028,  0.0010, -0.0070]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0008, -0.0034,  0.0077,  ...,  0.0024, -0.0033,  0.0078],\n",
      "        [ 0.0001, -0.0038,  0.0063,  ...,  0.0021, -0.0016,  0.0067],\n",
      "        [-0.0007, -0.0048,  0.0049,  ...,  0.0024, -0.0006,  0.0056]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0008, -0.0034,  0.0077,  ...,  0.0024, -0.0033,  0.0078],\n",
      "        [ 0.0001, -0.0038,  0.0063,  ...,  0.0021, -0.0016,  0.0067],\n",
      "        [-0.0007, -0.0048,  0.0049,  ...,  0.0024, -0.0006,  0.0056]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-6.6145e-03,  8.8310e-03, -1.1543e-03,  ...,  7.1113e-04,\n",
      "          7.3685e-05, -6.2144e-03],\n",
      "        [-4.6880e-03,  9.1676e-03,  6.4449e-04,  ...,  1.2440e-03,\n",
      "          1.7573e-03, -6.9086e-03],\n",
      "        [-7.9411e-03,  9.8134e-03,  1.8299e-03,  ...,  5.1148e-03,\n",
      "         -1.1815e-03, -5.1397e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0005, -0.0031,  0.0081,  ...,  0.0027, -0.0035,  0.0085],\n",
      "        [ 0.0011, -0.0047,  0.0068,  ...,  0.0025, -0.0018,  0.0065],\n",
      "        [-0.0006, -0.0052,  0.0048,  ...,  0.0019, -0.0009,  0.0059]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0005, -0.0031,  0.0081,  ...,  0.0027, -0.0035,  0.0085],\n",
      "        [ 0.0011, -0.0047,  0.0068,  ...,  0.0025, -0.0018,  0.0065],\n",
      "        [-0.0006, -0.0052,  0.0048,  ...,  0.0019, -0.0009,  0.0059]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0060,  0.0093,  0.0004,  ...,  0.0005, -0.0003, -0.0061],\n",
      "        [-0.0058,  0.0086,  0.0002,  ...,  0.0022,  0.0023, -0.0072],\n",
      "        [-0.0075,  0.0109,  0.0025,  ...,  0.0043, -0.0020, -0.0054]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0005, -0.0030,  0.0084,  ...,  0.0029, -0.0039,  0.0091],\n",
      "        [ 0.0017, -0.0053,  0.0073,  ...,  0.0027, -0.0022,  0.0067],\n",
      "        [-0.0005, -0.0054,  0.0048,  ...,  0.0017, -0.0009,  0.0061]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0005, -0.0030,  0.0084,  ...,  0.0029, -0.0039,  0.0091],\n",
      "        [ 0.0017, -0.0053,  0.0073,  ...,  0.0027, -0.0022,  0.0067],\n",
      "        [-0.0005, -0.0054,  0.0048,  ...,  0.0017, -0.0009,  0.0061]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0062,  0.0099,  0.0017,  ...,  0.0016, -0.0023, -0.0068],\n",
      "        [-0.0064,  0.0088,  0.0004,  ...,  0.0034,  0.0011, -0.0067],\n",
      "        [-0.0080,  0.0109,  0.0030,  ...,  0.0044, -0.0018, -0.0058]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0002, -0.0029,  0.0088,  ...,  0.0028, -0.0040,  0.0097],\n",
      "        [ 0.0020, -0.0058,  0.0077,  ...,  0.0030, -0.0026,  0.0070],\n",
      "        [-0.0004, -0.0057,  0.0048,  ...,  0.0016, -0.0012,  0.0065]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0002, -0.0029,  0.0088,  ...,  0.0028, -0.0040,  0.0097],\n",
      "        [ 0.0020, -0.0058,  0.0077,  ...,  0.0030, -0.0026,  0.0070],\n",
      "        [-0.0004, -0.0057,  0.0048,  ...,  0.0016, -0.0012,  0.0065]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0053,  0.0111,  0.0001,  ...,  0.0004, -0.0007, -0.0056],\n",
      "        [-0.0065,  0.0096,  0.0007,  ...,  0.0034,  0.0007, -0.0064],\n",
      "        [-0.0077,  0.0092,  0.0018,  ...,  0.0060,  0.0002, -0.0073]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0002, -0.0028,  0.0087,  ...,  0.0028, -0.0040,  0.0101],\n",
      "        [ 0.0022, -0.0064,  0.0080,  ...,  0.0033, -0.0030,  0.0071],\n",
      "        [-0.0005, -0.0059,  0.0047,  ...,  0.0016, -0.0015,  0.0069]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0002, -0.0028,  0.0087,  ...,  0.0028, -0.0040,  0.0101],\n",
      "        [ 0.0022, -0.0064,  0.0080,  ...,  0.0033, -0.0030,  0.0071],\n",
      "        [-0.0005, -0.0059,  0.0047,  ...,  0.0016, -0.0015,  0.0069]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0043,  0.0089,  0.0005,  ...,  0.0009,  0.0004, -0.0065],\n",
      "        [-0.0049,  0.0092, -0.0009,  ...,  0.0034,  0.0006, -0.0075],\n",
      "        [-0.0079,  0.0090,  0.0009,  ...,  0.0057,  0.0018, -0.0071]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0006, -0.0028,  0.0086,  ...,  0.0028, -0.0038,  0.0103],\n",
      "        [ 0.0022, -0.0067,  0.0082,  ...,  0.0036, -0.0031,  0.0071]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0006, -0.0028,  0.0086,  ...,  0.0028, -0.0038,  0.0103],\n",
      "        [ 0.0022, -0.0067,  0.0082,  ...,  0.0036, -0.0031,  0.0071]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-0.0049,  0.0085,  0.0009,  ...,  0.0020,  0.0009, -0.0059],\n",
      "        [-0.0061,  0.0091, -0.0014,  ...,  0.0037,  0.0020, -0.0077]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = True\n",
      "\t\t\t\tcurrent_length_index -= 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0006, -0.0027,  0.0085,  ...,  0.0029, -0.0039,  0.0107]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0006, -0.0027,  0.0085,  ...,  0.0029, -0.0039,  0.0107]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-4.2985e-03,  1.0303e-02,  8.7874e-04, -0.0000e+00, -5.5175e-03,\n",
      "          0.0000e+00, -8.3764e-03, -5.9984e-03,  3.2025e-03, -9.8587e-03,\n",
      "         -1.1393e-02, -5.1998e-03, -1.5748e-03,  5.2111e-03,  1.5943e-03,\n",
      "         -1.8450e-02,  0.0000e+00, -1.2531e-02,  1.9480e-03,  6.6313e-03,\n",
      "          3.2341e-03,  1.1532e-02, -7.9544e-03, -5.1439e-03, -2.7064e-03,\n",
      "         -4.9993e-03,  8.2859e-03,  1.2831e-03, -5.2819e-03, -5.6812e-03,\n",
      "         -1.8575e-02, -3.1830e-03, -2.6342e-03, -2.9080e-03, -8.4908e-04,\n",
      "         -1.2189e-03,  1.2715e-02,  1.3606e-03,  1.4880e-03,  0.0000e+00,\n",
      "          2.5992e-03, -1.1599e-02, -1.0900e-03,  6.6066e-03, -1.3148e-03,\n",
      "          4.2053e-03, -6.9499e-03,  0.0000e+00, -1.8255e-03,  4.9588e-03,\n",
      "          1.1117e-04, -3.3649e-03,  1.7918e-04, -1.1827e-02,  1.2100e-02,\n",
      "          8.3049e-03,  2.3648e-03,  0.0000e+00,  1.3552e-02, -6.2677e-04,\n",
      "         -9.0700e-03,  5.8874e-03, -2.2128e-02,  1.6407e-02, -1.3900e-03,\n",
      "          8.5490e-03,  2.0155e-03,  2.4603e-04, -7.4081e-04, -5.3483e-03,\n",
      "          4.2746e-03,  1.5211e-02,  7.0327e-04,  9.6394e-03,  1.1045e-02,\n",
      "         -1.2424e-03,  5.0359e-03, -9.5803e-03,  6.6450e-04, -3.5293e-03,\n",
      "         -5.2654e-03,  1.0259e-02, -4.3010e-03, -1.2716e-02, -3.6937e-03,\n",
      "          1.9517e-03, -6.6881e-03,  1.9642e-03, -1.1319e-02,  3.5108e-03,\n",
      "          4.7695e-03, -7.9577e-03, -7.0799e-04,  7.6776e-03, -1.1508e-03,\n",
      "         -7.9730e-03, -1.6697e-03, -6.5120e-03, -0.0000e+00, -3.9560e-03,\n",
      "          7.9183e-03, -1.7547e-03, -7.6275e-04,  2.4269e-03, -2.3304e-03,\n",
      "          1.7436e-02,  0.0000e+00,  5.1461e-03,  3.2677e-03,  6.0087e-04,\n",
      "         -1.6988e-03, -2.6879e-03, -9.9301e-03,  4.4684e-03, -1.4510e-02,\n",
      "          1.5228e-03,  5.7274e-03, -1.5717e-03, -4.4927e-04, -0.0000e+00,\n",
      "         -3.4263e-03, -1.0189e-02, -3.8054e-04,  7.2771e-03,  0.0000e+00,\n",
      "          6.4789e-03,  5.0093e-04,  9.9515e-03,  2.6346e-03, -1.6183e-02,\n",
      "         -2.5340e-03, -1.1336e-03, -2.5768e-03, -8.6440e-03,  4.9681e-03,\n",
      "          4.2806e-03,  1.1592e-02,  8.3482e-04, -6.3498e-03, -1.0369e-03,\n",
      "          8.4847e-04, -8.6995e-03,  4.1824e-03, -1.1843e-02,  1.0516e-03,\n",
      "         -8.6194e-03, -4.3855e-03,  5.6299e-03, -2.6962e-03,  9.3190e-03,\n",
      "         -4.3548e-03, -0.0000e+00, -3.5885e-04,  5.7900e-03, -2.7069e-03,\n",
      "          0.0000e+00, -6.7618e-03,  4.7574e-03,  3.2776e-03,  0.0000e+00,\n",
      "          6.2311e-04,  1.1843e-03, -3.2741e-03, -1.8836e-03, -3.0700e-03,\n",
      "         -1.7195e-03, -5.1693e-03,  3.8293e-03,  3.7237e-03, -1.2274e-02,\n",
      "          1.0226e-02,  2.9864e-04, -0.0000e+00, -1.8580e-03,  1.4688e-03,\n",
      "          9.7784e-03,  5.3699e-03,  2.8808e-03, -3.0833e-04, -8.0308e-03,\n",
      "         -6.0596e-03,  1.8496e-03, -1.2283e-02, -6.6389e-03, -6.2652e-03,\n",
      "          6.3091e-03,  9.4191e-04, -9.8136e-04, -1.0424e-03,  3.5837e-04,\n",
      "          2.1530e-03,  0.0000e+00,  1.5080e-03,  1.3564e-03,  9.9610e-03,\n",
      "          2.7501e-04,  1.3444e-02,  1.0215e-02, -0.0000e+00,  3.0950e-03,\n",
      "         -0.0000e+00, -1.6718e-03,  3.1321e-03,  1.7599e-03,  5.0652e-03,\n",
      "         -7.7974e-03, -2.4349e-03, -1.6784e-03,  2.1757e-03,  6.1475e-04,\n",
      "         -6.6711e-03,  3.0417e-03, -4.1980e-03,  1.0094e-03,  6.7566e-03,\n",
      "         -4.6375e-03,  1.3095e-02, -0.0000e+00, -1.0095e-02,  4.2931e-03,\n",
      "         -5.0019e-03,  6.7952e-03, -1.1271e-02, -4.0024e-03, -1.3114e-02,\n",
      "         -4.2069e-03,  6.0971e-03, -2.4235e-03,  0.0000e+00,  8.6191e-03,\n",
      "          3.3075e-03, -2.0269e-04,  3.9079e-03,  7.5152e-03, -6.4662e-04,\n",
      "          5.8690e-03, -0.0000e+00, -3.8820e-04, -1.4288e-02,  1.0628e-03,\n",
      "         -1.9746e-03, -1.7291e-03, -2.2480e-03, -1.8592e-03, -1.1047e-04,\n",
      "          1.8941e-03, -3.3100e-03, -5.1860e-03, -4.6670e-03,  3.2397e-03,\n",
      "          3.9653e-03, -4.4828e-03, -3.9746e-03, -6.7752e-03, -7.3968e-03,\n",
      "          2.0980e-03, -5.3376e-03, -4.0619e-03, -9.2258e-03, -9.4348e-03,\n",
      "          5.9470e-05, -0.0000e+00, -6.2568e-03,  2.0660e-03, -7.5437e-03,\n",
      "         -1.2879e-02, -9.3579e-03, -8.7519e-03,  6.8120e-03, -3.5941e-03,\n",
      "          1.0728e-02, -1.9226e-03, -1.2387e-02, -0.0000e+00,  3.7806e-03,\n",
      "         -9.4517e-03, -1.0200e-02, -1.0701e-02,  0.0000e+00, -1.2906e-02,\n",
      "          4.2473e-03,  2.0944e-03,  2.6340e-03,  8.2312e-03, -1.3902e-02,\n",
      "         -1.0576e-02,  1.3782e-02,  2.3041e-04,  8.0842e-03, -8.3526e-03,\n",
      "          7.2299e-05, -6.4064e-05,  1.0991e-02, -5.5814e-04, -3.0377e-03,\n",
      "         -1.1260e-02, -1.0845e-02, -1.9749e-03, -7.6565e-03, -4.9586e-03,\n",
      "          6.9334e-03, -2.4385e-04, -1.1032e-02, -1.8732e-03, -7.8170e-03,\n",
      "          2.0508e-03, -1.0065e-02, -9.9473e-03, -2.7110e-03,  8.1916e-03,\n",
      "          5.5822e-03, -2.2906e-03,  1.1122e-02,  2.1197e-03,  6.3424e-04,\n",
      "          1.4207e-03, -1.2737e-03,  6.4884e-03, -2.7736e-03,  4.2977e-03,\n",
      "          8.5654e-03, -4.2707e-03,  5.5976e-03, -5.0863e-03, -3.7947e-03,\n",
      "          9.6938e-03,  1.0374e-02,  3.2135e-03,  2.7673e-03, -3.9828e-03,\n",
      "         -1.4147e-04,  8.5876e-03, -8.3217e-03,  1.1764e-03,  3.3934e-03,\n",
      "          4.5288e-04,  1.1870e-02,  4.1836e-03,  1.2971e-02,  4.1165e-03,\n",
      "         -1.6908e-03, -1.7369e-02,  1.3987e-02, -3.0959e-04, -9.1092e-03,\n",
      "          3.7527e-03, -2.1722e-03, -4.4356e-03,  5.6139e-04,  1.2128e-02,\n",
      "          5.6973e-03, -5.7086e-03, -3.8477e-03,  4.3238e-03, -4.7931e-04,\n",
      "          3.1571e-03,  6.2060e-03,  3.4148e-03,  1.8396e-04, -7.1617e-03,\n",
      "          6.9864e-03, -1.7798e-03, -8.6298e-03, -1.2480e-02,  1.4529e-02,\n",
      "         -6.0485e-03,  5.2894e-03, -1.1500e-02,  5.5677e-03,  6.3974e-03,\n",
      "          8.6921e-03,  0.0000e+00, -9.6659e-03,  6.2410e-03, -7.2410e-03,\n",
      "          8.5380e-03,  6.2362e-03, -9.9816e-03, -8.1456e-03, -0.0000e+00,\n",
      "          0.0000e+00,  8.1330e-03,  6.2756e-03,  0.0000e+00,  1.3099e-02,\n",
      "          6.2291e-03,  4.3451e-03, -2.4107e-03, -2.3675e-03, -1.0175e-03,\n",
      "         -4.9987e-03,  3.1487e-04, -0.0000e+00,  3.4598e-03, -4.1050e-03,\n",
      "          1.0542e-02,  1.1579e-02, -9.9112e-03, -1.6453e-03,  1.6962e-03,\n",
      "         -2.5514e-03,  7.8842e-03,  3.8343e-03,  1.0905e-02, -0.0000e+00,\n",
      "         -7.9257e-03,  1.8092e-03, -3.3836e-03, -1.2194e-02,  0.0000e+00,\n",
      "         -6.8360e-03, -2.0876e-03, -7.5389e-04,  4.0540e-03, -3.0820e-03,\n",
      "         -6.7818e-03, -0.0000e+00,  6.1347e-03, -9.7541e-04,  2.8396e-04,\n",
      "         -2.8614e-03,  6.2687e-03, -1.2480e-02, -1.7092e-03, -8.7510e-03,\n",
      "         -2.4193e-03,  8.4999e-03,  0.0000e+00, -7.4076e-05,  7.6771e-03,\n",
      "          6.4094e-03,  6.5888e-03, -8.0292e-03, -1.2291e-02,  9.4127e-03,\n",
      "         -4.1965e-03, -6.8008e-04, -1.0913e-03, -1.7928e-02,  5.2971e-03,\n",
      "          3.2117e-03, -2.6362e-03,  8.5803e-03, -7.6560e-03, -3.2124e-03,\n",
      "          1.3211e-02,  8.1432e-04,  6.9385e-03, -2.4669e-03, -2.9482e-03,\n",
      "          8.2790e-03,  5.4895e-04, -9.9637e-03,  5.9293e-03,  5.0049e-03,\n",
      "         -9.6475e-03,  3.2990e-03,  1.3014e-03,  2.7350e-03, -2.4894e-03,\n",
      "         -2.4680e-03,  4.8017e-03,  2.4033e-03, -1.5501e-02, -1.8941e-02,\n",
      "         -1.5287e-03, -3.3203e-03, -8.2762e-04, -2.9426e-04, -1.1579e-02,\n",
      "          0.0000e+00, -9.7576e-03, -5.8943e-03,  8.3884e-05,  1.1619e-02,\n",
      "         -5.4341e-03,  4.6102e-03,  1.6819e-03,  4.3275e-03, -0.0000e+00,\n",
      "         -5.9584e-03, -1.7550e-02,  4.7440e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0768e-02, -8.4409e-03,  0.0000e+00, -2.2091e-03, -1.1243e-02,\n",
      "         -4.5582e-04,  1.0403e-02, -7.9578e-03, -7.9722e-03, -2.3840e-03,\n",
      "         -0.0000e+00, -1.0972e-02,  0.0000e+00, -1.0494e-02, -6.8521e-03,\n",
      "         -1.4139e-02,  1.1854e-02,  2.1235e-03, -3.7795e-03,  4.9609e-03,\n",
      "          8.3855e-03,  4.0641e-03,  2.7590e-03,  6.6160e-03,  8.8383e-04,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          5.9889e-04, -4.2547e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0006, -0.0027,  0.0083,  ...,  0.0032, -0.0040,  0.0109]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0006, -0.0027,  0.0083,  ...,  0.0032, -0.0040,  0.0109]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-4.2210e-03,  1.0837e-02,  1.3256e-03, -0.0000e+00, -2.8013e-03,\n",
      "          0.0000e+00, -7.8393e-03, -7.8627e-03,  4.8446e-03, -1.1081e-02,\n",
      "         -1.1005e-02, -6.1217e-03, -1.5110e-03,  4.6622e-03,  2.9732e-03,\n",
      "         -1.8683e-02,  0.0000e+00, -1.3954e-02,  2.4354e-03,  6.9518e-03,\n",
      "          2.2155e-03,  1.2884e-02, -8.1775e-03, -5.8422e-03, -3.5885e-03,\n",
      "         -6.9247e-03,  1.0661e-02,  6.2442e-04, -5.5987e-03, -6.3665e-03,\n",
      "         -1.9333e-02, -3.5743e-03, -1.0176e-03, -2.1559e-03, -1.7918e-03,\n",
      "         -1.1978e-03,  1.2388e-02,  3.0440e-03,  2.7393e-03,  0.0000e+00,\n",
      "          1.9675e-03, -1.3492e-02,  2.0616e-04,  4.9028e-03, -1.6371e-03,\n",
      "          3.6561e-03, -8.4670e-03,  0.0000e+00, -2.4021e-03,  5.3465e-03,\n",
      "          1.6457e-03, -4.4575e-03,  7.9564e-05, -1.1945e-02,  1.0218e-02,\n",
      "          1.0020e-02,  1.5888e-03,  0.0000e+00,  1.2650e-02, -7.3890e-04,\n",
      "         -8.4981e-03,  7.7284e-03, -2.5041e-02,  1.6822e-02, -4.2059e-03,\n",
      "          8.7297e-03,  2.3593e-03,  2.3714e-04, -1.9970e-03, -7.2128e-03,\n",
      "          4.5068e-03,  1.7270e-02,  5.4154e-04,  8.7529e-03,  1.2231e-02,\n",
      "         -2.9416e-03,  4.5749e-03, -8.3344e-03, -2.4084e-03, -1.7948e-03,\n",
      "         -5.2790e-03,  1.0353e-02, -4.4727e-03, -1.2373e-02, -4.2486e-03,\n",
      "          2.2916e-03, -6.7612e-03,  1.9911e-03, -1.1808e-02,  3.6068e-03,\n",
      "          4.2029e-03, -6.4737e-03, -1.5360e-04,  6.0082e-03, -8.3957e-04,\n",
      "         -6.5721e-03, -1.3901e-03, -7.0604e-03, -0.0000e+00, -4.1999e-03,\n",
      "          9.2185e-03, -1.8587e-03, -2.3826e-03,  4.6270e-03, -1.1524e-03,\n",
      "          1.7731e-02,  0.0000e+00,  5.4286e-03,  5.3595e-03,  1.0491e-03,\n",
      "         -1.7811e-03, -3.2528e-03, -9.7278e-03,  5.9792e-03, -1.5875e-02,\n",
      "          3.6051e-03,  6.7177e-03, -8.5916e-04, -5.6780e-04, -0.0000e+00,\n",
      "         -4.1981e-03, -1.1861e-02, -5.7156e-04,  7.0728e-03,  0.0000e+00,\n",
      "          4.6551e-03,  5.3359e-04,  1.0093e-02,  2.5537e-03, -1.6020e-02,\n",
      "         -1.3850e-03,  2.5680e-04, -2.6432e-03, -8.0716e-03,  4.0766e-03,\n",
      "          3.9321e-03,  1.2385e-02,  6.6674e-04, -5.7664e-03, -1.9997e-03,\n",
      "          1.1172e-03, -8.3818e-03,  6.1718e-03, -9.0416e-03,  1.9088e-03,\n",
      "         -8.1406e-03, -5.5948e-03,  5.1877e-03, -3.9109e-03,  1.0964e-02,\n",
      "         -5.9008e-03, -0.0000e+00, -2.9361e-04,  4.0488e-03, -1.5782e-03,\n",
      "          0.0000e+00, -6.2695e-03,  5.9042e-03,  4.3220e-03,  0.0000e+00,\n",
      "          2.1751e-03,  2.0898e-03, -2.7875e-03, -1.5972e-03, -5.0078e-03,\n",
      "         -1.9301e-03, -4.8166e-03,  3.6272e-03,  3.5531e-03, -1.4393e-02,\n",
      "          1.0896e-02,  2.3714e-03, -0.0000e+00, -3.0573e-03,  1.2037e-03,\n",
      "          1.1977e-02,  4.5703e-03,  3.8287e-03, -7.4166e-04, -9.7282e-03,\n",
      "         -8.4304e-03,  2.1715e-03, -1.2191e-02, -8.7621e-03, -6.8118e-03,\n",
      "          5.4753e-03,  9.7974e-04, -7.5839e-04,  3.1269e-04,  7.7488e-04,\n",
      "          1.9319e-03,  0.0000e+00,  4.7924e-04,  3.1287e-04,  8.5940e-03,\n",
      "         -9.2791e-04,  1.2319e-02,  1.0798e-02, -0.0000e+00,  4.4392e-03,\n",
      "         -0.0000e+00, -2.4505e-03,  3.3366e-05,  1.8391e-03,  5.2927e-03,\n",
      "         -9.4743e-03, -1.8614e-03, -7.6389e-04,  1.8065e-03,  3.2923e-04,\n",
      "         -8.2369e-03,  2.1216e-03, -3.5698e-03,  9.3016e-04,  6.2396e-03,\n",
      "         -4.3825e-03,  1.3499e-02, -0.0000e+00, -9.9319e-03,  4.5747e-03,\n",
      "         -7.1937e-03,  7.3926e-03, -1.2236e-02, -5.2687e-03, -1.3973e-02,\n",
      "         -4.5923e-03,  5.3597e-03, -4.4640e-03,  0.0000e+00,  9.3000e-03,\n",
      "          3.8658e-03, -5.7612e-04,  6.6057e-03,  7.1702e-03,  8.4525e-04,\n",
      "          6.1247e-03, -0.0000e+00, -3.7105e-04, -1.6391e-02,  2.4896e-03,\n",
      "         -8.0986e-04, -1.6379e-04, -2.5589e-03, -1.7985e-03,  5.9973e-04,\n",
      "          3.1432e-03, -3.4293e-03, -6.1209e-03, -6.2268e-03,  2.0815e-03,\n",
      "          4.9110e-03, -5.4448e-03, -3.2417e-03, -6.3420e-03, -5.3897e-03,\n",
      "          1.5460e-03, -4.6688e-03, -1.9977e-03, -1.1163e-02, -1.0102e-02,\n",
      "          1.8961e-03, -0.0000e+00, -4.8595e-03,  2.9794e-03, -9.2298e-03,\n",
      "         -1.2765e-02, -9.1658e-03, -9.9786e-03,  7.1397e-03, -4.8311e-03,\n",
      "          1.2656e-02, -1.8980e-04, -1.2572e-02, -0.0000e+00,  2.6416e-03,\n",
      "         -1.2189e-02, -9.6141e-03, -1.1133e-02,  0.0000e+00, -1.1706e-02,\n",
      "          3.2390e-03,  2.1907e-03,  2.0961e-03,  1.0872e-02, -1.2792e-02,\n",
      "         -1.0164e-02,  1.4539e-02,  1.8335e-03,  9.0649e-03, -1.0716e-02,\n",
      "          8.4940e-04, -2.2442e-03,  1.0967e-02, -1.1649e-03, -2.0485e-03,\n",
      "         -1.1717e-02, -1.1966e-02, -2.5660e-03, -8.3798e-03, -4.8335e-03,\n",
      "          8.2383e-03,  5.4580e-04, -1.0390e-02, -1.6567e-03, -6.3832e-03,\n",
      "          1.1233e-03, -1.0294e-02, -8.8167e-03, -1.7899e-03,  8.2172e-03,\n",
      "          8.0177e-03, -2.4741e-03,  1.2192e-02,  3.2664e-03,  2.0545e-04,\n",
      "          2.6201e-03, -2.4825e-03,  8.0021e-03, -3.0207e-03,  5.0332e-03,\n",
      "          1.0868e-02, -3.5969e-03,  4.9788e-03, -6.0230e-03, -4.9495e-03,\n",
      "          9.1041e-03,  9.8364e-03,  3.8725e-03,  1.7844e-03, -4.5436e-03,\n",
      "         -6.2472e-04,  9.0064e-03, -9.0496e-03,  1.7957e-03,  4.1549e-03,\n",
      "         -7.6888e-05,  1.0945e-02,  4.1025e-03,  1.3860e-02,  5.3681e-03,\n",
      "         -2.6510e-03, -1.6495e-02,  1.3319e-02, -2.3524e-03, -9.9985e-03,\n",
      "          3.2447e-03, -1.6623e-03, -4.0054e-03,  1.2833e-03,  1.1670e-02,\n",
      "          8.7288e-03, -5.4667e-03, -4.7259e-03,  5.0289e-03, -1.0092e-04,\n",
      "          2.5467e-03,  7.1112e-03,  3.4684e-03, -1.0198e-03, -8.2910e-03,\n",
      "          9.2164e-03, -2.1541e-03, -7.6008e-03, -1.0611e-02,  1.6514e-02,\n",
      "         -5.1327e-03,  5.2808e-03, -1.0854e-02,  7.0722e-03,  6.7405e-03,\n",
      "          8.1359e-03,  0.0000e+00, -1.1057e-02,  5.8559e-03, -7.2642e-03,\n",
      "          8.5875e-03,  7.3198e-03, -1.2254e-02, -6.9779e-03, -0.0000e+00,\n",
      "          0.0000e+00,  7.7093e-03,  8.7624e-03,  0.0000e+00,  1.2331e-02,\n",
      "          6.0245e-03,  4.3137e-03, -2.9651e-03, -1.5710e-03, -3.1020e-03,\n",
      "         -5.4448e-03,  4.5594e-04, -0.0000e+00,  4.0558e-03, -4.5859e-03,\n",
      "          1.1066e-02,  1.1629e-02, -8.1005e-03, -3.3792e-03,  1.8257e-03,\n",
      "         -2.0589e-03,  8.8570e-03,  4.2094e-03,  1.1294e-02, -0.0000e+00,\n",
      "         -1.0094e-02,  2.3091e-03, -4.2355e-03, -1.1887e-02,  0.0000e+00,\n",
      "         -5.9397e-03, -1.9422e-03, -1.4750e-03,  5.3497e-03, -3.4312e-03,\n",
      "         -6.3788e-03, -0.0000e+00,  6.1989e-03, -1.3532e-03,  8.0107e-04,\n",
      "         -4.3836e-03,  6.6429e-03, -1.2801e-02, -1.8865e-03, -1.0096e-02,\n",
      "         -2.9697e-03,  1.0146e-02,  0.0000e+00, -1.0414e-03,  9.5142e-03,\n",
      "          7.9544e-03,  6.3555e-03, -7.8217e-03, -1.3470e-02,  9.1859e-03,\n",
      "         -4.9979e-03, -1.5013e-03, -9.5383e-04, -1.9218e-02,  4.4135e-03,\n",
      "          4.3588e-03, -3.2334e-03,  9.0216e-03, -7.2257e-03, -3.8852e-03,\n",
      "          1.3523e-02,  1.9622e-03,  6.8204e-03, -4.6653e-03,  4.8274e-04,\n",
      "          9.3644e-03, -7.3144e-04, -1.1984e-02,  5.2409e-03,  5.2110e-03,\n",
      "         -8.9252e-03,  3.0498e-03,  1.7160e-03,  1.6092e-03, -2.8381e-03,\n",
      "         -1.2184e-03,  5.0420e-03,  2.7273e-03, -1.4905e-02, -1.8578e-02,\n",
      "         -5.8622e-04, -4.3679e-03, -1.4430e-03, -7.6729e-04, -1.2108e-02,\n",
      "          0.0000e+00, -9.8330e-03, -7.8950e-03, -8.9367e-04,  1.2410e-02,\n",
      "         -6.6076e-03,  4.9021e-03,  3.9439e-04,  5.5953e-03, -0.0000e+00,\n",
      "         -8.2334e-03, -1.7091e-02,  5.0088e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0510e-02, -1.0982e-02,  0.0000e+00, -3.2244e-03, -1.0743e-02,\n",
      "         -1.9531e-03,  1.1778e-02, -7.3533e-03, -7.2630e-03, -2.5206e-03,\n",
      "         -0.0000e+00, -1.1418e-02,  0.0000e+00, -1.2298e-02, -6.5549e-03,\n",
      "         -1.4585e-02,  1.3187e-02,  3.6221e-03, -2.6028e-03,  5.7320e-03,\n",
      "          8.4378e-03,  2.7279e-03,  3.7592e-03,  7.3454e-03,  7.2255e-04,\n",
      "         -1.3622e-03, -5.1824e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 9 since forward\n",
      "\t\t\t\tIn case forward\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[-0.0007, -0.0026,  0.0085,  ...,  0.0035, -0.0038,  0.0113]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[-0.0007, -0.0026,  0.0085,  ...,  0.0035, -0.0038,  0.0113]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[-5.5796e-03,  1.0460e-02,  5.4435e-04, -0.0000e+00, -4.8331e-03,\n",
      "          0.0000e+00, -7.1736e-03, -6.8142e-03,  4.0699e-03, -1.0938e-02,\n",
      "         -1.2264e-02, -5.0463e-03, -8.8614e-04,  5.2345e-03,  2.4756e-03,\n",
      "         -1.6792e-02,  0.0000e+00, -1.1406e-02,  3.1902e-03,  6.7269e-03,\n",
      "          1.9136e-03,  1.2479e-02, -9.0404e-03, -5.0567e-03, -2.4727e-03,\n",
      "         -5.2487e-03,  9.9181e-03,  2.5713e-03, -5.6663e-03, -8.9831e-03,\n",
      "         -1.7355e-02, -2.8050e-03, -1.3471e-03, -2.0385e-03, -2.0011e-03,\n",
      "         -9.5837e-04,  1.2212e-02,  3.4444e-03,  1.6961e-03,  0.0000e+00,\n",
      "          1.7138e-03, -1.2503e-02, -2.7501e-04,  3.4484e-03, -2.5320e-03,\n",
      "          4.7042e-03, -6.9151e-03,  0.0000e+00, -3.4041e-03,  4.9778e-03,\n",
      "          1.1350e-03, -3.8574e-03, -6.8576e-04, -1.2194e-02,  1.1792e-02,\n",
      "          9.9790e-03,  3.2995e-03,  0.0000e+00,  1.3654e-02, -2.0560e-03,\n",
      "         -9.2152e-03,  7.2412e-03, -2.4399e-02,  1.6258e-02, -3.4699e-03,\n",
      "          7.8095e-03,  2.1095e-03,  1.3609e-03, -2.1517e-03, -8.2764e-03,\n",
      "          2.7143e-03,  1.6547e-02, -2.6513e-04,  9.9164e-03,  1.1777e-02,\n",
      "         -1.3171e-03,  4.5242e-03, -8.7496e-03, -1.8396e-03, -1.8150e-03,\n",
      "         -5.1259e-03,  1.0977e-02, -4.3331e-03, -1.2614e-02, -4.1917e-03,\n",
      "          1.1472e-03, -6.7697e-03,  2.0150e-03, -1.1694e-02,  2.6069e-03,\n",
      "          4.6437e-03, -6.3520e-03, -4.4422e-05,  5.7107e-03, -1.2740e-03,\n",
      "         -7.5685e-03, -2.6187e-03, -5.9577e-03, -0.0000e+00, -4.0294e-03,\n",
      "          8.6554e-03, -1.8665e-03, -2.6262e-03,  3.8489e-03, -1.7566e-03,\n",
      "          1.6665e-02, -0.0000e+00,  4.1910e-03,  4.7001e-03, -7.7736e-04,\n",
      "         -8.4547e-04, -1.5418e-03, -1.0325e-02,  4.8653e-03, -1.5467e-02,\n",
      "          3.6836e-03,  7.0757e-03,  8.6749e-04,  1.1705e-03, -0.0000e+00,\n",
      "         -3.6577e-03, -1.1720e-02, -5.7189e-04,  7.5733e-03,  0.0000e+00,\n",
      "          4.3710e-03,  2.7940e-03,  1.0439e-02,  3.2387e-04, -1.6253e-02,\n",
      "         -1.4838e-03,  5.7921e-04, -3.0182e-03, -7.3177e-03,  5.0067e-03,\n",
      "          3.8710e-03,  1.1041e-02,  1.4959e-04, -4.4467e-03, -1.8669e-03,\n",
      "          1.1686e-03, -9.0094e-03,  4.4288e-03, -8.4598e-03,  1.4745e-03,\n",
      "         -6.9470e-03, -3.9278e-03,  6.4836e-03, -4.1292e-03,  9.5805e-03,\n",
      "         -4.3890e-03, -0.0000e+00,  1.4866e-03,  3.2354e-03, -3.1838e-04,\n",
      "          0.0000e+00, -6.3111e-03,  5.5652e-03,  5.5999e-03,  0.0000e+00,\n",
      "          2.1643e-03,  3.0649e-03, -1.3794e-03, -2.8244e-04, -2.5176e-03,\n",
      "         -3.0789e-03, -5.4491e-03,  2.5127e-03,  4.7325e-03, -1.5918e-02,\n",
      "          1.1300e-02,  1.7182e-03,  0.0000e+00, -2.8600e-03,  2.1505e-03,\n",
      "          1.2621e-02,  3.8506e-03,  2.1236e-03,  2.7465e-04, -9.2490e-03,\n",
      "         -6.9116e-03,  1.8253e-03, -1.0587e-02, -8.1748e-03, -6.7762e-03,\n",
      "          6.9015e-03,  1.7097e-03, -1.0995e-03, -1.9923e-03,  5.6320e-04,\n",
      "          2.6141e-03,  0.0000e+00,  3.1546e-04, -8.4781e-05,  8.3663e-03,\n",
      "         -8.6079e-05,  1.1705e-02,  9.5119e-03, -0.0000e+00,  5.5999e-03,\n",
      "         -0.0000e+00, -4.0271e-03,  2.6820e-03,  1.4564e-03,  5.7858e-03,\n",
      "         -8.2715e-03, -1.8544e-03, -1.1628e-03,  2.6769e-03,  7.4087e-04,\n",
      "         -8.8905e-03,  3.2713e-03, -5.0804e-03,  5.4179e-04,  5.8417e-03,\n",
      "         -4.0462e-03,  1.3006e-02, -0.0000e+00, -1.0704e-02,  5.3381e-03,\n",
      "         -5.7136e-03,  5.3037e-03, -1.1947e-02, -5.1200e-03, -1.3586e-02,\n",
      "         -4.7851e-03,  5.6883e-03, -2.6203e-03,  0.0000e+00,  9.6437e-03,\n",
      "          2.5158e-03, -1.7562e-04,  5.6804e-03,  6.1115e-03, -2.5566e-04,\n",
      "          5.6962e-03, -0.0000e+00, -1.0877e-03, -1.6351e-02,  2.4513e-03,\n",
      "         -2.0046e-03,  1.1486e-03, -1.2549e-03, -3.4059e-03,  8.7216e-04,\n",
      "          3.1113e-03, -3.3526e-03, -5.0408e-03, -6.2923e-03,  3.6841e-03,\n",
      "          2.4492e-03, -3.5961e-03, -3.2359e-03, -6.4689e-03, -5.4796e-03,\n",
      "          1.4849e-03, -4.9030e-03, -1.4007e-03, -8.9654e-03, -1.1268e-02,\n",
      "          1.8926e-03, -0.0000e+00, -6.2652e-03,  3.6295e-03, -9.0573e-03,\n",
      "         -1.1192e-02, -8.2720e-03, -9.2112e-03,  6.0264e-03, -3.7847e-03,\n",
      "          1.0140e-02, -6.4599e-04, -1.1400e-02, -0.0000e+00,  2.2621e-03,\n",
      "         -9.7050e-03, -7.9075e-03, -1.2767e-02,  0.0000e+00, -1.1037e-02,\n",
      "          4.3845e-03,  2.4956e-03,  1.7314e-03,  1.0654e-02, -1.3661e-02,\n",
      "         -1.0066e-02,  1.5097e-02,  1.5294e-03,  8.3298e-03, -9.6031e-03,\n",
      "          8.4579e-04, -1.9302e-03,  9.8618e-03, -2.1974e-04, -2.1797e-03,\n",
      "         -1.1152e-02, -1.1497e-02, -7.8611e-04, -7.5757e-03, -4.5053e-03,\n",
      "          6.5100e-03,  1.3283e-03, -1.1798e-02, -5.0427e-04, -5.3085e-03,\n",
      "          2.6004e-03, -9.8604e-03, -7.7613e-03, -2.1455e-03,  7.6811e-03,\n",
      "          5.7236e-03, -2.3550e-03,  1.1872e-02,  4.2448e-03,  2.1839e-03,\n",
      "          1.8758e-03, -1.9539e-03,  8.5398e-03, -3.6156e-03,  6.0452e-03,\n",
      "          9.4660e-03, -4.9938e-03,  5.6927e-03, -4.3404e-03, -5.9978e-03,\n",
      "          7.2392e-03,  9.6808e-03,  2.9880e-03,  1.7509e-03, -3.9106e-03,\n",
      "         -5.1742e-04,  9.0774e-03, -7.8063e-03,  1.7761e-03,  4.2863e-03,\n",
      "          4.3841e-04,  1.0196e-02,  3.7975e-03,  1.4147e-02,  6.3726e-03,\n",
      "         -2.3008e-03, -1.6276e-02,  1.5173e-02, -1.8664e-03, -9.5056e-03,\n",
      "          2.6951e-03, -2.2267e-03, -3.3493e-03,  2.0967e-04,  1.0773e-02,\n",
      "          7.4352e-03, -4.9726e-03, -4.9856e-03,  4.3250e-03, -4.9915e-04,\n",
      "          2.3304e-03,  7.1852e-03,  3.6089e-03,  1.2383e-03, -7.7251e-03,\n",
      "          8.9394e-03, -2.1008e-03, -8.1330e-03, -1.0479e-02,  1.5138e-02,\n",
      "         -5.2648e-03,  6.0115e-03, -1.0196e-02,  5.3268e-03,  6.0538e-03,\n",
      "          7.7646e-03,  0.0000e+00, -1.0503e-02,  7.4796e-03, -7.6565e-03,\n",
      "          8.3477e-03,  8.7286e-03, -1.1664e-02, -6.1995e-03, -0.0000e+00,\n",
      "          0.0000e+00,  7.2184e-03,  7.0186e-03,  0.0000e+00,  1.3583e-02,\n",
      "          4.7430e-03,  4.0686e-03, -3.2133e-03, -1.8953e-03, -3.7983e-03,\n",
      "         -4.5577e-03,  2.3583e-03, -0.0000e+00,  3.8326e-03, -5.1052e-03,\n",
      "          1.1782e-02,  1.1862e-02, -6.8434e-03, -2.4780e-03,  1.7725e-03,\n",
      "         -3.2807e-03,  8.0293e-03,  4.2059e-03,  1.1753e-02, -0.0000e+00,\n",
      "         -9.4846e-03,  1.4537e-03, -3.1028e-03, -1.1421e-02,  0.0000e+00,\n",
      "         -6.5290e-03, -4.6922e-03, -1.4199e-03,  5.5799e-03, -3.2511e-03,\n",
      "         -8.6927e-03, -0.0000e+00,  4.4973e-03, -1.2926e-03,  8.5871e-04,\n",
      "         -3.3861e-03,  6.8335e-03, -1.4042e-02, -1.6986e-03, -1.1232e-02,\n",
      "         -1.0900e-03,  9.7503e-03,  0.0000e+00, -1.7006e-03,  9.8681e-03,\n",
      "          6.2117e-03,  6.1906e-03, -6.5619e-03, -1.3211e-02,  8.4158e-03,\n",
      "         -5.8137e-03,  1.0136e-03, -9.4727e-04, -1.9240e-02,  2.9049e-03,\n",
      "          4.2738e-03, -4.2922e-03,  7.7434e-03, -7.0122e-03, -2.6048e-03,\n",
      "          1.2378e-02,  1.8210e-04,  7.7566e-03, -2.8613e-03, -1.0938e-03,\n",
      "          8.0454e-03,  5.0936e-04, -1.1754e-02,  5.2903e-03,  4.7322e-03,\n",
      "         -8.3082e-03,  3.2853e-03,  9.1608e-04,  1.5020e-03, -3.2914e-03,\n",
      "         -3.7379e-03,  7.1844e-03,  3.1453e-03, -1.4522e-02, -1.7661e-02,\n",
      "         -1.8941e-04, -4.1997e-03, -8.2730e-04,  1.4925e-03, -1.2132e-02,\n",
      "          0.0000e+00, -8.4128e-03, -4.3187e-03,  4.8235e-04,  1.1391e-02,\n",
      "         -6.8280e-03,  4.6393e-03, -2.7850e-04,  4.4424e-03, -0.0000e+00,\n",
      "         -8.2083e-03, -1.5163e-02,  5.6322e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0688e-02, -1.0525e-02,  0.0000e+00, -2.2694e-03, -9.9894e-03,\n",
      "         -1.4631e-03,  1.1880e-02, -6.0767e-03, -7.2082e-03, -2.4680e-03,\n",
      "         -0.0000e+00, -9.6652e-03,  0.0000e+00, -1.1221e-02, -5.6888e-03,\n",
      "         -1.4112e-02,  1.1102e-02,  2.9294e-03, -2.1290e-03,  6.5831e-03,\n",
      "          8.2249e-03,  3.1261e-03,  3.2505e-03,  7.3558e-03,  1.3763e-03,\n",
      "          6.0025e-04, -5.9503e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[ 3.6805e-03,  4.7124e-03,  2.7107e-03,  ..., -1.0836e-03,\n",
      "           0.0000e+00,  1.5043e-03],\n",
      "         [ 2.7964e-03,  4.5057e-03,  2.4832e-03,  ..., -1.1261e-03,\n",
      "          -8.8399e-05,  1.7326e-03],\n",
      "         [ 0.0000e+00,  4.5184e-03,  1.0823e-03,  ..., -1.0310e-03,\n",
      "           1.4377e-04,  1.9739e-03]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[-0.0008, -0.0024,  0.0087,  ...,  0.0035, -0.0037,  0.0115],\n",
      "         [ 0.0023, -0.0068,  0.0084,  ...,  0.0034, -0.0032,  0.0071],\n",
      "         [-0.0005, -0.0058,  0.0048,  ...,  0.0014, -0.0017,  0.0071]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tRUN backward_layer.forward method...\n",
      "\t\t\tinputs.size() = torch.Size([3, 10, 512])\n",
      "\t\t\tUnpacking batch_size, total_timesteps = inputs.size()\n",
      "\t\t\tbatch_size = 3, total_timesteps = 10\n",
      "\t\t\tCreate tensor(output_accumulator) which has (3, 10, 512) shape, filling 0.\n",
      "\t\t\tis `initial_state` is None? False\n",
      "\t\t\t\tOk, Using `initial_state`, create full_batch_previous memory and state.\n",
      "\t\t\t\t(previous_state) = initial_state[0] = tensor([[[-1.5158e-03, -5.0278e-04, -7.7537e-04,  ...,  6.3730e-05,\n",
      "           8.1574e-04,  0.0000e+00],\n",
      "         [-1.1270e-03, -3.8583e-04, -3.5395e-04,  ..., -6.9853e-05,\n",
      "           6.9651e-04,  1.1251e-04],\n",
      "         [-6.2726e-04, -0.0000e+00, -6.4494e-04,  ..., -6.1530e-04,\n",
      "           5.4886e-04,  3.3248e-04]]], device='cuda:0')\n",
      "\t\t\t\tfull_batch_previous_state = initial_state[0].squeeze(0) = tensor([[-1.5158e-03, -5.0278e-04, -7.7537e-04,  ...,  6.3730e-05,\n",
      "          8.1574e-04,  0.0000e+00],\n",
      "        [-1.1270e-03, -3.8583e-04, -3.5395e-04,  ..., -6.9853e-05,\n",
      "          6.9651e-04,  1.1251e-04],\n",
      "        [-6.2726e-04, -0.0000e+00, -6.4494e-04,  ..., -6.1530e-04,\n",
      "          5.4886e-04,  3.3248e-04]], device='cuda:0')\n",
      "\t\t\t\t(previous_memory) = initial_state[1] = tensor([[[ 2.6878e-03, -4.7545e-03, -3.0655e-05,  ...,  1.8927e-03,\n",
      "          -3.5848e-03, -1.0290e-03],\n",
      "         [ 1.7130e-03, -3.0089e-03,  9.5765e-05,  ...,  9.0395e-04,\n",
      "          -3.5894e-05, -1.9414e-03],\n",
      "         [ 2.4044e-04, -3.7126e-03, -1.3427e-04,  ...,  7.0461e-04,\n",
      "          -8.4922e-04,  3.3241e-04]]], device='cuda:0')\n",
      "\t\t\t\tfull_batch_previous_memory = initial_state[1].squeeze(0) = tensor([[ 2.6878e-03, -4.7545e-03, -3.0655e-05,  ...,  1.8927e-03,\n",
      "         -3.5848e-03, -1.0290e-03],\n",
      "        [ 1.7130e-03, -3.0089e-03,  9.5765e-05,  ...,  9.0395e-04,\n",
      "         -3.5894e-05, -1.9414e-03],\n",
      "        [ 2.4044e-04, -3.7126e-03, -1.3427e-04,  ...,  7.0461e-04,\n",
      "         -8.4922e-04,  3.3241e-04]], device='cuda:0')\n",
      "\t\t\t\tSet current_length_index... is it forward?? False\n",
      "\t\t\t\tOops, backward!! current_length_index = 0\n",
      "\t\t\t\tis recurrent_dropout_probability is larger than 0? True\n",
      "\t\t\t\tand is training? True\n",
      "\t\t\t\tok, both is True. Execute `get_dropout_mask` function! using full_abtch_previous_state!\n",
      "*-*** get_dropout_mask ***-*\n",
      "binary_mask tensor([[-1.5158e-03, -5.0278e-04, -7.7537e-04,  ...,  6.3730e-05,\n",
      "          8.1574e-04,  0.0000e+00],\n",
      "        [-1.1270e-03, -3.8583e-04, -3.5395e-04,  ..., -6.9853e-05,\n",
      "          6.9651e-04,  1.1251e-04],\n",
      "        [-6.2726e-04, -0.0000e+00, -6.4494e-04,  ..., -6.1530e-04,\n",
      "          5.4886e-04,  3.3248e-04]], device='cuda:0')\n",
      "binary_mask = tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n",
      "Calc 1.0 / (1 - p) or 0.0\n",
      "dropout_mask = tensor([[1.1111, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 1.1111],\n",
      "        [1.1111, 1.1111, 1.1111,  ..., 0.0000, 1.1111, 1.1111],\n",
      "        [0.0000, 1.1111, 1.1111,  ..., 1.1111, 1.1111, 0.0000]],\n",
      "       device='cuda:0')\n",
      "*-*** ---------------- ***-*\n",
      "\t\t\t\tStarting Loops with 10...\n",
      "\t\t\t\tindex = 9 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 2.6878e-03, -4.7545e-03, -3.0655e-05,  ...,  1.8927e-03,\n",
      "         -3.5848e-03, -1.0290e-03]], device='cuda:0')\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 2.6878e-03, -4.7545e-03, -3.0655e-05,  ...,  1.8927e-03,\n",
      "         -3.5848e-03, -1.0290e-03]], device='cuda:0')\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 8.3182e-03,  4.7178e-03, -9.4549e-04,  1.1550e-02,  9.4521e-03,\n",
      "          5.0806e-03,  9.1869e-03, -2.1874e-03,  1.2145e-04, -7.2220e-03,\n",
      "         -7.4796e-03,  5.0960e-03,  6.5411e-03, -1.1588e-02,  0.0000e+00,\n",
      "         -7.3711e-03,  0.0000e+00, -4.4700e-03, -2.8487e-03,  5.1425e-03,\n",
      "         -2.0970e-03,  2.6271e-03,  1.4989e-03, -1.0151e-03, -0.0000e+00,\n",
      "          8.3696e-03, -2.6506e-03,  4.4632e-03, -1.3683e-03,  3.6133e-03,\n",
      "         -3.2928e-03, -0.0000e+00,  6.2872e-03, -9.8900e-03,  7.7082e-03,\n",
      "         -1.0024e-02,  5.3957e-03,  3.4043e-03,  4.1692e-03, -3.8985e-03,\n",
      "          5.2595e-03,  1.1838e-02, -8.4565e-03,  9.8172e-03, -1.1644e-02,\n",
      "          1.2191e-02, -7.5877e-03, -1.1708e-03, -3.5505e-03,  3.4189e-03,\n",
      "          1.3485e-03, -9.6814e-03, -9.5625e-04, -6.7155e-03,  0.0000e+00,\n",
      "          4.0466e-03, -1.1278e-02, -5.8526e-03, -1.6070e-03, -7.7668e-04,\n",
      "          1.7523e-02,  1.1177e-03,  4.8181e-03,  3.1261e-03, -1.1466e-03,\n",
      "          6.1564e-04,  1.7175e-03,  6.1771e-03, -3.0263e-03, -9.2213e-04,\n",
      "          1.0622e-02,  4.5037e-03, -3.1870e-03,  4.2421e-03, -4.4243e-03,\n",
      "          0.0000e+00,  0.0000e+00,  1.0899e-04,  8.1095e-04,  1.0898e-03,\n",
      "         -7.9842e-04, -5.1645e-03,  0.0000e+00, -1.0597e-02, -1.9273e-03,\n",
      "          3.9795e-03,  1.5832e-02, -4.4279e-04, -1.7352e-03, -1.2512e-02,\n",
      "         -9.9757e-03, -9.2173e-03,  0.0000e+00,  4.6479e-03, -8.5378e-03,\n",
      "         -1.2363e-02,  7.2827e-03, -1.1955e-03,  6.1217e-03,  4.0914e-03,\n",
      "         -4.6102e-03,  4.0980e-03,  1.7728e-03, -2.8872e-04, -1.1016e-03,\n",
      "          8.5414e-03,  0.0000e+00,  6.2771e-03,  0.0000e+00,  1.4762e-03,\n",
      "         -4.3369e-04,  0.0000e+00,  3.6091e-03, -1.2819e-03,  4.7862e-03,\n",
      "          6.9306e-03,  1.4891e-03,  0.0000e+00,  2.4578e-03, -1.1104e-02,\n",
      "         -1.6133e-04, -9.8167e-03, -4.6021e-03, -2.6012e-03,  1.2153e-02,\n",
      "         -9.6971e-03, -1.2933e-02, -1.0219e-02,  6.8289e-03,  6.3190e-03,\n",
      "          8.2391e-04, -2.0543e-03,  9.4466e-04,  0.0000e+00,  0.0000e+00,\n",
      "          2.4739e-03,  9.0725e-03, -0.0000e+00, -3.6155e-03, -9.8092e-03,\n",
      "          2.7374e-03,  2.6261e-03, -2.6828e-03, -3.5530e-03,  4.5969e-03,\n",
      "          9.2476e-03,  7.1269e-03,  4.5935e-03,  8.1534e-03, -9.0054e-04,\n",
      "          4.6887e-03, -4.7473e-03, -1.2999e-03,  5.1157e-03, -1.5093e-03,\n",
      "          1.1142e-03,  6.3951e-03,  5.5570e-03, -8.3716e-04,  1.0893e-02,\n",
      "          1.1100e-02,  2.7091e-03, -5.4827e-03,  2.4211e-04, -2.0566e-03,\n",
      "          9.2933e-03,  2.9655e-04,  1.1698e-02, -1.0861e-03,  6.7391e-03,\n",
      "          5.1712e-03, -8.5704e-03, -4.7626e-03, -5.4334e-03,  3.4845e-03,\n",
      "         -1.8258e-03,  1.0020e-02, -6.9406e-03, -6.4622e-03, -2.7723e-03,\n",
      "          1.2836e-04,  1.1162e-02,  1.0001e-02, -1.7571e-03,  2.3911e-03,\n",
      "         -3.9191e-03,  5.9401e-03,  8.7550e-03,  7.9066e-03,  1.3312e-02,\n",
      "         -2.6626e-03,  7.3923e-03, -4.3582e-04,  7.8673e-03, -6.1836e-03,\n",
      "         -1.6173e-02, -0.0000e+00,  1.6473e-02, -1.3377e-02,  1.0734e-02,\n",
      "          0.0000e+00,  2.5866e-03,  9.2934e-03,  4.6765e-03,  2.5966e-03,\n",
      "          6.2952e-03, -4.5980e-03,  6.1417e-03, -5.7414e-03,  1.2992e-02,\n",
      "         -7.6613e-03,  4.8148e-03, -1.6270e-03, -8.9125e-03,  1.9527e-03,\n",
      "         -7.1088e-03, -3.1732e-03,  8.5814e-03,  8.6690e-04, -4.6524e-03,\n",
      "         -7.7901e-03, -8.1673e-04,  1.0213e-02,  1.3231e-02,  4.3343e-03,\n",
      "         -4.6625e-03,  0.0000e+00, -5.8844e-03, -5.0323e-03,  5.2443e-03,\n",
      "          2.0456e-03,  8.7974e-03,  4.9113e-03, -6.5305e-03, -1.0145e-02,\n",
      "          1.2706e-02,  4.7810e-03, -4.0597e-04,  7.0288e-03,  0.0000e+00,\n",
      "          8.8709e-03,  9.1450e-03, -7.7642e-03, -0.0000e+00,  1.0399e-02,\n",
      "          1.8313e-03,  1.1010e-02, -7.4871e-03, -3.8145e-03, -7.7183e-03,\n",
      "         -0.0000e+00,  2.4073e-03,  1.1801e-02,  1.0202e-02, -2.8608e-03,\n",
      "         -5.4775e-03, -0.0000e+00,  6.8244e-03, -1.7665e-03, -5.4118e-03,\n",
      "         -7.1513e-03, -6.4545e-03, -0.0000e+00, -2.6188e-03, -2.6452e-03,\n",
      "         -6.5646e-03, -2.5509e-03, -2.8486e-03,  3.0639e-03,  1.4189e-02,\n",
      "         -6.2000e-03, -8.1448e-04, -0.0000e+00, -2.1797e-03,  3.6241e-03,\n",
      "          0.0000e+00,  2.6278e-03,  0.0000e+00,  2.2682e-03,  0.0000e+00,\n",
      "         -1.5351e-02,  1.3229e-02,  0.0000e+00,  3.2168e-03,  0.0000e+00,\n",
      "          2.2666e-04, -1.8211e-03, -1.5464e-02, -2.9332e-03, -1.3491e-02,\n",
      "         -4.1234e-03,  4.5275e-03, -0.0000e+00,  6.1161e-04,  1.0643e-02,\n",
      "         -5.4480e-03,  4.3711e-03,  5.1373e-03, -5.2612e-03, -2.3882e-03,\n",
      "         -2.6434e-05, -1.3376e-02, -7.2133e-03,  2.3836e-03, -4.3832e-03,\n",
      "         -9.6495e-04, -1.2542e-03,  3.2248e-03,  4.1283e-03, -4.0404e-03,\n",
      "          5.4429e-04, -1.6287e-02,  0.0000e+00, -3.6840e-03,  3.4960e-03,\n",
      "          1.3835e-02, -1.4834e-03,  1.2861e-02, -0.0000e+00,  8.8386e-03,\n",
      "          3.5063e-03,  0.0000e+00,  2.9265e-03,  5.4649e-04, -2.2510e-03,\n",
      "         -5.4137e-03, -2.2676e-03,  2.8849e-04, -2.8800e-04, -5.4254e-03,\n",
      "         -1.1278e-02,  6.9656e-03, -2.9890e-05,  1.3644e-02,  9.1334e-03,\n",
      "         -1.3229e-03, -0.0000e+00, -5.9125e-04, -3.3258e-04, -3.7868e-03,\n",
      "         -8.4642e-06,  6.6916e-03, -2.3261e-03,  5.8335e-03, -1.8414e-03,\n",
      "          0.0000e+00, -0.0000e+00,  6.7773e-03, -1.1707e-02,  7.3350e-03,\n",
      "         -1.2863e-02,  5.6961e-03,  2.2999e-03, -1.0234e-02, -4.6585e-03,\n",
      "          1.1896e-02, -4.1227e-03,  4.4824e-03, -2.3100e-03,  6.5665e-03,\n",
      "         -5.6212e-03, -8.4714e-03, -4.9580e-03,  5.2023e-03, -6.8909e-03,\n",
      "         -7.1585e-03, -4.1747e-04, -4.4330e-03,  0.0000e+00,  8.7386e-03,\n",
      "         -7.2468e-03,  4.2143e-03,  1.3338e-02,  5.2564e-03,  0.0000e+00,\n",
      "         -1.0095e-02, -1.3359e-02, -7.3777e-04,  1.1745e-02,  0.0000e+00,\n",
      "          4.0611e-03,  9.5791e-03, -7.4365e-03,  1.4335e-02, -1.9776e-03,\n",
      "          7.2523e-04,  4.8313e-03, -7.9936e-03, -8.6759e-03,  5.0550e-03,\n",
      "          4.0956e-03, -4.2084e-04,  0.0000e+00,  7.4767e-03, -4.2737e-05,\n",
      "         -4.6620e-03, -1.2123e-03, -6.5424e-03,  0.0000e+00, -3.7828e-03,\n",
      "          4.2589e-03,  9.6066e-04,  0.0000e+00, -7.1666e-03,  0.0000e+00,\n",
      "         -9.9309e-03,  7.6170e-03, -0.0000e+00,  8.7055e-03,  0.0000e+00,\n",
      "          1.3398e-03, -3.7059e-03, -0.0000e+00,  4.8096e-03, -3.3862e-03,\n",
      "         -1.4424e-03,  6.5419e-03, -7.0621e-03, -4.9498e-03,  2.4759e-03,\n",
      "          2.4529e-03, -2.0054e-03,  9.3424e-03,  9.3592e-04, -4.9297e-03,\n",
      "          6.5526e-03, -1.2895e-02, -2.9012e-03, -3.9031e-03,  2.0578e-03,\n",
      "         -0.0000e+00,  4.7322e-03,  2.2525e-03,  0.0000e+00,  5.5454e-05,\n",
      "         -5.3615e-03,  8.7102e-04, -0.0000e+00,  3.5964e-03, -4.5277e-03,\n",
      "         -2.6973e-03,  9.3011e-03,  8.6167e-03, -8.9711e-04,  4.6253e-03,\n",
      "         -2.3337e-03,  4.9327e-03,  6.2651e-03,  8.5538e-03,  7.9093e-03,\n",
      "         -1.0811e-02,  1.6427e-02,  6.2017e-03, -5.3318e-03,  2.4839e-03,\n",
      "         -8.3664e-03, -2.7943e-04, -5.0685e-04, -1.1632e-02,  3.9668e-03,\n",
      "         -7.5468e-03,  9.8324e-03, -2.1276e-02, -5.7066e-03,  6.2534e-03,\n",
      "          2.7478e-03, -3.9954e-04, -6.3196e-03,  3.0488e-03, -4.1202e-03,\n",
      "         -1.1864e-02, -3.5547e-03, -2.2989e-03,  9.9665e-03, -6.3493e-03,\n",
      "         -1.0740e-02,  2.5658e-03,  2.7630e-03,  1.1305e-02, -1.2593e-02,\n",
      "         -6.3171e-03,  2.4219e-03,  4.8158e-03,  6.3519e-03, -6.1502e-03,\n",
      "          5.2661e-03, -5.7774e-03, -1.5109e-02,  5.3525e-03,  1.3474e-03,\n",
      "         -2.5252e-03,  1.0806e-02,  0.0000e+00, -8.6097e-03, -6.7721e-03,\n",
      "          4.9422e-03,  1.0303e-03, -0.0000e+00, -1.3396e-02,  1.0829e-02,\n",
      "          0.0000e+00, -0.0000e+00, -2.0671e-03, -7.2085e-03, -8.5780e-03,\n",
      "          1.9226e-03,  1.0994e-02,  1.9663e-03,  1.4440e-04, -3.4830e-03,\n",
      "          9.3001e-03, -3.4284e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 8 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0021, -0.0045,  0.0002,  ...,  0.0017, -0.0030, -0.0007]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0021, -0.0045,  0.0002,  ...,  0.0017, -0.0030, -0.0007]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 9.9253e-03,  5.8274e-03, -1.5807e-03,  1.1769e-02,  1.0381e-02,\n",
      "          5.0972e-03,  8.8046e-03, -2.0939e-03,  1.1758e-03, -9.3583e-03,\n",
      "         -6.8832e-03,  5.5247e-03,  6.5831e-03, -1.2757e-02,  0.0000e+00,\n",
      "         -8.0188e-03,  0.0000e+00, -4.7110e-03, -2.3522e-03,  4.8622e-03,\n",
      "         -3.5613e-03,  3.8129e-03,  3.2390e-03, -1.2584e-03, -0.0000e+00,\n",
      "          1.0092e-02, -8.6090e-04,  5.1578e-03, -1.9082e-03,  4.8065e-03,\n",
      "         -3.7945e-03, -0.0000e+00,  7.0929e-03, -9.9338e-03,  8.6929e-03,\n",
      "         -1.1778e-02,  5.0288e-03,  4.4196e-03,  4.6200e-03, -3.6314e-03,\n",
      "          5.0707e-03,  1.3973e-02, -1.0638e-02,  8.7833e-03, -1.3004e-02,\n",
      "          1.3660e-02, -5.7744e-03, -9.4477e-04, -1.6877e-03,  4.5867e-03,\n",
      "          1.6509e-03, -1.0728e-02, -1.1985e-03, -5.5725e-03,  0.0000e+00,\n",
      "          5.0092e-03, -1.1356e-02, -7.4640e-03, -1.1646e-03,  1.3924e-03,\n",
      "          1.8370e-02,  1.6646e-04,  3.7247e-03,  3.4256e-03, -8.3195e-04,\n",
      "          5.9792e-04, -1.0359e-03,  5.2553e-03, -2.0135e-03, -1.2523e-03,\n",
      "          1.2300e-02,  5.8622e-03, -2.5272e-03,  3.5387e-03, -4.0790e-03,\n",
      "          0.0000e+00,  0.0000e+00, -4.5724e-04,  1.4309e-03,  2.0762e-03,\n",
      "         -3.2099e-03, -5.4648e-03,  0.0000e+00, -1.1527e-02, -7.2825e-04,\n",
      "          4.7614e-03,  1.6149e-02, -3.9214e-05, -1.8872e-03, -1.2979e-02,\n",
      "         -1.0678e-02, -8.3970e-03,  0.0000e+00,  4.7757e-03, -7.9355e-03,\n",
      "         -1.6327e-02,  6.4428e-03,  1.5421e-04,  6.7019e-03,  2.2926e-03,\n",
      "         -4.1660e-03,  5.2991e-03,  8.4888e-04,  1.0375e-03, -1.0167e-03,\n",
      "          1.0579e-02,  0.0000e+00,  6.0242e-03,  0.0000e+00,  4.7140e-03,\n",
      "         -3.5239e-05,  0.0000e+00,  3.4512e-03, -1.5718e-03,  5.1235e-03,\n",
      "          7.2443e-03,  2.5855e-03,  0.0000e+00,  1.6731e-03, -9.0997e-03,\n",
      "         -8.3588e-04, -9.3498e-03, -5.3886e-03, -1.9004e-03,  1.2663e-02,\n",
      "         -8.2238e-03, -1.2293e-02, -1.0936e-02,  7.9875e-03,  6.2476e-03,\n",
      "          2.4306e-03, -1.7431e-03, -2.6761e-04, -0.0000e+00,  0.0000e+00,\n",
      "          1.6348e-03,  9.3657e-03, -0.0000e+00, -3.5780e-03, -1.1839e-02,\n",
      "          5.4022e-03,  2.7312e-03, -3.1712e-03, -5.6005e-03,  5.2960e-03,\n",
      "          9.6238e-03,  6.2543e-03,  7.1524e-03,  9.9962e-03, -1.7245e-03,\n",
      "          3.3986e-03, -5.3928e-03, -2.1451e-03,  6.1652e-03, -1.8318e-03,\n",
      "         -4.1923e-04,  5.6587e-03,  6.7200e-03,  1.8421e-03,  1.0527e-02,\n",
      "          1.3045e-02,  3.4892e-03, -5.3212e-03,  2.1673e-03, -1.9447e-03,\n",
      "          9.2725e-03,  1.1800e-03,  1.0915e-02, -1.9648e-03,  6.5262e-03,\n",
      "          6.8723e-03, -9.5266e-03, -6.2629e-03, -6.9470e-03,  1.6681e-03,\n",
      "          3.3133e-04,  1.0304e-02, -6.2790e-03, -6.1404e-03, -3.2629e-03,\n",
      "          4.3379e-04,  1.1962e-02,  1.0994e-02, -2.6986e-03,  2.6645e-03,\n",
      "         -4.6999e-03,  4.5263e-03,  1.0516e-02,  7.6093e-03,  1.4531e-02,\n",
      "         -1.8314e-03,  7.3672e-03, -1.3935e-03,  8.9251e-03, -7.0253e-03,\n",
      "         -1.6871e-02, -0.0000e+00,  1.7407e-02, -1.3661e-02,  1.3414e-02,\n",
      "          0.0000e+00,  3.8703e-03,  8.4908e-03,  3.0958e-03,  3.1028e-03,\n",
      "          3.9022e-03, -4.5245e-03,  5.5065e-03, -6.5343e-03,  1.5790e-02,\n",
      "         -1.0006e-02,  5.0519e-03, -2.7361e-03, -9.9705e-03,  9.2527e-04,\n",
      "         -7.2836e-03, -1.6120e-03,  1.0638e-02,  2.0283e-03, -3.1346e-03,\n",
      "         -7.8724e-03, -1.7735e-03,  1.0801e-02,  1.5981e-02,  4.8939e-03,\n",
      "         -4.7362e-03,  0.0000e+00, -6.4775e-03, -4.5903e-03,  3.7804e-03,\n",
      "         -5.7462e-04,  9.1820e-03,  3.7542e-03, -6.9035e-03, -9.3563e-03,\n",
      "          1.3982e-02,  5.9304e-03, -7.9390e-04,  6.7788e-03,  0.0000e+00,\n",
      "          8.6573e-03,  1.1174e-02, -7.5759e-03, -0.0000e+00,  1.1138e-02,\n",
      "          1.3979e-03,  1.2109e-02, -9.5989e-03, -3.2900e-03, -7.0139e-03,\n",
      "         -0.0000e+00,  3.3444e-03,  1.1464e-02,  1.0184e-02, -3.2707e-03,\n",
      "         -4.2197e-03, -0.0000e+00,  6.2500e-03, -2.3086e-03, -6.2976e-03,\n",
      "         -7.0664e-03, -7.0008e-03,  0.0000e+00, -4.3421e-03, -3.0543e-03,\n",
      "         -7.7947e-03, -3.3012e-03, -1.1713e-03,  1.1214e-03,  1.5024e-02,\n",
      "         -7.4462e-03, -1.4367e-03, -0.0000e+00, -6.2596e-04,  4.1934e-03,\n",
      "          0.0000e+00,  1.6847e-03,  0.0000e+00,  4.2164e-03,  0.0000e+00,\n",
      "         -1.4981e-02,  1.2520e-02,  0.0000e+00,  2.0581e-03,  0.0000e+00,\n",
      "         -2.6988e-04, -1.6987e-03, -1.6719e-02, -3.7105e-03, -1.4413e-02,\n",
      "         -3.7310e-03,  4.5358e-03, -0.0000e+00, -5.8973e-05,  9.8116e-03,\n",
      "         -6.8485e-03,  3.3250e-03,  5.4523e-03, -5.0820e-03, -4.5347e-03,\n",
      "          4.4224e-04, -1.6864e-02, -6.6353e-03,  2.4093e-03, -6.3224e-03,\n",
      "         -1.3702e-03, -2.9564e-03,  3.8645e-03,  3.0984e-03, -1.8340e-03,\n",
      "          4.8675e-04, -1.6990e-02,  0.0000e+00, -4.9649e-03,  2.6365e-03,\n",
      "          1.4181e-02, -3.6929e-03,  1.3403e-02, -0.0000e+00,  1.1064e-02,\n",
      "          3.1673e-03,  0.0000e+00,  2.5655e-03,  6.8353e-04, -3.4840e-03,\n",
      "         -4.9366e-03, -2.3037e-03,  1.0133e-03, -1.9428e-03, -6.2705e-03,\n",
      "         -1.1636e-02,  7.3748e-03, -9.5111e-04,  1.4412e-02,  7.8000e-03,\n",
      "          7.6933e-05, -0.0000e+00, -1.2403e-03, -2.3414e-03, -4.3944e-03,\n",
      "         -2.0481e-04,  6.8074e-03, -2.5615e-03,  4.0666e-03, -2.4733e-03,\n",
      "          0.0000e+00, -0.0000e+00,  6.5636e-03, -1.1610e-02,  8.1951e-03,\n",
      "         -1.2239e-02,  6.4470e-03,  1.4501e-03, -1.0303e-02, -4.0669e-03,\n",
      "          1.4491e-02, -2.8993e-03,  2.2493e-03, -2.2183e-03,  7.3375e-03,\n",
      "         -6.1538e-03, -8.8812e-03, -5.0513e-03,  6.5099e-03, -7.8539e-03,\n",
      "         -5.1207e-03, -2.9830e-03, -4.7638e-03,  0.0000e+00,  8.7836e-03,\n",
      "         -6.8671e-03,  4.4027e-03,  1.4602e-02,  5.6583e-03, -0.0000e+00,\n",
      "         -1.1304e-02, -1.3516e-02, -2.8521e-03,  1.2344e-02,  0.0000e+00,\n",
      "          3.5528e-03,  8.6584e-03, -8.5657e-03,  1.2810e-02, -2.8821e-03,\n",
      "          9.4298e-04,  5.4108e-03, -8.3051e-03, -8.0985e-03,  5.7846e-03,\n",
      "          3.8485e-03,  8.7116e-05,  0.0000e+00,  7.5617e-03, -1.3878e-03,\n",
      "         -7.4426e-03,  2.3389e-04, -6.9501e-03,  0.0000e+00, -5.9216e-03,\n",
      "          3.5489e-03,  2.5122e-03,  0.0000e+00, -5.8532e-03,  0.0000e+00,\n",
      "         -9.7556e-03,  7.7767e-03, -0.0000e+00,  8.4308e-03,  0.0000e+00,\n",
      "          1.6223e-03, -6.5506e-03, -0.0000e+00,  3.3750e-03, -3.9990e-03,\n",
      "         -2.7313e-03,  5.8945e-03, -8.6933e-03, -6.1011e-03,  3.1423e-03,\n",
      "          2.6573e-03, -3.0150e-04,  9.4045e-03,  1.1164e-03, -4.2572e-03,\n",
      "          7.6183e-03, -1.3481e-02, -2.4712e-03, -1.9255e-03,  1.5984e-03,\n",
      "         -0.0000e+00,  5.2267e-03,  2.7308e-03,  0.0000e+00, -7.1703e-04,\n",
      "         -6.4872e-03,  1.2957e-03, -0.0000e+00,  4.5030e-03, -3.3018e-03,\n",
      "         -4.8929e-03,  1.1411e-02,  9.0570e-03, -5.7202e-04,  3.9660e-03,\n",
      "         -3.3743e-03,  4.5062e-03,  9.1527e-03,  8.4370e-03,  7.4084e-03,\n",
      "         -1.2227e-02,  1.7118e-02,  8.3339e-03, -4.8876e-03,  1.5257e-03,\n",
      "         -9.4138e-03, -1.5056e-03, -2.5110e-03, -1.2441e-02,  5.0792e-03,\n",
      "         -7.4679e-03,  1.0326e-02, -2.1479e-02, -5.0490e-03,  6.6781e-03,\n",
      "          2.0545e-03, -6.5599e-04, -7.5262e-03,  4.3170e-03, -4.9504e-03,\n",
      "         -1.4616e-02, -3.3994e-03, -3.4630e-03,  9.6434e-03, -6.6253e-03,\n",
      "         -1.1181e-02,  2.0724e-03,  2.3392e-03,  1.1486e-02, -1.3428e-02,\n",
      "         -7.2771e-03,  2.8054e-03,  5.9818e-03,  5.0716e-03, -5.6355e-03,\n",
      "          4.8294e-03, -7.2549e-03, -1.6210e-02,  6.0391e-03,  1.6854e-03,\n",
      "         -1.8420e-03,  1.0901e-02, -0.0000e+00, -9.5387e-03, -5.6607e-03,\n",
      "          5.8115e-03,  1.2108e-04, -0.0000e+00, -1.3434e-02,  1.0249e-02,\n",
      "          0.0000e+00, -0.0000e+00, -3.5453e-03, -6.7703e-03, -8.8962e-03,\n",
      "          6.5899e-04,  1.0526e-02,  2.4637e-03, -7.9686e-04, -5.1352e-03,\n",
      "          1.1116e-02, -4.3418e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 7 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 10\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 0.0018, -0.0043,  0.0006,  ...,  0.0016, -0.0027, -0.0005]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 0.0018, -0.0043,  0.0006,  ...,  0.0016, -0.0027, -0.0005]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 8.1116e-03,  4.6586e-03, -2.3102e-03,  1.1320e-02,  1.0430e-02,\n",
      "          3.9052e-03,  9.8737e-03, -2.5766e-03,  9.7226e-05, -9.0238e-03,\n",
      "         -7.5954e-03,  4.1148e-03,  7.5699e-03, -1.3256e-02,  0.0000e+00,\n",
      "         -8.5030e-03,  0.0000e+00, -4.9478e-03, -2.0796e-03,  6.3855e-03,\n",
      "         -3.9749e-03,  5.0516e-03,  2.6183e-03, -2.0221e-03, -0.0000e+00,\n",
      "          9.1213e-03, -1.6810e-03,  5.6216e-03, -1.7907e-03,  5.3300e-03,\n",
      "         -5.1717e-03, -0.0000e+00,  6.4034e-03, -1.0909e-02,  8.9253e-03,\n",
      "         -1.1612e-02,  3.4600e-03,  3.7760e-03,  5.1689e-03, -3.4934e-03,\n",
      "          5.2797e-03,  1.3308e-02, -1.1103e-02,  8.4824e-03, -1.3006e-02,\n",
      "          1.4109e-02, -6.0704e-03, -2.3847e-03, -2.2176e-03,  6.3847e-03,\n",
      "          1.4774e-03, -8.0460e-03, -1.5001e-03, -4.3333e-03,  0.0000e+00,\n",
      "          5.5244e-03, -1.0415e-02, -7.5756e-03, -1.5520e-03,  7.6282e-04,\n",
      "          1.7975e-02,  4.4730e-04,  4.4818e-03,  3.0974e-03, -7.8721e-04,\n",
      "          9.4582e-04, -1.3842e-03,  5.2450e-03, -1.0573e-03, -2.2492e-03,\n",
      "          1.0908e-02,  5.9878e-03,  8.5758e-04,  1.6123e-03, -5.6749e-03,\n",
      "          0.0000e+00,  0.0000e+00,  1.4277e-03,  9.9647e-04,  2.8114e-03,\n",
      "         -2.7267e-03, -4.3394e-03,  0.0000e+00, -1.2133e-02,  1.7023e-04,\n",
      "          5.5799e-03,  1.5597e-02,  1.6395e-03, -3.3462e-03, -1.2711e-02,\n",
      "         -1.1457e-02, -9.8244e-03,  0.0000e+00,  4.0687e-03, -7.9519e-03,\n",
      "         -1.5604e-02,  5.8486e-03,  6.8066e-04,  6.6821e-03,  1.3992e-03,\n",
      "         -3.6709e-03,  6.4875e-03,  1.3420e-03,  1.9932e-03, -3.8892e-04,\n",
      "          1.1155e-02,  0.0000e+00,  6.6981e-03,  0.0000e+00,  4.5315e-03,\n",
      "          5.8382e-04,  0.0000e+00,  3.6712e-03, -1.8739e-03,  6.0283e-03,\n",
      "          7.6121e-03,  1.8862e-03,  0.0000e+00,  4.9720e-04, -1.1210e-02,\n",
      "          1.0360e-04, -9.7616e-03, -5.1324e-03, -1.9222e-03,  1.2905e-02,\n",
      "         -8.8030e-03, -1.2111e-02, -9.9429e-03,  6.6997e-03,  7.4882e-03,\n",
      "          2.1692e-03, -8.8772e-04,  9.3314e-04, -0.0000e+00,  0.0000e+00,\n",
      "          8.8142e-04,  8.5968e-03, -0.0000e+00, -5.2265e-03, -1.0490e-02,\n",
      "          4.9059e-03,  3.7159e-03, -4.7895e-03, -5.8312e-03,  6.1688e-03,\n",
      "          9.6219e-03,  5.9493e-03,  9.3046e-03,  8.9457e-03, -9.7393e-04,\n",
      "          3.7978e-03, -6.0689e-03, -1.5544e-03,  7.1350e-03,  8.0620e-04,\n",
      "         -2.7431e-04,  6.4996e-03,  5.4459e-03,  1.0400e-03,  1.0296e-02,\n",
      "          1.2131e-02,  3.1927e-03, -4.9365e-03,  1.5853e-03, -2.9416e-03,\n",
      "          8.8657e-03,  5.8230e-04,  1.0188e-02, -1.9002e-03,  7.5877e-03,\n",
      "          6.6935e-03, -1.1150e-02, -5.9576e-03, -5.8026e-03,  6.0771e-04,\n",
      "          6.1327e-04,  1.0955e-02, -7.3257e-03, -4.5622e-03, -3.3877e-03,\n",
      "          9.8771e-04,  1.2571e-02,  9.2644e-03, -2.8184e-03,  3.6530e-03,\n",
      "         -4.4539e-03,  3.7322e-03,  8.3429e-03,  6.9041e-03,  1.3101e-02,\n",
      "         -1.1715e-03,  7.1596e-03, -3.3262e-03,  8.4408e-03, -7.1831e-03,\n",
      "         -1.5594e-02, -0.0000e+00,  1.7204e-02, -1.3804e-02,  1.3131e-02,\n",
      "          0.0000e+00,  4.1740e-03,  7.4700e-03,  4.1879e-03,  3.6059e-03,\n",
      "          3.9258e-03, -4.6554e-03,  6.9648e-03, -5.4579e-03,  1.6569e-02,\n",
      "         -9.8667e-03,  4.1849e-03, -2.7358e-03, -8.9544e-03, -7.4930e-05,\n",
      "         -6.9405e-03, -1.1258e-03,  1.0096e-02,  2.0329e-03, -4.1314e-03,\n",
      "         -7.9356e-03, -1.0470e-03,  9.7546e-03,  1.6307e-02,  3.6824e-03,\n",
      "         -5.5674e-03,  0.0000e+00, -5.7869e-03, -4.6230e-03,  1.3438e-03,\n",
      "          8.7091e-06,  1.0104e-02,  3.5183e-03, -5.7953e-03, -9.6415e-03,\n",
      "          1.3267e-02,  6.0571e-03, -7.7381e-04,  6.2365e-03,  0.0000e+00,\n",
      "          9.4418e-03,  1.1195e-02, -8.9812e-03, -0.0000e+00,  1.1074e-02,\n",
      "          3.8979e-03,  1.0276e-02, -9.2756e-03, -5.9995e-03, -8.0196e-03,\n",
      "         -0.0000e+00,  4.5400e-03,  1.0652e-02,  9.2586e-03, -3.3924e-03,\n",
      "         -2.5701e-03,  0.0000e+00,  7.0507e-03, -3.9156e-03, -5.5168e-03,\n",
      "         -6.6655e-03, -7.4892e-03, -0.0000e+00, -4.3421e-03, -3.8813e-03,\n",
      "         -7.1063e-03, -2.4606e-03, -3.7467e-04,  2.3414e-03,  1.5284e-02,\n",
      "         -7.8152e-03, -1.9902e-03, -0.0000e+00, -1.4925e-03,  4.7213e-03,\n",
      "          0.0000e+00,  1.7806e-03,  0.0000e+00,  4.1108e-03,  0.0000e+00,\n",
      "         -1.3755e-02,  1.2486e-02,  0.0000e+00,  1.5470e-03,  0.0000e+00,\n",
      "         -9.6426e-04, -1.8180e-03, -1.8242e-02, -4.5269e-03, -1.3989e-02,\n",
      "         -3.4443e-03,  3.6916e-03, -0.0000e+00, -1.6114e-03,  1.0802e-02,\n",
      "         -5.8367e-03,  1.7025e-03,  5.8594e-03, -6.6678e-03, -4.3322e-03,\n",
      "          1.5127e-03, -1.3403e-02, -7.2206e-03,  1.6420e-03, -5.9213e-03,\n",
      "         -8.9283e-04, -1.8406e-03,  1.2835e-03,  3.1280e-03, -1.8577e-03,\n",
      "          1.0880e-03, -1.6776e-02, -0.0000e+00, -6.6485e-03,  4.1162e-03,\n",
      "          1.3511e-02, -3.6133e-03,  1.2182e-02, -0.0000e+00,  1.1172e-02,\n",
      "          3.9108e-03,  0.0000e+00,  6.9390e-04,  7.1027e-04, -3.9731e-03,\n",
      "         -3.3814e-03, -1.2518e-03,  2.0577e-04, -3.3661e-04, -5.6697e-03,\n",
      "         -1.1354e-02,  6.5336e-03, -1.3389e-03,  1.3516e-02,  6.3476e-03,\n",
      "         -9.6329e-04, -0.0000e+00, -1.8701e-03, -3.0240e-03, -4.4178e-03,\n",
      "         -4.7994e-04,  5.7772e-03, -2.9208e-03,  4.7773e-03, -1.3060e-03,\n",
      "          0.0000e+00, -0.0000e+00,  7.4585e-03, -1.0841e-02,  8.0090e-03,\n",
      "         -1.3010e-02,  4.5867e-03,  1.3443e-03, -1.1456e-02, -4.0681e-03,\n",
      "          1.5749e-02, -2.3779e-03,  6.0964e-04, -2.2991e-03,  7.7723e-03,\n",
      "         -5.5693e-03, -9.6383e-03, -6.3288e-03,  4.7658e-03, -7.9003e-03,\n",
      "         -5.1661e-03, -2.4752e-03, -4.7694e-03,  0.0000e+00,  8.6071e-03,\n",
      "         -6.2325e-03,  2.9999e-03,  1.4199e-02,  4.8996e-03, -0.0000e+00,\n",
      "         -1.1303e-02, -1.3332e-02, -1.5512e-03,  1.1790e-02,  0.0000e+00,\n",
      "          4.2213e-03,  8.8406e-03, -8.2346e-03,  1.1436e-02, -1.2832e-03,\n",
      "          6.4300e-04,  4.9720e-03, -6.8125e-03, -7.3797e-03,  5.2913e-03,\n",
      "          3.4904e-03,  2.4877e-05,  0.0000e+00,  5.2927e-03, -2.6556e-03,\n",
      "         -6.3642e-03, -1.1072e-03, -7.5509e-03,  0.0000e+00, -5.0224e-03,\n",
      "          4.7684e-03,  5.0329e-04,  0.0000e+00, -7.3825e-03,  0.0000e+00,\n",
      "         -9.1717e-03,  4.7960e-03, -0.0000e+00,  7.0491e-03,  0.0000e+00,\n",
      "          1.5235e-03, -4.9059e-03, -0.0000e+00,  3.8200e-03, -3.4760e-03,\n",
      "         -2.7696e-03,  6.5295e-03, -7.6024e-03, -5.1182e-03,  3.4111e-03,\n",
      "          1.6082e-03, -1.4628e-03,  8.6388e-03,  1.0554e-03, -4.9681e-03,\n",
      "          8.1793e-03, -1.3265e-02, -3.5096e-03, -1.5724e-03,  7.6900e-04,\n",
      "         -0.0000e+00,  5.3584e-03,  2.7358e-03,  0.0000e+00, -5.3996e-04,\n",
      "         -7.2379e-03,  6.5336e-04, -0.0000e+00,  4.7365e-03, -3.3960e-03,\n",
      "         -4.9124e-03,  1.0444e-02,  7.3298e-03, -3.9955e-04,  4.6907e-03,\n",
      "         -2.6717e-03,  3.2631e-03,  9.9830e-03,  9.0133e-03,  6.0106e-03,\n",
      "         -1.2251e-02,  1.6767e-02,  1.2255e-02, -4.6495e-03,  2.2889e-03,\n",
      "         -9.6265e-03, -1.2924e-03, -3.8996e-03, -1.0454e-02,  4.0219e-03,\n",
      "         -8.4367e-03,  1.1038e-02, -2.1534e-02, -3.6013e-03,  5.4605e-03,\n",
      "          2.1193e-03, -7.7932e-04, -7.8289e-03,  5.0685e-03, -4.9696e-03,\n",
      "         -1.3391e-02, -3.2949e-03, -4.8274e-03,  8.4117e-03, -7.0371e-03,\n",
      "         -1.1752e-02,  1.7146e-03,  1.3316e-03,  1.1581e-02, -1.2685e-02,\n",
      "         -8.4979e-03,  2.6920e-03,  5.7808e-03,  5.3393e-03, -6.0802e-03,\n",
      "          4.4462e-03, -6.4916e-03, -1.5941e-02,  6.7584e-03,  2.0613e-03,\n",
      "         -2.5322e-03,  1.0917e-02, -0.0000e+00, -1.1505e-02, -6.7557e-03,\n",
      "          5.7461e-03, -3.8202e-04, -0.0000e+00, -1.5048e-02,  1.0343e-02,\n",
      "          0.0000e+00, -0.0000e+00, -1.8987e-03, -6.5353e-03, -9.3206e-03,\n",
      "         -6.6376e-06,  1.1193e-02,  2.3762e-03, -2.0399e-03, -4.1801e-03,\n",
      "          8.9889e-03, -2.8595e-03]], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([1, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([1, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 6 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 7\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 1.7443e-03, -3.9814e-03,  7.4506e-04,  ...,  1.6563e-03,\n",
      "         -2.4261e-03, -1.9948e-04],\n",
      "        [ 1.7130e-03, -3.0089e-03,  9.5765e-05,  ...,  9.0395e-04,\n",
      "         -3.5894e-05, -1.9414e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 1.7443e-03, -3.9814e-03,  7.4506e-04,  ...,  1.6563e-03,\n",
      "         -2.4261e-03, -1.9948e-04],\n",
      "        [ 1.7130e-03, -3.0089e-03,  9.5765e-05,  ...,  9.0395e-04,\n",
      "         -3.5894e-05, -1.9414e-03]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0079,  0.0050, -0.0011,  ..., -0.0031,  0.0083, -0.0027],\n",
      "        [ 0.0095,  0.0045, -0.0014,  ..., -0.0000,  0.0074, -0.0005]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([2, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([2, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([2, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 5 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tcurrent_length_index += 1\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 1.5730e-03, -3.6724e-03,  8.8536e-04,  ...,  1.8001e-03,\n",
      "         -2.4014e-03,  1.5095e-05],\n",
      "        [ 1.8719e-03, -3.7791e-03,  7.1046e-06,  ...,  7.4576e-04,\n",
      "          1.1645e-05, -2.0745e-03],\n",
      "        [ 2.4044e-04, -3.7126e-03, -1.3427e-04,  ...,  7.0461e-04,\n",
      "         -8.4922e-04,  3.3241e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 1.5730e-03, -3.6724e-03,  8.8536e-04,  ...,  1.8001e-03,\n",
      "         -2.4014e-03,  1.5095e-05],\n",
      "        [ 1.8719e-03, -3.7791e-03,  7.1046e-06,  ...,  7.4576e-04,\n",
      "          1.1645e-05, -2.0745e-03],\n",
      "        [ 2.4044e-04, -3.7126e-03, -1.3427e-04,  ...,  7.0461e-04,\n",
      "         -8.4922e-04,  3.3241e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0083,  0.0052, -0.0016,  ..., -0.0033,  0.0096, -0.0025],\n",
      "        [ 0.0101,  0.0028, -0.0010,  ..., -0.0000,  0.0060, -0.0006],\n",
      "        [ 0.0085,  0.0053, -0.0013,  ..., -0.0029,  0.0079, -0.0003]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 4 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 1.4492e-03, -3.4578e-03,  1.1544e-03,  ...,  1.8597e-03,\n",
      "         -2.3733e-03,  3.1029e-05],\n",
      "        [ 2.2051e-03, -4.3382e-03, -3.5509e-05,  ...,  5.6680e-04,\n",
      "         -2.5714e-04, -2.1844e-03],\n",
      "        [ 9.5454e-04, -4.5679e-03, -1.7788e-04,  ...,  1.7615e-04,\n",
      "         -8.7198e-04,  3.3243e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 1.4492e-03, -3.4578e-03,  1.1544e-03,  ...,  1.8597e-03,\n",
      "         -2.3733e-03,  3.1029e-05],\n",
      "        [ 2.2051e-03, -4.3382e-03, -3.5509e-05,  ...,  5.6680e-04,\n",
      "         -2.5714e-04, -2.1844e-03],\n",
      "        [ 9.5454e-04, -4.5679e-03, -1.7788e-04,  ...,  1.7615e-04,\n",
      "         -8.7198e-04,  3.3243e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0079,  0.0052, -0.0017,  ..., -0.0027,  0.0090, -0.0025],\n",
      "        [ 0.0088,  0.0040, -0.0011,  ..., -0.0000,  0.0069, -0.0009],\n",
      "        [ 0.0090,  0.0060, -0.0015,  ..., -0.0025,  0.0080, -0.0004]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 3 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 1.4039e-03, -3.2300e-03,  1.4175e-03,  ...,  1.8738e-03,\n",
      "         -2.3255e-03, -1.0887e-05],\n",
      "        [ 2.2071e-03, -4.8538e-03, -1.3371e-04,  ...,  5.8808e-04,\n",
      "         -2.6591e-04, -2.3053e-03],\n",
      "        [ 1.5737e-03, -4.6180e-03, -2.0036e-04,  ..., -2.8446e-04,\n",
      "         -9.0412e-04,  2.5263e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 1.4039e-03, -3.2300e-03,  1.4175e-03,  ...,  1.8738e-03,\n",
      "         -2.3255e-03, -1.0887e-05],\n",
      "        [ 2.2071e-03, -4.8538e-03, -1.3371e-04,  ...,  5.8808e-04,\n",
      "         -2.6591e-04, -2.3053e-03],\n",
      "        [ 1.5737e-03, -4.6180e-03, -2.0036e-04,  ..., -2.8446e-04,\n",
      "         -9.0412e-04,  2.5263e-04]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0098,  0.0066, -0.0019,  ..., -0.0036,  0.0105, -0.0031],\n",
      "        [ 0.0107,  0.0038, -0.0010,  ..., -0.0000,  0.0058,  0.0009],\n",
      "        [ 0.0089,  0.0062, -0.0020,  ..., -0.0022,  0.0073,  0.0007]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 2 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 1.3695e-03, -3.0532e-03,  1.5572e-03,  ...,  1.8113e-03,\n",
      "         -2.2324e-03, -1.3285e-04],\n",
      "        [ 2.0607e-03, -5.5391e-03, -2.5798e-04,  ...,  4.2449e-04,\n",
      "         -3.7122e-04, -2.3950e-03],\n",
      "        [ 1.8677e-03, -4.6289e-03, -3.9432e-04,  ..., -5.7951e-04,\n",
      "         -5.9543e-04, -6.0301e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 1.3695e-03, -3.0532e-03,  1.5572e-03,  ...,  1.8113e-03,\n",
      "         -2.2324e-03, -1.3285e-04],\n",
      "        [ 2.0607e-03, -5.5391e-03, -2.5798e-04,  ...,  4.2449e-04,\n",
      "         -3.7122e-04, -2.3950e-03],\n",
      "        [ 1.8677e-03, -4.6289e-03, -3.9432e-04,  ..., -5.7951e-04,\n",
      "         -5.9543e-04, -6.0301e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0080,  0.0064, -0.0010,  ..., -0.0022,  0.0089, -0.0050],\n",
      "        [ 0.0112,  0.0047, -0.0002,  ...,  0.0000,  0.0055,  0.0009],\n",
      "        [ 0.0092,  0.0057, -0.0006,  ..., -0.0011,  0.0063, -0.0005]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 1 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 9.3795e-04, -3.0219e-03,  1.6129e-03,  ...,  1.8034e-03,\n",
      "         -2.2206e-03, -5.6857e-05],\n",
      "        [ 1.7165e-03, -5.8182e-03, -3.9655e-04,  ...,  2.2543e-04,\n",
      "         -2.6781e-04, -2.4259e-03],\n",
      "        [ 2.0603e-03, -4.7528e-03, -4.9199e-04,  ..., -3.9582e-04,\n",
      "         -7.3563e-04,  1.3888e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 9.3795e-04, -3.0219e-03,  1.6129e-03,  ...,  1.8034e-03,\n",
      "         -2.2206e-03, -5.6857e-05],\n",
      "        [ 1.7165e-03, -5.8182e-03, -3.9655e-04,  ...,  2.2543e-04,\n",
      "         -2.6781e-04, -2.4259e-03],\n",
      "        [ 2.0603e-03, -4.7528e-03, -4.9199e-04,  ..., -3.9582e-04,\n",
      "         -7.3563e-04,  1.3888e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0069,  0.0057,  0.0003,  ..., -0.0015,  0.0089, -0.0059],\n",
      "        [ 0.0106,  0.0047, -0.0009,  ..., -0.0000,  0.0054,  0.0015],\n",
      "        [ 0.0090,  0.0059, -0.0008,  ..., -0.0006,  0.0056, -0.0003]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tindex = 0 since backward\n",
      "\t\t\t\tIn case backward,\n",
      "\t\t\t\tbatch_lengths[current_length_index] <= index = False\n",
      "\t\t\t\tbatch_lengths[length_index] is 6\n",
      "\t\t\t\tGet a previous memory...\n",
      "tensor([[ 5.8107e-04, -3.3744e-03,  1.5471e-03,  ...,  1.8350e-03,\n",
      "         -1.9967e-03,  8.1014e-05],\n",
      "        [ 1.6975e-03, -5.7833e-03, -6.0189e-04,  ...,  3.0246e-04,\n",
      "         -2.3077e-04, -2.4285e-03],\n",
      "        [ 2.1031e-03, -4.6326e-03, -2.4892e-04,  ..., -5.0046e-04,\n",
      "         -8.4787e-04,  8.7565e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 4096])\n",
      "\t\t\t\tGet a previous state...\n",
      "tensor([[ 5.8107e-04, -3.3744e-03,  1.5471e-03,  ...,  1.8350e-03,\n",
      "         -1.9967e-03,  8.1014e-05],\n",
      "        [ 1.6975e-03, -5.7833e-03, -6.0189e-04,  ...,  3.0246e-04,\n",
      "         -2.3077e-04, -2.4285e-03],\n",
      "        [ 2.1031e-03, -4.6326e-03, -2.4892e-04,  ..., -5.0046e-04,\n",
      "         -8.4787e-04,  8.7565e-05]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tGet a timestep input...\n",
      "tensor([[ 0.0078,  0.0054,  0.0002,  ..., -0.0031,  0.0096, -0.0045],\n",
      "        [ 0.0106,  0.0045, -0.0008,  ..., -0.0000,  0.0068,  0.0014],\n",
      "        [ 0.0092,  0.0057, -0.0007,  ..., -0.0022,  0.0068, -0.0002]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([3, 512])\n",
      "\t\t\t\tProjection to 4*cell_size...\n",
      "\t\t\t\t`input_linearity`: W1 * timestep_input\n",
      "\t\t\t\tprojected_input.shape = torch.Size([3, 16384])\n",
      "\t\t\t\t`state_linearity`: W2 * previous_state + b\n",
      "\t\t\t\tprojected_state.shape = torch.Size([3, 16384])\n",
      "\t\t\t\tCalc LSTM hidden unit...\n",
      "\t\t\t\tis memory_cell_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at memory_cell_clip_value3\n",
      "\t\t\t\tCalc next timestep output...\n",
      "\t\t\t\tstate_projection_clip_value is exist? Yes\n",
      "\t\t\t\tOh, it's float. Set lower bound and upper bound at state_projection_clip_value3\n",
      "\t\t\t\tIf dropout_mask exists, Adjust.\n",
      "\t\t\t\tset full_batch_previous memory/state!!\n",
      "\t\t\t\tfinal_state = (tensor([[[-1.3994e-03, -3.0564e-04, -1.6608e-03,  ...,  7.8062e-04,\n",
      "           1.1323e-03,  3.7102e-04],\n",
      "         [-1.6801e-03, -4.1904e-04, -9.4694e-04,  ...,  0.0000e+00,\n",
      "           1.4231e-03, -3.6675e-04],\n",
      "         [-0.0000e+00, -1.1648e-03, -2.0439e-03,  ...,  6.8208e-05,\n",
      "           9.9208e-04,  0.0000e+00]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>), tensor([[[ 0.0005, -0.0037,  0.0016,  ...,  0.0017, -0.0018,  0.0003],\n",
      "         [ 0.0018, -0.0060, -0.0008,  ...,  0.0003, -0.0003, -0.0023],\n",
      "         [ 0.0023, -0.0048, -0.0002,  ..., -0.0006, -0.0009,  0.0003]]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>))\n",
      "\t\tsince layer_index != 0, adding cache to output sequence\n",
      "stacked_sequence_output.shape = torch.Size([2, 3, 10, 1024])\n",
      "final_states = (tensor([[[-5.5796e-03,  1.0460e-02,  5.4435e-04,  ..., -3.1331e-03,\n",
      "           9.6490e-03, -4.4888e-03],\n",
      "         [-6.0546e-03,  9.0555e-03, -1.3511e-03,  ..., -0.0000e+00,\n",
      "           6.8154e-03,  1.4438e-03],\n",
      "         [-7.9315e-03,  9.0336e-03,  9.2295e-04,  ..., -2.2244e-03,\n",
      "           6.8417e-03, -1.5449e-04]],\n",
      "\n",
      "        [[ 3.6805e-03,  4.7124e-03,  2.7107e-03,  ...,  7.8062e-04,\n",
      "           1.1323e-03,  3.7102e-04],\n",
      "         [ 2.7964e-03,  4.5057e-03,  2.4832e-03,  ...,  0.0000e+00,\n",
      "           1.4231e-03, -3.6675e-04],\n",
      "         [ 0.0000e+00,  4.5184e-03,  1.0823e-03,  ...,  6.8208e-05,\n",
      "           9.9208e-04,  0.0000e+00]]], device='cuda:0', grad_fn=<CatBackward>), tensor([[[-0.0169, -0.0180,  0.0054,  ..., -0.0127, -0.0196,  0.0183],\n",
      "         [-0.0129, -0.0160,  0.0032,  ..., -0.0053, -0.0150,  0.0178],\n",
      "         [-0.0185, -0.0187,  0.0045,  ..., -0.0039, -0.0280,  0.0172]],\n",
      "\n",
      "        [[-0.0008, -0.0024,  0.0087,  ...,  0.0017, -0.0018,  0.0003],\n",
      "         [ 0.0023, -0.0068,  0.0084,  ...,  0.0003, -0.0003, -0.0023],\n",
      "         [-0.0005, -0.0058,  0.0048,  ..., -0.0006, -0.0009,  0.0003]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "stacked\n",
      "num_layers = 2\n",
      "num_valid = 3\n",
      "returned_timesteps = 10\n",
      "encoder_dim = 1024\n",
      "num_valid < batch_size -> False\n",
      "sequence_length_difference = total_sequence_length - returned_timesteps\n",
      "sequence_length_difference = 0\n",
      "sequence_length_difference is larger than 0? : False\n",
      "UPDATE STATES... inputs: final_states, restoration_indices\n",
      "_EncoderBase의 `_update_states` 메서드 실행\n",
      "inputs:\n",
      "final_states = (tensor([[[-5.5796e-03,  1.0460e-02,  5.4435e-04,  ..., -3.1331e-03,\n",
      "           9.6490e-03, -4.4888e-03],\n",
      "         [-6.0546e-03,  9.0555e-03, -1.3511e-03,  ..., -0.0000e+00,\n",
      "           6.8154e-03,  1.4438e-03],\n",
      "         [-7.9315e-03,  9.0336e-03,  9.2295e-04,  ..., -2.2244e-03,\n",
      "           6.8417e-03, -1.5449e-04]],\n",
      "\n",
      "        [[ 3.6805e-03,  4.7124e-03,  2.7107e-03,  ...,  7.8062e-04,\n",
      "           1.1323e-03,  3.7102e-04],\n",
      "         [ 2.7964e-03,  4.5057e-03,  2.4832e-03,  ...,  0.0000e+00,\n",
      "           1.4231e-03, -3.6675e-04],\n",
      "         [ 0.0000e+00,  4.5184e-03,  1.0823e-03,  ...,  6.8208e-05,\n",
      "           9.9208e-04,  0.0000e+00]]], device='cuda:0', grad_fn=<CatBackward>), tensor([[[-0.0169, -0.0180,  0.0054,  ..., -0.0127, -0.0196,  0.0183],\n",
      "         [-0.0129, -0.0160,  0.0032,  ..., -0.0053, -0.0150,  0.0178],\n",
      "         [-0.0185, -0.0187,  0.0045,  ..., -0.0039, -0.0280,  0.0172]],\n",
      "\n",
      "        [[-0.0008, -0.0024,  0.0087,  ...,  0.0017, -0.0018,  0.0003],\n",
      "         [ 0.0023, -0.0068,  0.0084,  ...,  0.0003, -0.0003, -0.0023],\n",
      "         [-0.0005, -0.0058,  0.0048,  ..., -0.0006, -0.0009,  0.0003]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>))\n",
      "restoration_indices = tensor([1, 2, 0], device='cuda:0')\n",
      "new_unsorted_states = [tensor([[[-6.0546e-03,  9.0555e-03, -1.3511e-03,  ..., -0.0000e+00,\n",
      "           6.8154e-03,  1.4438e-03],\n",
      "         [-7.9315e-03,  9.0336e-03,  9.2295e-04,  ..., -2.2244e-03,\n",
      "           6.8417e-03, -1.5449e-04],\n",
      "         [-5.5796e-03,  1.0460e-02,  5.4435e-04,  ..., -3.1331e-03,\n",
      "           9.6490e-03, -4.4888e-03]],\n",
      "\n",
      "        [[ 2.7964e-03,  4.5057e-03,  2.4832e-03,  ...,  0.0000e+00,\n",
      "           1.4231e-03, -3.6675e-04],\n",
      "         [ 0.0000e+00,  4.5184e-03,  1.0823e-03,  ...,  6.8208e-05,\n",
      "           9.9208e-04,  0.0000e+00],\n",
      "         [ 3.6805e-03,  4.7124e-03,  2.7107e-03,  ...,  7.8062e-04,\n",
      "           1.1323e-03,  3.7102e-04]]], device='cuda:0',\n",
      "       grad_fn=<IndexSelectBackward>), tensor([[[-0.0129, -0.0160,  0.0032,  ..., -0.0053, -0.0150,  0.0178],\n",
      "         [-0.0185, -0.0187,  0.0045,  ..., -0.0039, -0.0280,  0.0172],\n",
      "         [-0.0169, -0.0180,  0.0054,  ..., -0.0127, -0.0196,  0.0183]],\n",
      "\n",
      "        [[ 0.0023, -0.0068,  0.0084,  ...,  0.0003, -0.0003, -0.0023],\n",
      "         [-0.0005, -0.0058,  0.0048,  ..., -0.0006, -0.0009,  0.0003],\n",
      "         [-0.0008, -0.0024,  0.0087,  ...,  0.0017, -0.0018,  0.0003]]],\n",
      "       device='cuda:0', grad_fn=<IndexSelectBackward>)]\n",
      "self._states is None = False\n",
      "이전 상태가 존재합니다. 현재 상태와 입력받은 final_state로 새로운 상태를 update합니다.\n",
      "current_state_batch_size = 3 = self._states[0].size(1)\n",
      "new_state_batch_size = 3 = final_states[0].size(1)\n"
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(token_embedding, Variable(masks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0148,  0.0061,  0.0092,  ..., -0.0006,  0.0175,  0.0060],\n",
       "         [ 0.0002, -0.0079,  0.0007,  ..., -0.0130, -0.0100,  0.0008],\n",
       "         [ 0.0216, -0.0244,  0.0024,  ...,  0.0078, -0.0203,  0.0181],\n",
       "         ...,\n",
       "         [-0.0116, -0.0098,  0.0071,  ...,  0.0057,  0.0145,  0.0200],\n",
       "         [-0.0077, -0.0073,  0.0339,  ..., -0.0176, -0.0071,  0.0008],\n",
       "         [-0.0097, -0.0068, -0.0024,  ..., -0.0159,  0.0052, -0.0018]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.0164,  0.0037, -0.0220,  ...,  0.0029,  0.0011, -0.0237],\n",
       "         [ 0.0266, -0.0056, -0.0164,  ...,  0.0149, -0.0072, -0.0359],\n",
       "         [ 0.0032,  0.0254, -0.0032,  ..., -0.0012,  0.0275, -0.0041],\n",
       "         ...,\n",
       "         [-0.0057, -0.0114, -0.0027,  ..., -0.0069, -0.0053, -0.0084],\n",
       "         [-0.0136, -0.0056,  0.0011,  ...,  0.0311, -0.0035, -0.0125],\n",
       "         [ 0.0118, -0.0069,  0.0113,  ...,  0.0058, -0.0077,  0.0075]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.0015, -0.0121,  0.0004,  ..., -0.0041,  0.0002,  0.0110],\n",
       "         [ 0.0075,  0.0149,  0.0025,  ..., -0.0048,  0.0101,  0.0080],\n",
       "         [ 0.0140, -0.0047, -0.0083,  ...,  0.0061,  0.0128,  0.0079],\n",
       "         ...,\n",
       "         [ 0.0127, -0.0006,  0.0034,  ...,  0.0088, -0.0040, -0.0095],\n",
       "         [-0.0089,  0.0146, -0.0015,  ..., -0.0054,  0.0009,  0.0151],\n",
       "         [-0.0141,  0.0017,  0.0055,  ..., -0.0142, -0.0077, -0.0133]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[-0.0266, -0.0423, -0.0124,  ...,  0.0089, -0.0259, -0.0033],\n",
       "         [-0.0090, -0.0319,  0.0370,  ...,  0.0349,  0.0108,  0.0091],\n",
       "         [ 0.0126,  0.0064, -0.0203,  ...,  0.0285,  0.0019, -0.0013],\n",
       "         ...,\n",
       "         [-0.0128, -0.0100,  0.0038,  ...,  0.0032,  0.0001, -0.0013],\n",
       "         [-0.0008,  0.0012, -0.0062,  ..., -0.0042, -0.0120, -0.0130],\n",
       "         [ 0.0097,  0.0042, -0.0223,  ...,  0.0054, -0.0293, -0.0115]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.0186, -0.0093,  0.0038,  ..., -0.0050, -0.0027, -0.0066],\n",
       "         [ 0.0032,  0.0238,  0.0092,  ...,  0.0192,  0.0149,  0.0141],\n",
       "         [-0.0057,  0.0158,  0.0037,  ..., -0.0117,  0.0004,  0.0138],\n",
       "         ...,\n",
       "         [-0.0062, -0.0002,  0.0092,  ...,  0.0119, -0.0005, -0.0081],\n",
       "         [-0.0093,  0.0049, -0.0028,  ...,  0.0113, -0.0221, -0.0276],\n",
       "         [ 0.0195, -0.0128,  0.0049,  ..., -0.0015, -0.0362, -0.0176]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[-0.0070,  0.0101, -0.0117,  ...,  0.0037,  0.0124,  0.0032],\n",
       "         [-0.0130, -0.0050,  0.0097,  ...,  0.0132,  0.0088,  0.0148],\n",
       "         [-0.0063, -0.0139,  0.0077,  ..., -0.0134,  0.0067,  0.0030],\n",
       "         ...,\n",
       "         [ 0.0026, -0.0023, -0.0028,  ..., -0.0088,  0.0085, -0.0097],\n",
       "         [ 0.0039, -0.0136, -0.0031,  ...,  0.0128,  0.0073,  0.0140],\n",
       "         [ 0.0004,  0.0035,  0.0116,  ..., -0.0093, -0.0133,  0.0074]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.0152, -0.0036, -0.0047,  ..., -0.0009,  0.0125, -0.0082],\n",
       "         [ 0.0107,  0.0240, -0.0002,  ..., -0.0168, -0.0062, -0.0160],\n",
       "         [-0.0423,  0.0264,  0.0047,  ...,  0.0004,  0.0046, -0.0264],\n",
       "         ...,\n",
       "         [-0.0055,  0.0088,  0.0212,  ..., -0.0027, -0.0173, -0.0095],\n",
       "         [ 0.0248,  0.0021,  0.0214,  ...,  0.0159, -0.0177, -0.0138],\n",
       "         [-0.0062,  0.0058,  0.0065,  ..., -0.0192,  0.0242,  0.0208]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.0167, -0.0060,  0.0005,  ..., -0.0004, -0.0228,  0.0174],\n",
       "         [-0.0298, -0.0066,  0.0042,  ...,  0.0011, -0.0093, -0.0153],\n",
       "         [ 0.0003, -0.0037, -0.0041,  ...,  0.0118, -0.0022, -0.0188],\n",
       "         ...,\n",
       "         [-0.0043, -0.0182, -0.0024,  ...,  0.0028, -0.0276,  0.0101],\n",
       "         [-0.0223,  0.0010, -0.0138,  ...,  0.0057,  0.0128, -0.0344],\n",
       "         [ 0.0123, -0.0066,  0.0107,  ...,  0.0125,  0.0046,  0.0066]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.0102, -0.0108, -0.0013,  ..., -0.0013, -0.0091,  0.0035],\n",
       "         [-0.0154,  0.0030,  0.0107,  ...,  0.0097, -0.0124,  0.0151],\n",
       "         [ 0.0029,  0.0119, -0.0155,  ...,  0.0041, -0.0134,  0.0140],\n",
       "         ...,\n",
       "         [ 0.0119, -0.0076,  0.0049,  ..., -0.0131, -0.0046, -0.0057],\n",
       "         [-0.0052, -0.0111, -0.0105,  ...,  0.0140,  0.0003,  0.0044],\n",
       "         [ 0.0155, -0.0044,  0.0048,  ...,  0.0066,  0.0009, -0.0081]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 3.9108e-03, -7.6969e-04,  4.5646e-03,  ...,  4.7997e-03,\n",
       "           4.4154e-03,  6.1050e-03],\n",
       "         [-1.3166e-02, -2.1583e-02, -5.2084e-03,  ...,  3.5496e-03,\n",
       "          -3.4694e-05, -5.6743e-03],\n",
       "         [ 1.5248e-03,  5.3007e-03, -6.6066e-03,  ..., -1.2371e-02,\n",
       "          -1.9074e-02,  1.7441e-02],\n",
       "         ...,\n",
       "         [-2.5336e-02,  1.8370e-02,  3.6333e-02,  ...,  1.4916e-02,\n",
       "           2.7277e-03,  1.0447e-02],\n",
       "         [ 1.0354e-02, -1.6655e-02,  4.7177e-02,  ..., -2.0467e-02,\n",
       "           5.8156e-04, -3.2606e-03],\n",
       "         [ 1.8258e-02,  1.7095e-02,  4.7706e-03,  ...,  2.5143e-02,\n",
       "          -1.2634e-02,  1.6092e-02]], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.0131,  0.0060,  0.0042,  ..., -0.0231,  0.0041,  0.0366],\n",
       "         [-0.0145,  0.0122, -0.0015,  ...,  0.0127, -0.0010,  0.0260],\n",
       "         [-0.0014,  0.0211,  0.0208,  ..., -0.0068,  0.0070, -0.0221],\n",
       "         ...,\n",
       "         [ 0.0198, -0.0032, -0.0097,  ..., -0.0138, -0.0162, -0.0249],\n",
       "         [ 0.0114, -0.0120, -0.0135,  ...,  0.0289, -0.0034,  0.0469],\n",
       "         [-0.0030,  0.0081, -0.0012,  ...,  0.0241, -0.0098,  0.0300]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[-4.2721e-04,  1.2143e-03, -6.4853e-03,  ..., -3.6195e-03,\n",
       "           6.3183e-03, -9.2529e-03],\n",
       "         [ 1.2835e-02, -4.6528e-03,  6.1556e-03,  ...,  1.2608e-02,\n",
       "           3.0972e-03,  1.1854e-02],\n",
       "         [-1.3769e-02, -1.4622e-02, -2.1948e-03,  ...,  3.9228e-03,\n",
       "          -1.0853e-02, -5.3308e-03],\n",
       "         ...,\n",
       "         [-4.2983e-03,  1.0211e-02, -1.2070e-02,  ..., -8.9444e-05,\n",
       "           4.2334e-03,  4.7698e-03],\n",
       "         [-9.9240e-03, -2.6679e-03, -4.1620e-03,  ...,  1.2345e-02,\n",
       "           1.9998e-03, -1.2710e-02],\n",
       "         [ 5.7289e-03,  1.0467e-02, -1.4219e-02,  ...,  2.4632e-03,\n",
       "          -6.4712e-03, -1.4266e-02]], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
